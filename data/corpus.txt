1 
 LECTURE NOTES  
ON 
ARTIFICIAL INTELLIGENCE  
 
 
 
 
 
PREPARED BY  
DR. PRASHANTA KUMAR PATRA  
COLLEGE OF ENGINEERING AND TECHNOLOGY , 
BHUBANESWAR  
         
 
 
 
2 
  
ARTIFICIAL INTELLIGENCE  SYLLABUS  
Module 1                                                                                                                                                            12Hrs  
What is Artificial Intelligence? AI Technique, Level of the Model,Problem Spaces, and Search: Defining 
the Problem as a State Space Search, Production Systems, Problem Characteristics, Production System 
Characteristics, Issues in the Design of Searc h  Programs. Heuristic Search Techniques: Generate -and-
Test, Hill Climbing, Best -first Search, Problem Reduction, Constraint Satisfaction, Means -ends     
Analysis, Knowledge Representation: Representations and Mappings, Approaches to Knowledge 
Representati on, Using Predicate Logic: Representing Simple Facts in Logic, Representing Instance and 
ISA Relationships, Computable Functions and Predicates, Resolution, Natural Deduction.Using Rules: 
Procedural Versus Declarative Knowledge, Logic Programming, Forward Versus Backward Reasoning,  
Matching, Control Knowledge.Symbolic Reasoning Under Uncertainty: Introduction to Nonmonotonic 
Reasoning, Logics for Nonmonotonic Reasoning, Implementation Issues, Augmenting a Problem -solver, 
Depth -first Search, Breadthfirst Sea rch.Weak and Strong Slot -and-Filler Structures: Semantic Nets, 
Frames, Conceptual Dependency  Scripts, CYC.  
Module 2 10Hrs  
Game Playing: The Minimax Search Procedure, Adding Alpha -beta Cutoffs, Iterative Deepening.Planning: 
The Blocks World, Components of a Planning System, Goal Stack Planning, Nonlinear Planning Using 
Constraint Posting, Hierarchical PlanningOther Planning Techniques.Understanding: What is 
Understanding, What Makes Understanding Hard?, Understanding as Constraint Satisfaction.Natural 
Langu age Processing: Introduction, Syntactic Processing, Semantic Analysis, Discourse and Pragmatic 
Processing, Statistical Natural Language Processing, Spell Checking.  
Module 3                                                                                                                                                 8Hrs  
Learning: Rote Learning, learning by Taking Advice, Learning in Problem -solving, Learning from 
Examples: Induction, Explanation -based Learning, Discovery, Analogy, Formal Learning Theory, Ne ural 
Net Learning and Genetic Learning. Expert Systems: Representing and Using Domain Knowledge, Expert 
System Shells, Explanation, Knowledge Acquisition.  
Text Book:  
1. Elaine Rich, Kevin Knight, & Shivashankar B Nair, Artificial Intelligence,  McGraw Hill,  3rd ed.,2009  
References:  
1) Introduction to Artificial Intelligence & Expert Systems, Dan W Patterson,  PHI.,2010  
2) S Kaushik, Artificial Intelligence, Cengage Learning, 1st ed.2011  
 
 
 
3 
 Module 1                                                                                                                                       
ARTIFICIAL INTELLIGENCE  
What is Artificial Intelligence?  
It is a branch of Computer Science that  pursues creating the computers or machines as intelligent 
as human beings.   
It is the s cience and engineering of making intelligent machines, especially intelligent computer 
programs.  
It is related to the similar task of using computers to understand human intelligence, but  AI does 
not have to confine itself to methods that are biologically  observable  
Definition:  Artificial Intelligence  is the study of how to make computers do things, which, at the 
moment, people do better.  
According to the father of Artificial Intelligence, John McCarthy, it is  “The science and 
engineering of making intelli gent machines, especially intelligent computer programs”.  
Artificial Intelligence is a way of  making a computer, a computer -controlled robot, or a 
software think intelligently , in the similar manner the intelligent humans think.  
AI is accomplished by study ing how human brain thinks and how humans learn, decide, and 
work while trying to solve a problem, and then using the outcomes of this study as a basis of 
developing intelligent software and systems.  
It has gained prominence recently due, in part, to  big d ata, or the increase in speed, size and 
variety of data businesses are now collecting. AI can perform tasks such as identifying patterns 
in the data more efficiently than humans, enabling businesses to gain more insight out of 
their data.  
From a business perspective AI is a set of very powerful tools, and  methodologies for using 
those tools to solve business problems.  
From a programming perspective, AI includes the study of symbolic  programming, problem 
solving, and search.  
AI Vocabulary  
Intelligence relate s to tasks involving higher mental processes, e.g. creativity, solving problems, 
pattern recognition, classification, learning, induction, deduction, building analogies, 
optimization, language processing, knowledge and many more. Intelligence is the comput ational 
part of the ability to achieve goals.  
Intelligent behaviour is depicted by perceiving one’s environment, acting in complex 
environments, learning and understanding from experience, reasoning to solve problems and 
discover hidden knowledge, applying  knowledge successfully in new situations, thinking 
abstractly, using analogies, communicating with others and more.  
4 
  
Science based goals of AI pertain to developing concepts, mechanisms and understanding 
biological intelligent behaviour. The emphasis is o n understanding intelligent behaviour.  
 
Engineering based goals of AI relate to developing concepts, theory and practice of building 
intelligent machines. The emphasis is on system building.  
 
AI Techniques depict how we represent, manipulate and reason wit h knowledge in order to 
solve problems. Knowledge is a collection of ‘facts’. To manipulate these facts by a program, a 
suitable representation is required. A good representation facilitates problem solving.  
 
Learning means that programs learn from what fa cts or behaviour can represent. Learning 
denotes changes in the systems that are adaptive in other words, it enables the system to do the 
same task(s) more efficiently next time.  
 
Applications of AI refers to problem solving, search and control strategies,  speech recognition, 
natural language understanding, computer vision, expert systems, etc.  
 
Problems of AI:  
Intelligence does not imply perfect understanding; every intelligent being has limited perception, 
memory and computation. Many points on the spectr um of intelligence versus cost are viable, 
from insects to humans. AI seeks to understand the computations required from intelligent 
behaviour and to produce computer systems that exhibit intelligence. Aspects of intelligence 
studied by AI include percepti on, communicational using human languages, reasoning, planning, 
learning and memory.  
 
The following questions are to be considered before we can step forward:  
1. What are the underlying assumptions about intelligence?  
2. What kinds of techniques will be us eful for solving AI problems?  
3. At what level human intelligence can be modelled?  
4. When will it be realized when an intelligent program has been built?  
 
Branches of AI : 
 
A list of branches of AI is given below. However some branches are surely missing,  because no 
one has identified them yet. Some of these may be regarded as concepts  or topics rather than full 
branches.  
 
Logical AI  — In general the facts of the specific situation in which it must act, and its goals are 
all represented by sentences of some  mathematical logical language. The program  decides what 
to do by inferring that certain actions are appropriate for achieving its  goals.  
 
5 
 Search  — Artificial Intelligence programs often examine large numbers of possibilities – for 
example, moves in a ches s game and inferences by a theorem proving program.  Discoveries are 
frequently made about how to do this more efficiently in various  domains.  
 
Pattern Recognition  — When a program makes observations of some kind, it is often planned 
to compare  what it sees  with a pattern. For example, a vision program may try to match a  pattern 
of eyes and a nose in a scene in order to find a face. More complex patterns  are like a natural 
language text, a chess position or in the history of some event.  These more complex pa tterns 
require quite different methods than do the simple  patterns that have been studied the most.  
 
Representation  — Usually languages of mathematical logic are used to represent the facts about 
the world.  
 
Inference  — Others can be inferred from some fac ts. Mathematical logical deduction is 
sufficient  for some purposes, but new methods of non-monotonic inference have been added  to 
the logic since the 1970s. The simplest kind of non -monotonic reasoning is default  reasoning in 
which a conclusion is to be in ferred by default. But the conclusion can  be withdrawn if there is 
evidence to the divergent. For example, when we hear of a  bird, we infer that it can fly, but this 
conclusion can be reversed when we hear that  it is a penguin. It is the possibility that a  
conclusion may have to be withdrawn that  constitutes the non -monotonic character of the 
reasoning. Normal logical reasoning  is monotonic, in that the set of conclusions can be drawn 
from a set of premises, i.e.  monotonic increasing function of the premise s. Circumscription is 
another form of  non-monotonic reasoning.  
 
Common sense knowledge and Reasoning  — This is the area in which AI is farthest from the 
human level, in spite of the fact that  it has been an active research area since the 1950s. While 
there  has been considerable  progress in developing systems of non-monotonic reasoning and 
theories of action,  yet more new ideas are needed.  
 
Learning from experience  — There are some rules expressed in logic for learning. Programs 
can only learn what  facts or behaviour their formalisms can represent, and unfortunately learning 
systems  are almost all based on very limited abilities to represent information.  
 
Planning  — Planning starts with general facts about the world (especially facts about the effects  
of acti ons), facts about the particular situation and a statement of a goal. From  these, planning 
programs generate a strategy for achieving the goal. In the most  common cases, the strategy is 
just a sequence of actions.  
 
Epistemology  — This is a study of the kin ds of knowledge that are required for solving 
problems in  the world.  
 
Ontology  — Ontology is the study of the kinds of things that exist. In AI the programs and  
sentences deal with various kinds of objects and we study what these kinds are and  what their 
basic properties are. Ontology assumed importance from the 1990s.  
 
6 
 Heuristics  — A heuristic is a way of trying to discover something or an idea embedded in a  
program. The term is used variously in AI. Heuristic functions are used in some  approaches to 
searc h or to measure how far a node in a search tree seems to be from  a goal. Heuristic 
predicates that compare two nodes in a search tree to see if one is  better than the other, i.e. 
constitutes an advance toward the goal, and may be more  useful.  
 
Genetic prog ramming  — Genetic programming is an automated method for creating a working 
computer  program from a high -level problem statement of a problem. Genetic programming  
starts from a high -level statement of ‘what needs to be done’ and automatically  creates a 
computer program to solve the problem.  
 
Applications of AI 
 
AI has applications in all fields of human study, such as finance and economics, environmental 
engineering, chemistry, computer science, and so on. Some of the applications of AI are listed 
below:  
 Perception  
■ Machine vision  
■ Speech understanding  
■ Touch ( tactile or haptic ) sensation  
 Robotics  
 Natural Language Processing  
■ Natural Language Understanding  
■ Speech Understanding  
■ Language Generation  
■ Machine Translation  
 Planning  
 Expert Systems  
 Machine Learning  
 Theorem Proving  
 Symbolic Mathematics  
 Game Playing  
 
AI Technique:  
 
Artificial Intelligence research during the last three decades has concluded that  Intelligence 
requires knowledge . To compensate overwhelming quality, knowledge  possesses less desirable 
properties.  
A. It is huge.  
B. It is difficult to characterize correctly.  
C. It is constantly varying.  
D. It differs from data by being organized in a way that corresponds to its  application.  
E. It is complicated.  
 
 
7 
 An AI technique is a method that exploits kn owledge that is represented so that:  
 The knowledge captures generalizations that share properties, are grouped  
together, rather than being allowed separate representation.  
 
 It can be understood by people who must provide it —even though for many  
programs bu lk of the data comes automatically from readings.  
 
 In many AI domains, how the people understand the same people must supply  the 
knowledge to a program.  
 
 It can be easily modified to correct errors and reflect changes in real conditions.  
 
 It can be widely used even if it is incomplete or inaccurate.  
 
 It can be used to help overcome its own sheer bulk by helping to narrow the  range 
of possibilities that must be usually considered.  
 
In order to characterize an AI technique let us consider initially OXO or tic -tac-toe and use a 
series of different approaches to play the game.  
The programs increase in complexity, their use of generalizations, the clarity  of their 
knowledge and the extensibility of their approach. In this way they move  towards being 
representatio ns of AI techniques.  
 
Example -1: Tic-Tac-Toe 
 
1.1 The first approach (simple)  
 
The Tic -Tac-Toe game consists of a nine element vector called BOARD; it represents the 
numbers 1 to 9 in three rows.  
 
An element contains the value 0 for blank, 1 for X and 2 for O . A MOVETABLE vector consists 
of 19,683 elements (39) and is needed where each element is a nine element vector. The contents 
of the vector are especially chosen to help the algorithm.  
The algorithm makes moves by pursuing the following:  
1. View the vector as  a ternary number. Convert it to a decimal number.  
2. Use the decimal number as an index in MOVETABLE and access the vector.  
3. Set BOARD to this vector indicating how the board looks after the move. This approach is 
capable in time but it has several disadvanta ges. It takes more space and requires stunning 
8 
 effort to calculate the decimal numbers. This method is specific to this game and cannot be 
completed.  
 
1.2 The second approach  
 
The structure of the data is as before but we use 2 for a blank, 3 for an X and 5 fo r an O. 
A variable called TURN indicates 1 for the first move and 9 for the last. The algorithm consists 
of three actions:  
MAKE2 which returns 5 if the centre square is blank; otherwise it returns any blank non -
corner square, i.e. 2, 4, 6 or 8. POSSWIN (p)  returns 0 if player p cannot win on the next move 
and otherwise returns the number of the square that gives a winning move.  
 It checks each line using products 3*3*2 = 18 gives a win for X, 5*5*2=50 gives a win 
for O, and the winning move is the holder of  the blank. GO (n) makes a move to square n setting 
BOARD[n] to 3 or 5.  
This algorithm is more involved and takes longer but it is more efficient in storage which 
compensates for its longer time. It depends on the programmer’s skill.  
 
1.3 The final approa ch 
 
The structure of the data consists of BOARD which contains a nine element vector, a list of 
board positions that could result from the next move and a number representing an estimation of how 
the board position leads to an ultimate win for the player t o move.  
This algorithm looks ahead to make a decision on the next move by deciding which the most 
promising move or the most suitable move at any stage would be and selects the same.  
Consider all possible moves and replies that the program can make. Conti nue this process for 
as long as time permits until a winner emerges, and then choose the move that leads to the computer 
program winning, if possible in the shortest time.  
Actually this is most difficult to program by a good limit but it is as far that the  technique can 
be extended to in any game. This method makes relatively fewer loads on the programmer in terms 
of the game technique but the overall game strategy must be known to the adviser.  
 
Example -2: Question Answering  
 
Let us consider Question Answer ing systems that accept input in English and provide 
answers also in English. This problem is harder than the previous one as it is more difficult to 
specify the problem properly. Another area of difficulty concerns deciding whether the answer 
obtained is correct, or not, and further what is meant by ‘correct’. For example, consider the 
following situation:  
 
2.1 Text  
 
Rani went shopping for a new Coat. She found a red one she really liked.  
When she got home, she found that it went perfectly with her favouri te dress.  
 
2.2 Question  
 
1. What did Rani go shopping for?  
9 
 2. What did Rani find that she liked?  
3. Did Rani buy anything?  
 
Method 1  
 
2.3 Data Structures  
 
A set of templates that match common questions and produce patterns used to match  
against inputs. Tem plates and patterns are used so that a template that matches a  given question 
is associated with the corresponding pattern to find the answer in the  input text. For example, the 
template who did x y generates x y z if a match occurs  and z is the answer to the question. The 
given text and the question are both stored  as strings.  
 
2.4 Algorithm  
 
Answering a question requires the following four steps to be followed:  
 
 Compare the template against the questions and store all successful matches to produce a 
set of text patterns.  
 
 Pass these text patterns through a substitution process to change the person  or voice and 
produce an expanded set of text patterns.  
 
 Apply each of these patterns to the text; collect all the answers and then print  the 
answers.  
 
2.5 Exampl e 
 
In question 1 we use the template WHAT DID X Y which generates  Rani go shopping for z and 
after substitution we get  Rani goes shopping for z and Rani went shopping for z giving z 
[equivalence] a  new coat  
 
In question 2 we need a very large number of tem plates and also a scheme to allow  the insertion 
of ‘find’ before ‘that she liked’; the insertion of ‘really’ in the text; and  the substitution of ‘she’ 
for ‘Rani’ gives the answer ‘a red one’.  
 
Question 3 cannot be answered.  
 
2.6 Comments  
 
This is a very p rimitive approach basically not matching the criteria we set for  
intelligence and worse than that, used in the game. Surprisingly this type of technique  was 
actually used in ELIZA which will be considered later in the course.  
 
 
 
10 
 Method 2  
2.7 Data Structure s 
 
A structure called English consists of a dictionary, grammar and some semantics  about 
the vocabulary we are likely to come across. This data structure provides the  knowledge to 
convert English text into a storable internal form and also to convert  the r esponse back into 
English. The structured representation of the text is a processed  form and defines the context of 
the input text by making explicit all references such  as pronouns. There are three types of such 
knowledge representation systems:  productio n rules of the form ‘if x then y’, slot and filler 
systems and statements in  mathematical logic. The system used here will be the slot and filler 
system.  
 
Take,  for example sentence:   
‘She found a red one she really liked’.  
 
Event2         Event2  
instance:    finding      instance:    liking  
tense:    past      tense:     past 
agent:    Rani      modifier:    much  
object:    Thing1     object:   Thing1  
 
Thing1  
instance:   coat 
colour:   red 
 
The question is stored in two forms: as input and in the above form.  
 
2.8 Algorithm  
 
 Convert the question to a structured form using English know how, then use  a marker to 
indicate the substring (like ‘who’ or ‘what’) of the structure, that  should be returned as an 
answer. If a slot and filler system is used a special  marke r can be placed in more than one 
slot. 
 The answer appears by matching this structured form against the structured  text. 
 The structured form is matched against the text and the requested segments  of the 
question are returned.  
 
2.9 Examples  
 
Both questions 1  and 2 generate answers via a new coat and a red coat respectively.  
Question 3 cannot be answered, because there is no direct response.  
 
2.10 Comments  
 
This approach is more meaningful than the previous one and so is more effective.  The 
extra power given m ust be paid for by additional search time in the knowledge  bases. A warning 
11 
 must be given here: that is – to generate unambiguous  English  knowledge base is a complex task 
and must be left until later in the course. The  problems of handling pronouns are dif ficult.  
 
For example:  
Rani walked up to the salesperson: she asked where the toy department was.  
Rani walked up to the salesperson: she asked her if she needed any help.  
 
Whereas in the original text the linkage of ‘she’ to ‘Rani’ is easy, linkage of  ‘she’ in each of the 
above sentences to Rani and to the salesperson requires additional  knowledge about the context 
via the people in a shop.  
 
Method 3  
 
2.11 Data Structures  
 
World model contains knowledge about objects, actions and situations that are  describ ed 
in the input text. This structure is used to create integrated text from input  text. The diagram 
shows how the system’s knowledge of shopping might be  represented and stored. This 
information is known as a script and in this case is a  shopping script.  (See figure 1.1  next page )  
 
1.8.2.12 Algorithm  
 
Convert the question to a structured form using both the knowledge contained in Method 
2 and the World model, generating even more possible structures, since even more knowledge is 
being used. Sometimes filte rs are introduced to prune the possible answers.  
To answer a question, the scheme followed is:  Convert the question to a structured form 
as before but use the world model to resolve any ambiguities that may occur. The structured 
form is matched against th e text and the requested segments of the question are returned.  
 
2.13 Example  
 
Both questions 1 and 2 generate answers, as in the previous program. Question 3 can now 
be answered. The shopping script is instantiated and from the last sentence the path thro ugh step 
14 is the one used to form the representation. ‘M’ is bound to the red coat -got home. ‘ Rani buys 
a red coat’  comes from step 10 and the integrated text generates that she bought a red coat.  
 
2.14 Comments  
 
This program is more powerful than both t he previous programs because it has more 
knowledge. Thus, like the last game program it is exploiting AI techniques. However, we are not 
yet in a position to handle any English question. The major omission is that of a general 
reasoning mechanism known as inference to be used when the required answer is not explicitly 
given in the input text. But this approach can handle, with some modifications, questions of the 
following form with the answer —Saturday morning Rani went shopping. Her brother tried to 
call h er but she did not answer.  
 
Question :  Why couldn’t Rani’s brother reach her?  
12 
 Answer :  Because she was not in.  
 
This answer is derived because we have supplied an additional fact that a person cannot 
be in two places at once. This patch is not sufficiently  general so as to work in all cases and does 
not provide  the type of solution we are really looking for.  
 
 
 
13 
  
LEVEL OF THE AI MODEL  
 
‘What is our goal in trying to produce programs that do the intelligent things that people do?’  
 
Are we trying to produce p rograms that do the tasks the same way that people do?  
OR 
Are we trying to produce programs that simply do the tasks the easiest way that is 
possible?  
 
Programs in the first class attempt to solve problems that a computer can easily solve and 
do not usual ly use AI techniques. AI techniques usually include a search, as no direct method is 
available, the use of knowledge about the objects involved in the problem area and abstraction on 
which allows an element of pruning to occur, and to enable a solution to be found in real time; 
otherwise, the data could explode in size. Examples of these trivial problems in the first class, 
which are now of interest only to psychologists are EPAM (Elementary Perceiver and 
Memorizer) which memorized garbage syllables.  
 
The s econd class of problems attempts to solve problems that are non -trivial for a computer and 
use AI techniques. We wish to model human performance on these:  
 
1. To test psychological theories of human performance. Ex. PARRY [Colby, 1975] – a 
program to simulate  the conversational behavior of a paranoid person.  
2. To enable computers to understand human reasoning – for example, programs that 
answer questions based upon newspaper articles indicating human behavior.  
3. To enable people to understand computer reasoning. S ome people are reluctant to accept 
computer results unless they understand the mechanisms involved in arriving at the 
results.  
4. To exploit the knowledge gained by people who are best at gathering information. This 
persuaded the earlier workers to simulate h uman behavior in the SB part of AISB 
simulated behavior . Examples  of this type of approach led to GPS (General Problem 
Solver) . 
 
Questions  for Practice : 
 
1. What is intelligence ? How do we measure it? Are these measurements useful?  
2. When the temperature falls and the thermostat turns the heater on, does it act because it 
believes the room to be too cold? Does it feel cold? What sorts of things can have beliefs 
or feelings? Is this related to the idea of consciousness?  
3. Some people believe that the relationship b etween your mind (a non -physical thing) and 
your brain (the physical thing inside your skull) is exactly like the relationship between a 
computational process (a non -physical thing) and a physical computer. Do you agree?  
4. How good are machines at playing ch ess? If a machine can consistently beat all the best 
human chess players, does this prove that the machine is intelligent ? 
5. What is AI Technique? Explain Tic -Tac-Toe Problem using AI Technique.  
 
14 
  
PROBLEMS, PROBLEM SPACES AND SEARCH  
 
 
To solve the problem of  building a system you should take the following steps:  
1. Define the problem accurately including detailed specifications and what constitutes a 
suitable solution.  
2. Scrutinize the problem carefully, for some features may have a central affect on the 
chosen method of solution.  
3. Segregate and represent the background knowledge needed in the solution of the 
problem.  
4. Choose the best solving techniques for the problem to solve a solution.  
 
Problem solving is a process of generating solutions from observe d data.  
• a ‘problem’ is characterized by a set of goals , 
• a set of objects , and  
• a set of operations . 
These could be ill -defined and may evolve during problem solving.  
• A ‘problem space ’ is an abstract space.  
 A problem space encompasses all valid state s that can be generated by the 
application of any combination of operators on any combination of objects . 
 The problem space may contain one or more solutions . A solution is a 
combination of operations and objects that achieve the goals . 
• A ‘ search ’ refers  to the search for a solution in a problem space.  
 Search proceeds with different types of ‘ search control strategies ’. 
 The depth -first search and breadth -first search are the two common search  
strategies.  
 
2.1 AI - General Problem Solving  
 
Problem solving has been the key area of concern for Artificial Intelligence.  
 
Problem solving is a process of generating solutions from observed or given data. It is however 
not always possible to use direct methods (i.e. go directly from data to solution). Instead, 
problem solving often needs to use indirect or modelbased methods.  
 
General Problem Solver (GPS) was a computer program created in 1957 by Simon and Newell 
to build a universal problem solver machine. GPS was based on Simon and Newell’s theoretical 
work on log ic machines. GPS in principle can solve any formalized symbolic problem, such as 
theorems proof and geometric problems and chess playing. GPS solved many simple problems, 
such as the Towers of Hanoi, that could be sufficiently formalized, but GPS could not  solve any 
real-world problems . 
 
To build a system to solve a particular problem, we need to:  
 Define the problem precisely – find input situations as well as final situations for an 
acceptable solution to the problem  
15 
  Analyze  the problem – find few importan t features that may have impact on the 
appropriateness of various possible techniques for solving the problem  
 Isolate and represent task knowledge necessary to solve the problem  
 Choose the best problem -solving technique(s) and apply to the particular probl em 
 
 
Problem definitions  
A problem is defined by its ‘ elements ’ and their ‘ relations ’. To provide a formal  description of a 
problem, we need to do the following:  
 
a. Define a state space that contains all the possible configurations of the relevant objects, 
including some impossible ones.  
b. Specify one or more states that describe possible situations, from which the problem -
solving process may start. These states are called initial states . 
c. Specify one or more states that would be acceptable solution to the probl em. 
 
These states are called goal states . 
 
Specify a set of rules that describe the actions ( operators ) available.  
 
The problem can then be solved by using the rules , in combination with an  appropriate control 
strategy , to move through the problem space until a path from  an initial state to a goal state is 
found. This process is known as ‘search’ . Thus:  
 
 Search is fundamental to the problem -solving process.  
 Search is a general mechanism that can be used when a more direct 
method is not  known.  
 Search provid es the framework into which more direct methods for 
solving subparts of     a problem can be embedded. A very large number of 
AI problems are formulated as    search problems.  
 Problem space  
 
A problem space is represented by a directed graph, where nodes represent  search state and paths 
represent the operators applied to change the state .  
 
To simplify search algorithms, it is often convenient to logically and  programmatically represent 
a problem space as a tree. A tree usually decreases  the complexity of a  search at a cost. Here, the 
cost is due to duplicating some  nodes on the tree that were linked numerous times in the graph, 
e.g. node B and node D. 
 
A tree is a graph in which any two vertices are connected by exactly one  path. Alternatively, any 
connecte d graph with no cycles is a tree . 
 
 
16 
 
 
 
• Problem solving : The term, Problem Solving relates to analysis in AI. Problem solving may be  
characterized as a systematic search through a range of possible actions to  reach some predefined 
goal or solution. Proble m-solving methods are  categorized as special purpose and general 
purpose . 
 
• A special -purpose method is tailor -made for a particular problem, often  exploits very specific 
features of the situation in which the problem is  embedded.  
 
• A general -purpose met hod is applicable to a wide variety of problems. One  General -purpose 
technique used in AI is ‘means -end analysis’: a step -bystep,  or incremental, reduction of the 
difference between current state and  final goal.  
 
 
 
 
 
 
 
 
17 
 2.3 DEFINING PROBLEM AS A STATE SPAC E SEARCH  
 
To solve the problem of playing a game, we require the rules of the game and  targets for winning 
as well as representing positions in the game. The opening position  can be defined as the initial 
state and a winning position as a goal state. Moves  from  initial state to other states leading to the 
goal state follow legally. However, the  rules are far too abundant in most games — especially in 
chess, where they exceed  the number of particles in the universe. Thus, the rules cannot be 
supplied accurate ly and computer programs cannot handle easily. The storage also presents 
another  problem but searching can be achieved by hashing.  
 
The number of rules that are used must be minimized and the set can be  created by expressing 
each rule in a form as possible . The representation of games  leads to a state space representation 
and it is common for well -organized games  with some structure. This representation allows for 
the formal definition of a problem  that needs the movement from a set of initial positions to one 
of a set of target  positions. It means that the solution involves using known techniques and a 
systematic  search. This is quite a common method in Artificial Intelligence.  
 
 
 2.3.1 State Space Search  
A state space represents a problem in terms of state s and operators that change  states.  
A state space consists of:  
 A representation of the states the system can be in. For example, in a 
board game, the board represents the current state of the game.  
 A set of operators that can change one state into another state. In a board 
game, the operators are the legal moves from any given state. Often the 
operators are represented as programs that change a state representation to 
represent the new state.  
 An initial state . 
 A set of final states ; some of these may be des irable, others undesirable. 
This set is often represented implicitly by a program that detects terminal 
states.  
 
2.3.2 The Water Jug Problem  
 
In this problem, we use two jugs called four and three; four holds a maximum of  four gallons of 
water and three a maximum of three gallons of water. How can we  get two gallons of water in 
the four jug? 
 
The state space is a set of prearranged pairs giving the number of gallons of  water in the pair of 
jugs at any time, i.e., ( four, three ) where four = 0, 1, 2, 3 or 4  and three = 0, 1, 2 or 3.  
 
The start state is (0, 0) and the goal state is (2, n) where n may be any but it is  limited to three 
holding from 0 to 3 gallons of water or empty. Three and four  shows the name and numerical 
number shows the amount of water in ju gs for solving  the water jug problem. The major 
production rules for solving this problem are  shown below:  
 
18 
 Initial condition        Goal comment  
1. (four, three) if four < 4      (4, three) fill four from tap  
2. (four, three) if three< 3      (four, 3) fil l three from tap  
3. (four, three) If four > 0      (0, three) empty four into drain  
4. (four, three) if three > 0      (four, 0) empty three into drain  
5. (four, three) if four + three<4  (four + three, 0) empty three into 
four 
6. (four, three) if four + t hree<3  (0, four + three) empty four into 
three  
7. (0, three) If three > 0      (three, 0) empty three into four  
8. (four, 0) if four > 0       (0, four) empty four into three  
9. (0, 2)        (2, 0) empty three into four  
10. (2, 0)        (0, 2) empty fou r into three  
11. (four, three) if four < 4      (4, three -diff) pour diff, 4 -four, into  
four from three  
12. (three, four) if three < 3      (four -diff, 3) pour diff, 3 -three, into  
three from four and a solution is 
given  below four three rule  
(Fig. 2.2 Prod uction Rules for the Water Jug Problem ) 
 
 
Gallons in Four Jug   Gallons in Three Jug    Rules Applied  
0      0      - 
0      3      2 
3      0      7 
3      3      2 
4      2      11 
0      2      3 
2      0      10 
(Fig. 2.3 One Solution to the Water Jug P roblem ) 
 
The problem solved by using the production rules in combination with an appropriate  control 
strategy, moving through the problem space until a path from an initial state  to a goal state is 
found. In this problem solving process, search is the fund amental  concept. For simple problems 
it is easier to achieve this goal by hand but there will  be cases where this is far too difficult.  
 
2.4 PRODUCTION SYSTEMS  
 
Production systems provide appropriate structures for performing and describing  search 
processe s. A production system has four basic components as enumerated  below.  
 
 A set of rules each consisting of a left side that determines the applicability of the 
rule and a right side that describes the operation to be performed if the rule is 
applied.  
 A datab ase of current facts established during the process of inference.  
19 
  A control strategy that specifies the order in which the rules will be compared 
with facts in the database and also specifies how to resolve conflicts in selection 
of several rules or select ion of more facts.  
 A rule firing module.  
 
The production rules operate on the knowledge database. Each rule has a  precondition —that is, 
either satisfied or not by the knowledge database. If the  precondition is satisfied, the rule can be 
applied. Applicatio n of the rule changes  the knowledge database. The control system chooses 
which applicable rule should  be applied and ceases computation when a termination condition on 
the knowledge  database is satisfied.  
 
Example: Eight puzzle (8 -Puzzle)  
 
The 8 -puzzle is a 3 × 3 array containing eight square pieces, numbered 1 through 8,  and 
one empty space. A piece can be moved horizontally or vertically into the empty  space, in effect 
exchanging the positions of the piece and the empty space. There  are four possible move s, UP 
(move the blank space up), DOWN, LEFT and RIGHT.  The aim of the game is to make a 
sequence of moves that will convert the board  from the start state into the goal state:  
 
 
This example can be solved by the operator sequence UP, RIGHT, UP, LEFT, DOWN . 
 
Example: Missionaries and Cannibals  
 
The Missionaries and Cannibals problem illustrates the use of state space search for  planning 
under constraints:  
Three missionaries and three cannibals wish to cross a river using a two person boat. If 
at any time th e cannibals outnumber the missionaries on either side of the river, they will eat the 
missionaries. How can a sequence of boat trips be performed that will get everyone to the other 
side of the river without any missionaries being eaten?  
 
State representat ion: 
 
1. BOAT position: original (T) or final (NIL) side of the river.  
2. Number of Missionaries and Cannibals on the original side of the river.  
3. Start is (T 3 3); Goal is (NIL 0 0).  
 
Operators:  
20 
 
 
 
21 
 2.4.1 Control Strategies  
 
The word ‘ search ’ refers to the search for a solution in a problem space . 
• Search proceeds with different types of ‘search control strategies ’. 
• A strategy is defined by picking the order in which the nodes expand.  
The Search strategies are evaluated along the following dimensions:  Completeness, Time 
complexity, Space complexity, Optimality (the search - related  terms are first explained, and then 
the search algorithms and control strategies are  illustrated next).  
 
Search -related terms  
• Algorithm’s performance and complexity  
Ideally  we want a common measure so that we can compare approaches in  order to select 
the most appropriate algorithm for a given situation.  
 Performance of an algorithm depends on internal and external factors.  
 
Internal factors/ External factors  
  Time required to  run 
  Size of input to the algorithm  
  Space (memory) required to run  
  Speed of the computer  
  Quality of the compiler  
 Complexity is a measure of the performance of an algorithm. Complexity 
measures the internal factors, usually in time than space.  
• Computa tional complexity \ 
It is the measure of resources in terms of Time and Space . 
 
 If A is an algorithm that solves a decision problem f, then run -time of A is the number of 
steps taken on the input of length n. 
 Time Complexity T(n) of a decision problem f is the run -time of the ‘best’ algorithm A 
for f. 
 Space Complexity S(n) of a decision problem f is the amount of memory used by the 
‘best’ algorithm A for f. 
 
• ‘Big - O’ notation  
The Big-O, theoretical measure of the execution of an algorithm , usually  indicat es the time or the 
memory needed, given the problem size n, which is  usually the number of items.  
• Big-O notation  
The Big-O notation is used to give an approximation to the run-time- efficiency  of an algorithm; 
the letter ‘ O’ is for order of magnitude of operations or  space at run -time.  
• The Big-O of an Algorithm A 
 If an algorithm A requires time proportional to f(n), then algorithm A is said to be 
of order f(n), and it is denoted as O(f(n)) . 
 If algorithm A requires time proportional to n2, then the order  of the algorithm is 
said to be O(n2).  
 If algorithm A requires time proportional to n, then the order of the algorithm is 
said to be O(n).  
22 
 The function f(n) is called the algorithm’s growth -rate function . In other words, if  an algorithm 
has performance com plexity O(n) , this means that the run -time t should  be directly proportional 
to n, ie t • n or t = k n where k is constant of proportionality.  
Similarly, for algorithms having performance complexity O(log2(n)), O(log N),  O(N log N), 
O(2N) and so on.  
 
Examp le 1: 
Determine the Big-O of an algorithm:  
 
Calculate the sum of the n elements in an integer array a[0..n -1]. 
 
Line no.   Instructions    No of execution steps  
line 1    sum     1 
line 2    for (i = 0; i < n; i++)   n + 1  
line 3    sum += a[i]    n 
line 4     print sum    1 
Total     2n + 3  
 
Thus, the polynomial (2n + 3) is dominated by the 1st term as n while the number  of elements in 
the array becomes very large.  
 
• In determining the Big-O, ignore constants such as 2 and 3. So the algorithm  is of order n. 
• So the Big-O of the algorithm is O(n) . 
• In other words the run -time of this algorithm increases roughly as the size of  the input data n, 
e.g., an array of size n. 
 
Tree structure  
Tree is a way of organizing objects, related in a hierarchical fashion.  
• Tree is a type of data structure in which each element is attached to one or  more 
elements directly beneath it.  
• The connections between elements are called branches . 
• Tree is often called inverted trees because it is drawn with the root at the top.  
• The elements that have no elements below them are called leaves . 
• A binary tree is a special type: each element has only two branches below it.  
Properties  
• Tree is a special case of a graph . 
• The topmost node in a tree is called the root node . 
• At root  node all operations on the tree begin.  
• A node has at most one parent.  
• The topmost node (root node) has no parents.  
• Each node has zero or more child nodes , which are below it .  
• The nodes at the bottommost level of the tree are called leaf nodes . 
Since leaf nodes are at the bottom most level, they do not have children.  
• A node that has a child is called the child’s parent node . 
• The depth of a node n is the length of the path from the root to the node.  
• The root node is at depth zero.  
23 
 • Stacks and  Queues  
 
The Stacks and Queues are data structures that maintain the order of last-in, first-out and first-in, 
first-out respectively. Both stacks and queues are often  implemented as linked lists, but that is  
not the only possible implementation.  
Stack - Last In First Out (LIFO) lists  
 An ordered list; a sequence of items, piled one on top of the other.  
 The insertions and deletions are made at one end only, called Top. 
 If Stack S = (a[1], a[2], . . . . a[n]) then a[1] is bottom most element  
 Any intermediate element (a[i]) is on top of element a[i-1], 1 < i <= n.  
 In Stack all operation take place on Top. 
 
The Pop operation removes item from top of the stack.  
The Push operation adds an item on top of the stack.  
 
Queue - First In First Out (FIFO) lists  
 
• An ord ered list; a sequence of items; there are restrictions about how items  can be added to and 
removed from the list. A queue has two ends.  
• All insertions (enqueue ) take place at one end, called Rear or Back  
• All deletions (dequeue) take place at other end , called Front . 
• If Queue has a[n] as rear element then a[i+1] is behind a[i] , 1 < i <= n.  
• All operation takes place at one end of queue or the other.  
 
The Dequeue operation removes the item at Front of the queue.  
The Enqueue operation adds an item to the Rear of the queue.  
Search  
 
Search is the systematic examination of states to find path from the start / root state  to the goal 
state . 
 
• Search usually results from a lack of knowledge.  
• Search explores knowledge alternatives to arrive at the best ans wer. 
• Search algorithm output is a solution, that is, a path from the initial state to a  state that satisfies 
the goal test.  
 
For general -purpose problem -solving – ‘Search’ is an approach . 
 
• Search deals with finding nodes having certain properties in a graph that represents search 
space.  
• Search methods explore the search space ‘intelligently’, evaluating  possibilities without 
investigating every single possibility.  
Examples:  
• For a Robot this might consist of PICKUP, PUTDOWN, MOVEFORWARD,  MOVEBACK, 
MOVELEFT, and MOVERIGHT —until the goal is reached.  
• Puzzles and Games have explicit rules: e.g., the ‘ Tower of Hanoi ’ puzzle  
24 
 
 
This puzzle involves a set of rings of different sizes that can be placed on  three different pegs.  
• The puzzle starts with the ri ngs arranged as shown in Figure 2.4(a)  
• The goal of this puzzle is to move them all as to Figure 2.4(b)  
• Condition: Only the top ring on a peg can be moved, and it may only be  placed on a smaller 
ring, or on an empty peg.  
 
In this Tower of Hanoi puzzle:  
• Situations encountered while solving the problem are described as states . 
• Set of all possible configurations of rings on the pegs is called ‘problem space ’. 
• States  
A State is a representation of elements in a given moment.  
A problem is defined by its  elements and their relations . 
At each instant of a problem, the elements have specific descriptors and  relations; the descriptors 
indicate how to select elements?  
Among all possible states, there are two special states called:  
  Initial state – the start p oint 
 Final state – the goal state  
• State Change: Successor Function  
A ‘successor function ’ is needed for state change. The Successor Function  moves one state to 
another state.  
Successor Function:  
 It is a description of possible actions; a set of operators . 
 It is a transformation function on a state representation, which converts that state into 
another state.  
 It defines a relation of accessibility among states.  
 It represents the conditions of applicability of a state and corresponding transformation 
functi on. 
 
• State space  
A state space is the set of all states reachable from the initial state . 
  A state space forms a graph (or map) in which the nodes are states and the arcs between 
nodes are actions.  
  In a state space , a path is a sequence of states connec ted by a sequence of actions.  
  The solution of a problem is part of the map formed by the state space . 
25 
 • Structure of a state space  
The structures of a state space are trees and graphs . 
  A tree is a hierarchical structure in a graphical form.  
  A graph is a non-hierarchical structure.  
• A tree has only one path to a given node;  
i.e., a tree has one and only one path from any point to any other point.  
• A graph consists of a set of nodes (vertices) and a set of edges (arcs). Arcs  establish 
relationships (conn ections) between the nodes; i.e., a graph has  several paths to a given node.  
• The Operators are directed arcs between nodes.  
A search process explores the state space . In the worst case, the search explores  all possible 
paths between the initial state and the goal state . 
• Problem solution  
In the state space , a solution is a path from the initial state to a goal state or, sometimes, just a 
goal state . 
 A solution cost function assigns a numeric cost to each path; it also gives the cost of 
applying the opera tors to the states . 
 A solution quality is measured by the path cost function ; and an optimal solution has the 
lowest path cost among all solutions.  
 The solutions can be any or optimal or all . 
 The importance of cost depends on the problem and the type of so lution asked  
 
• Problem description  
A problem consists of the description of:  
 The current state of the world,  
 The actions that can transform one state of the world into another,  
 The desired state of the world.  
The following action one taken to describe the  problem:  
  State space is defined explicitly or implicitly  
A state space should describe everything that is needed to solve a problem  and nothing that is 
not needed to solve the problem.  
  Initial state is start state  
 Goal state is the conditions it has to fulfill.  
The description by a desired state may be complete or partial.  
 Operators are to change state  
 Operators do actions that can transform one state into another;  
 Operators consist of: Preconditions and Instructions;  
 
Preconditions provide partial descr iption of the state of the world that  must be true in order to 
perform the action, and  
Instructions tell the user how to create the next state.  
 Operators should be as general as possible, so as to reduce their number.  
 Elements of the domain has relevance t o the problem  
 Knowledge of the starting point.  
 Problem solving is finding a solution  
 Find an ordered sequence of operators that transform the current (start) state 
into a goal state.  
26 
  Restrictions are solution quality any, optimal, or all  
 Finding the shorte st sequence, or  
 finding the least expensive sequence defining cost, or  
 finding any sequence as quickly as possible.  
This can also be explained with the help of algebraic function as given below.  
 
 
PROBLEM CHARACTERISTICS  
 
Heuristics cannot be generalized, as they are domain specific. Production systems  provide ideal 
techniques for representing such heuristics in the form of IF -THEN  rules. Most problems 
requiring simulation of intelligence use heuristic search  extensively. Some heuristics are used to 
define the control structure that guides the  search process, as seen in the example described 
above. But heuristics can also be  encoded in the rules to represent the domain knowledge. Since 
most AI problems  make use of knowledge and guided search through the know ledge, AI can be  
described as the study of techniques for solving exponentially hard problems in  polynomial time 
by exploiting knowledge about problem domain . 
 
To use the heuristic search for problem solving, we suggest analysis of the  problem for the 
following considerations:  
 Decomposability of the problem into a set of independent smaller subproblems  
 Possibility of undoing solution steps, if they are found to be unwise  
 Predictability of the problem universe  
 Possibility of obtaining an obvious solution to a problem without comparison of all other 
possible solutions  
 Type of the solution: whether it is a state or a path to the goal state  
 Role of knowledge in problem solving  
 Nature of solution process: with or without interacting with the user  
 
The general cla sses of engineering problems such as planning, classification,  diagnosis, 
monitoring and design are generally knowledge intensive and use a large  amount of heuristics. 
Depending on the type of problem, the knowledge  representation schemes and control strat egies 
for search are to be adopted. Combining  heuristics with the two basic search strategies have been 
discussed above. There are  a number of other general -purpose search techniques which are 
essentially heuristics  based. Their efficiency primarily depend s on how they exploit the domain -
specific  knowledge to abolish undesirable paths. Such search methods are called ‘weak  
methods’, since the progress of the search depends heavily on the way the domain  knowledge is 
exploited. A few of such search techniques which form the centre of  many AI systems are briefly 
presented in the following sections.  
 
Problem Decomposition  
 
Suppose to solve the expression is: + (X³ + X² + 2X + 3sinx)  dx 
27 
 
 
This problem can be solved by breaking it into smaller problems, each of  which we can solve by 
using a small collection of specific rules. Using this technique  of problem decomposition, we can 
solve very large problems very easily. This can  be considered as an intelligent behaviour.  
 
Can Solution Steps be Ignored?  
 
Suppose we are  trying to prove a mathematical theorem: first we proceed considering  that 
proving a lemma will be useful. Later we realize that it is not at all useful. We  start with another 
one to prove the theorem. Here we simply ignore the first method.  
Consider the 8 -puzzle problem to solve: we make a wrong move and realize  that mistake. But 
here, the control strategy must keep track of all the moves, so that  we can backtrack to the initial 
state and start with some new move.  
Consider the problem of playing chess. Her e, once we make a move we never recover  from that 
step. These problems are illustrated in the three important classes of  problems mentioned below:  
1. Ignorable, in which solution steps can be ignored.  Eg: Theorem Proving  
2. Recoverable, in which solution s teps can be undone.  Eg: 8 -Puzzle  
3. Irrecoverable, in which solution steps cannot be undone.  Eg: Chess  
 
Is the Problem Universe Predictable?  
 
Consider the 8 -Puzzle problem. Every time we make a move, we know exactly  what will happen. 
This means that it is possible to plan an entire sequence of moves  and be confident what the 
resulting state will be. We can backtrack to earlier moves  if they prove unwise.  
 
Suppose we want to play Bridge. We need to plan before the first play, but we  cannot play with 
certaint y. So, the outcome of this game is very uncertain. In case  of 8-Puzzle, the outcome is 
very certain. To solve uncertain outcome problems, we  follow the process of plan revision as the 
plan is carried out and the necessary  feedback is provided. The disadvan tage is that the planning 
in this case is often very  expensive.  
 
Is Good Solution Absolute or Relative?  
Consider the problem of answering questions based on a database of simple facts  such as the 
following:  
28 
 1. Siva was a man.  
2. Siva was a worker in a comp any. 
3. Siva was born in 1905.  
4. All men are mortal.  
5. All workers in a factory died when there was an accident in 1952.  
6. No mortal lives longer than 100 years.  
Suppose we ask a question: ‘Is Siva alive?’  
By representing these facts in a formal languag e, such as predicate logic, and  then using formal 
inference methods we can derive an answer to this question easily.  
There are two ways to answer the question shown below:  
Method I:  
1. Siva was a man.  
2. Siva was born in 1905.  
3. All men are mortal.  
4. Now  it is 2008, so Siva’s age is 103 years.  
5. No mortal lives longer than 100 years.  
Method II:  
1. Siva is a worker in the company.  
2. All workers in the company died in 1952.  
Answer: So Siva is not alive. It is the answer from the above methods.  
 
We are int erested to answer the question; it does not matter which path we  follow. If we follow 
one path successfully to the correct answer, then there is no  reason to go back and check another 
path to lead the solution.  
 
CHARACTERISTICS OF PRODUCTION SYSTEMS  
Produc tion systems provide us with good ways of describing the operations that can  be 
performed in a search for a solution to a problem.  
At this time, two questions may arise:  
1. Can production systems be described by a set of characteristics? And how can they be 
easily implemented?  
2. What relationships are there between the problem types and the types of production 
systems well suited for solving the problems?  
To answer these questions, first consider the following definitions of classes  of production 
systems:  
1. A mono tonic production system is a production system in which the application of a 
rule never prevents the later application of another rule that could also have been 
applied at the time the first rule was selected.  
2. A non -monotonic production system is one in wh ich this is not true.  
3. A partially communicative production system is a production system with the 
property that if the application of a particular sequence of rules transforms state P into 
state Q, then any combination of those rules that is allowable also  transforms state P 
into state Q.  
4. A commutative production system is a production system that is both monotonic and 
partially commutative.  
 
29 
 
 
 
Is there any relationship between classes of production systems and classes  of problems? 
For any solvable problem s, there exist an infinite number of production  systems that show how 
to find solutions. Any problem that can be solved by any  production system can be solved by a 
commutative one, but the commutative one is  practically useless. It may use individual state s to 
represent entire sequences of  applications of rules of a simpler, non -commutative system. In the 
formal sense,  there is no relationship between kinds of problems and kinds of production systems  
Since all problems can be solved by all kinds of systems.  But in the practical sense,  there is 
definitely such a relationship between the kinds of problems and the kinds  of systems that lend 
themselves to describing those problems.  
 
Partially commutative, monotonic productions systems are useful for solving  ignorable 
problems. These are important from an implementation point of view  without the ability to 
backtrack to previous states when it is discovered that an  incorrect path has been followed. Both 
types of partially commutative production  systems are signific ant from an implementation point; 
they tend to lead to many  duplications of individual states during the search process.  Production 
systems that are not partially commutative are useful for many  problems in which permanent 
changes occur.  
 
Issues in the Des ign of Search Programs  
 
Each search process can be considered to be a tree traversal. The object of the  search is to find a 
path from the initial state to a goal state using a tree. The number  of nodes generated might be 
huge; and in practice many of the n odes would not be  needed. The secret of a good search 
routine is to generate only those nodes that are  likely to be useful, rather than having a precise 
tree. The rules are used to represent  the tree implicitly and only to create nodes explicitly if they 
are actually to be of  use. 
 
The following issues arise when searching:  
• The tree can be searched forward from the initial node to the goal state or  backwards from the 
goal state to the initial state.  
• To select applicable rules, it is critical to have an efficient procedure for  matching rules against 
states.  
• How to represent each node of the search process? This is the knowledge  representation 
problem or the frame problem. In games, an array suffices; in  other problems, more complex 
data structures are n eeded.  
 
Finally in terms of data structures, considering the water jug as a typical  problem do we use a 
graph or tree? The breadth -first structure does take note of all  nodes generated but the depth -first 
one can be modified.  
30 
 Check duplicate nodes  
 
1. Obse rve all nodes that are already generated, if a new node is present.  
2. If it exists add it to the graph.  
3. If it already exists, then  
a. Set the node that is being expanded to the point to the already existing  node 
corresponding to its successor rather th an to the new one. The new  one can be thrown 
away.  
 
b. If the best or shortest path is being determined, check to see if this path is  better or 
worse than the old one. If worse, do nothing.  
 
Better save the new path and work the change in length through th e chain of successor  nodes if 
necessary.  
 
Example: Tic -Tac-Toe 
 
State spaces are good representations for board games such as Tic -Tac-Toe. The  position of a 
game can be explained by the contents of the board and the player  whose turn is next. The board 
can be represented as an array of 9 cells, each of  which may contain an X or O or be empty.  
• State:  
 Player to move next: X or O.  
 Board configuration:  
 
 
• Operators: Change an empty cell to X or O.  
• Start State: Board empty; X’s turn.  
• Terminal States: Three X’s in a row; Three O’s in a row; All cells full.  
 
Search Tree  
 
The sequence of states formed by possible moves is called a search tree . Each level  of the tree is 
called a ply. 
 
Since the same state may be reachable by different sequences of moves, the  state space may in 
general be a graph. It may be treated as a tree for simplicity, at  the cost of duplicating states.  
31 
 
 
Solving problems using search  
• Given an informal description of the problem, construct a formal description  as a state space:  
 Define a data structure to represent the state . 
 Make a representation for the initial state from the given data.  
 Write programs to represent operators that change a given state representation to a new 
state representation.  
 Write a program to detect terminal states . 
 
• Choose an appropriate search technique:  
 How large is the search space?  
 How well structured is the domain?  
 What knowledge about the domain can be used to guide the search?  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
32 
  
HEURISTIC SEARCH TECHNIQUES:  
 
Search Algorithms  
Many traditiona l search algorithms are used in AI applications. For complex  problems, the 
traditional algorithms are unable to find the solutions within some  practical time and space 
limits. Consequently, many special techniques are  developed, using heuristic functions.  
The algorithms that use heuristic functions are called heuristic algorithms . 
 
• Heuristic algorithms are not really intelligent; they appear to be intelligent  because they 
achieve better performance.  
• Heuristic algorithms are more efficient because they t ake advantage of  feedback from the data 
to direct the search path.  
• Uninformed search algorithms or Brute -force algorithms , search through  the search space all 
possible candidates for the solution checking whether  each candidate satisfies the problem’s 
statement.  
• Informed search algorithms use heuristic functions that are specific to the  problem, apply 
them to guide the search through the search space to try to  reduce the amount of time spent in 
searching.  
 
A good heuristic will make an informed search d ramatically outperform any  uninformed search: 
for example, the Traveling Salesman Problem (TSP), where the  goal is to find is a good solution 
instead of finding the best solution.  
 
In such problems, the search proceeds using current information about the  problem to predict 
which path is closer to the goal and follow it, although it does  not always guarantee to find the 
best possible solution. Such techniques help in  finding a solution within reasonable time and 
space (memory). Some prominent  intelligent sea rch algorithms are stated below:  
1. Generate and Test Search  
2. Best -first Search  
3. Greedy Search  
4. A* Search  
5. Constraint Search  
6. Means -ends analysis  
 
There are some more algorithms. They are either improvements or combinations of  these.  
• Hierarc hical Representation of Search Algorithms: A Hierarchical  representation of most 
search algorithms is illustrated below. The  representation begins with two types of search:  
• Uninformed Search: Also called blind, exhaustive or brute -force search, it  uses n o 
information about the problem to guide the search and therefore may  not be very efficient.  
• Informed Search: Also called heuristic or intelligent search, this uses  information about the 
problem to guide the search —usually guesses the  distance to a goal state and is therefore 
efficient, but the search may not be  always possible.  
 
33 
 
 
 
The first requirement is that it causes motion , in a game playing program, it moves  on the board 
and in the water jug problem, filling water is used to fill jugs. It means  the control strategies 
without the motion will never lead to the solution.  
The second requirement is that it is systematic , that is, it corresponds to the  need for global 
motion as well as for local motion. This is a clear condition that  neither would it be r ational to 
fill a jug and empty it repeatedly, nor it would be  worthwhile to move a piece round and round 
on the board in a cyclic way in a  game. We shall initially consider two systematic approaches for 
searching. Searches  can be classified by the order i n which operators are tried: depth -first, 
breadth -first, bounded  depth -first. 
 
34 
 
 
Breadth -first search  
A Search strategy, in which the highest layer of a decision tree is searched completely  before 
proceeding to the next layer is called Breadth -first search  (BFS).  
• In this strategy, no viable solutions are omitted and therefore it is guaranteed  that an optimal 
solution is found.  
• This strategy is often not feasible when the search space is large.  
Algorithm  
1. Create a variable called LIST and set it to be the starting state.  
2. Loop until a goal state is found or LIST is empty, Do  
a. Remove the first element from the LIST and call it E. If the LIST is empty,  quit. 
b. For every path each rule can match the state E, Do  
(i) Apply the rule to generate a new sta te. 
(ii) If the new state is a goal state, quit and return this state.  
(iii) Otherwise, add the new state to the end of LIST.  
 
 
35 
 Advantages  
1. Guaranteed to find an optimal solution (in terms of shortest number of steps  
to reach the goal).  
2. Can always fin d a goal node if one exists (complete).  
Disadvantages  
1. High storage requirement: exponential with tree depth.  
Depth -first search  
A search strategy that extends the current path as far as possible before backtracking  to the last 
choice point and trying th e next alternative path is called Depth -first search (DFS).  
• This strategy does not guarantee that the optimal solution has been found.  
• In this strategy, search reaches a satisfactory solution more rapidly than breadth  first, an 
advantage when the searc h space is large.  
Algorithm  
Depth -first search applies operators to each newly generated state, trying to drive  directly toward 
the goal.  
1. If the starting state is a goal state, quit and return success.  
2. Otherwise, do the following until success or fai lure is signalled:  
a. Generate a successor E to the starting state. If there are no more successors,  then signal failure.  
b. Call Depth -first Search with E as the starting state.  
c. If success is returned signal success; otherwise, continue in the loop.  
Advantages  
1. Low storage requirement: linear with tree depth.  
2. Easily programmed: function call stack does most of the work of maintaining  state of the 
search.  
Disadvantages  
1. May find a sub -optimal solution (one that is deeper or more costly than the  best solution).  
2. Incomplete: without a depth bound, may not find a solution even if one exists.  
2.4.2.3 Bounded depth -first search  
Depth -first search can spend much time (perhaps infinite time) exploring a very  deep path that 
does not contain a solution, w hen a shallow solution exists. An easy  way to solve this problem is 
to put a maximum depth bound on the search. Beyond  the depth bound, a failure is generated 
automatically without exploring any deeper.  
Problems:  
1. It’s hard to guess how deep the solution  lies. 
2. If the estimated depth is too deep (even by 1) the computer time used is  dramatically 
increased, by a factor of bextra . 
3. If the estimated depth is too shallow, the search fails to find a solution; all  that computer time 
is wasted.  
 
Heuristics  
A heuristic is a method that improves the efficiency of the search process. These are  like tour 
guides. There are good to the level that they may neglect the points in  general interesting 
directions; they are bad to the level that they may neglect points  of interest to particular 
individuals. Some heuristics help in the search process without  sacrificing any claims to entirety 
that the process might previously had. Others may  occasionally cause an excellent path to be 
overlooked. By sacrificing entirety it  increases efficiency. Heuristics may not find the best 
36 
 solution every time but  guarantee that they find a good solution in a reasonable time. These are 
particularly  useful in solving tough and complex problems, solutions of which would require  
infinite time , i.e. far longer than a lifetime for the problems which are not solved in  any other 
way.  
Heuristic search  
To find a solution in proper time rather than a complete solution in unlimited time  we use 
heuristics. ‘A heuristic function is a function that maps from problem state  descriptions to 
measures of desirability, usually represented as numbers’. Heuristic  search methods use 
knowledge about the problem domain and choose promising  operators first. These heuristic 
search methods use heuristic functions to ev aluate  the next state towards the goal state. For 
finding a solution, by using the heuristic  technique, one should carry out the following steps:  
1. Add domain —specific information to select what is the best path to continue  searching along.  
2. Define a he uristic function h(n) that estimates the ‘goodness’ of a node n.  
Specifically, h(n) = estimated cost(or distance) of minimal cost path from n     to a goal state.  
3. The term, heuristic means ‘serving to aid discovery’ and is an estimate, based  on domain 
specific information that is computable from the current state  description of how close we are to 
a goal.  
Finding a route from one city to another city is an example of a search problem in  which 
different search orders and the use of heuristic knowledge are easily  understood.  
1. State: The current city in which the traveller is located.  
2. Operators: Roads linking the current city to other cities.  
3. Cost Metric: The cost of taking a given road between cities.  
4. Heuristic information: The search could be gui ded by the direction of the  goal city from the 
current city, or we could use airline distance as an estimate  of the distance to the goal.  
Heuristic search techniques  
For complex problems, the traditional algorithms, presented above, are unable to  find the 
solution within some practical time and space limits. Consequently, many  special techniques are 
developed, using heuristic functions.  
• Blind search is not always possible, because it requires too much time or  Space (memory).  
 
Heuristics are rules of thumb ; they do not guarantee a solution to a problem.  
• Heuristic Search is a weak technique but can be effective if applied correctly;  it requires 
domain specific information.  
 
Characteristics of heuristic search  
• Heuristics are knowledge about domain, which help search and reasoning in  its domain.  
• Heuristic search incorporates domain knowledge to improve efficiency over  blind search.  
• Heuristic is a function that, when applied to a state, returns value as estimated  merit of state, 
with respect to goal.  
 Heuristics might (for reasons) underestimate or overestimate the merit of a state with 
respect to goal.  
 Heuristics that underestimate are desirable and called admissible.  
• Heuristic evaluation function estimates likelihood of given state leading to  goal sta te. 
• Heuristic search function estimates cost from current state to goal, presuming  function is 
efficient.  
 
37 
  
Heuristic search compared with other search  
The Heuristic search is compared with Brute force or Blind search techniques below:  
 
Comparison of Alg orithms  
 
Brute force / Blind search     Heuristic search  
Can only search what it has knowledge   Estimates ‘distance’ to goal state  
about already       through explored nodes  
 
No knowledge about how far a node    Guides search process toward goal  
node from  goal state  
Prefers states (nodes) that lead  
close to and not away from goal  
state 
 
Example: Travelling salesman  
A salesman has to visit a list of cities and he must visit each city only once. There  are different 
routes between the cities. The problem is t o find the shortest route  between the cities so that the 
salesman visits all the cities at once.  
 
Suppose there are N cities, then a solution would be to take N! possible  combinations to find the 
shortest distance to decide the required route. This is not  efficient as with N=10 there are 
36,28,800 possible routes. This is an example of  combinatorial explosion . 
 
There are better methods for the solution of such problems: one is called  branch and bound . 
First, generate all the complete paths and find the dist ance of the first complete  path. If the next 
path is shorter, then save it and proceed this way avoiding the path  when its length exceeds the 
saved shortest path length, although it is better than the  previous method.  
 
Generate a nd Test Strategy  
 
Generate -And-Test Algorithm  
Generate -and-test search algorithm is a very simple algorithm that guarantees to find a solution if 
done systematically and there exists a solution.  
Algorithm: Generate -And-Test 
1.Generate a possible solution.  
2.Test to see if this is th e expected solution.  
3.If the solution has been found quit else go to step 1.  
Potential solutions that need to be generated vary depending on the kinds of problems. For some 
problems the possible solutions may be particular points in the problem space and for some 
problems, paths from the start state.  
38 
 
 
Figure: Generate And Test  
Generate -and-test, like depth -first search, requir es that complete solutions be generated for 
testing. In its most systematic form, it is only an exhaustive search of the problem space. 
Solutions can also be generated randomly but solution is not guaranteed. This approach is what is 
known as British Museu m algorithm: finding an object in the British Museum by wandering 
randomly.  
Systematic Generate -And-Test 
While generating complete solutions and generating random solutions are the two extremes there 
exists another approach that lies in between. The approa ch is that the search process proceeds 
systematically but some paths that unlikely to lead the solution are not considered. This 
evaluation is performed by a heuristic function.  
Depth -first search tree with backtracking can be used to implement systematic generate -and-test 
procedure. As per this procedure, if some intermediate states are likely to appear often in the 
tree, it would be better to modify that procedure to traverse a graph rather than a tree.  
Generate -And-Test And Planning  
Exhaustive generate -and-test is very useful for simple problems. But for complex problems even 
heuristic generate -and-test is not very effective technique. But this may be made effective by 
combining with other techniques in such a way that the space in which to search is rest ricted. An 
AI program DENDRAL, for example, uses plan -Generate -and-test technique. First, the planning 
process uses constraint -satisfaction techniques and creates lists of recommended and 
contraindicated substructures. Then the generate -and-test procedure uses the lists generated and 
required to explore only a limited set of structures. Constrained in this way, generate -and-test 
proved highly effective. A major weakness of planning is that it often produces inaccurate 
solutions as there is no feedback from the world. But if it is used to produce only pieces of 
solutions then lack of detailed accuracy becomes unimportant.  
 
39 
 Hill Climbing  
Hill Climbing is heuristic search used for mathematical optimization problems in the field of 
Artificial Intelligence .  
Give n a large set of inputs and a good heuristic function, it tries to find a sufficiently good 
solution to the problem. This solution may not be the global optimal maximum.  
 In the above definition,  mathematical optimization problems  implies that hill climbing  
solves the problems where we need to maximize or minimize a given real function by 
choosing values from the given inputs. Example -Travelling salesman problem  where we 
need to  minimize the distance traveled by salesman.  
 ‘Heuristic search’ means that this search algorithm may not find the optimal solution to 
the problem. However, it will give a good solution in  reasonable time.  
 A heuristic function  is a function that will rank a ll the possible alternatives at any 
branching step in search algorithm based on the available information. It helps the 
algorithm to select the best route out of possible routes.  
Features of Hill Climbing  
1. Variant of generate and test algorithm :  It is a va riant of generate and test algorithm. The 
generate and test algorithm is as follows :  
 
1. Generate a possible solutions.  
2. Test to see if this is the expected solution.  
3. If the solution has been found quit else go to step 1.  
Hence we call Hill climbing as a variant of generate and test algorithm as it takes the feedback 
from test procedure. Then this feedback is utilized by the generator in deciding the next move in 
search space.  
2. Uses the  Greedy approach  : At any point in state space, the search moves in  that direction 
only which optimizes the cost of function with the hope of finding the optimal solution at 
the end.  
Types of Hill Climbing  
 
 
 
1. Simple Hill climbing :  It examines the neighboring nodes one by one and selects the first 
neighboring node which o ptimizes the current cost as next node.  
Algorithm for Simple Hill climbing  : 
Step 1 :  Evaluate the initial state. If it is a goal state then stop and return success. Otherwise, 
make initial state as current state.  
Step 2 :  Loop until the solution state is found or there are no new operators present which can be 
applied to current state.  
a) Select a state that has not been yet applied to the current state and apply it to produce a new 
state.  
b) Perform these to evaluate new state  
    i. If the current state is a goal state, then stop and return success.  
    ii. If it is better than the current state, then make it current state and proceed further.  
    iii. If it is not better than the current state, then continue in the loop until a solution is found.  
 
Step 3  : Exit. 
40 
 2. Steepest -Ascent Hill climbing :  It first examines all the neighboring nodes and then 
selects the node closest to the solution state as next node.  
 
Step 1 :  Evaluate the initial state. If it is goal state then exit else make the current state as in itial 
state  
Step 2 :  Repeat these steps until a solution is found or current state does not change  
i. Let ‘target’ be a state such that any successor of the current state will be better than it;  
ii. for each operator that applies to the current state  
     a. apply the new operator and create a new state  
     b. evaluate the new state  
     c. if this state is goal state then quit else compare with ‘target’  
     d. if this state is better than ‘target’, set this state as ‘target’  
     e. if target is better t han current state set current state to Target  
Step 3 :  Exit 
3. Stochastic hill climbing :  It does not examine all the neighboring nodes before deciding 
which node to select .It just selects a neighboring node at random, and decides (based on 
the amount of imp rovement in that neighbor) whether to move to that neighbor or to 
examine another.  
State Space diagram for Hill Climbing  
State space diagram is a graphical representation of the set of states our search algorithm can 
reach vs the value of our objective fun ction(the function which we wish to maximize).  
X-axis :  denotes the state space ie states or configuration our algorithm may reach.  
Y-axis :  denotes the values of objective function corresponding to to a particular state.  
The best solution will be that sta te space where objective function has maximum value(global 
maximum).  
 
Different regions in the State Space Diagram  
1. Local maximum :  It is a state which is better than its neighbori ng state however there 
exists a state which is better than it(global maximum). This state is better because here 
value of objective function is higher than its neighbors.  
41 
 2. Global maximum :  It is the best possible state in the state space diagram. This becau se at 
this state, objective function has highest value.  
3. Plateua/flat local maximum :  It is a flat region of state space where neighboring states 
have the same value.  
4. Ridge :  It is region which is higher than its neighbours but itself has a slope. It is a s pecial 
kind of local maximum.  
5. Current state :  The region of state space diagram where we are currently present during 
the search.  
6. Shoulder :  It is a plateau that has an uphill edge.  
Problems in different regions in Hill climbing  
Hill climbing cannot reach the optimal/best state(global maximum) if it enters any of the 
following regions :  
1. Local maximum :  At a local maximum all neighboring states have a values which is 
worse than than the current state. Since hill climbing uses greedy approach, it will not 
move to the worse state and terminate itself. The process will end even though a better 
solution may exist.  
To overcome local maximum problem :  Utilize  backtracking technique . Maintain a list of 
visited states. If the search reaches an undesirable state, it c an backtrack to the previous 
configuration and explore a new path.  
2. Plateau :  On plateau all neighbors have same value . Hence, it is not possible to select the 
best direction.  
To overcome plateaus :  Make a big jump. Randomly select a state far away from cu rrent state. 
Chances are that we will land at a non -plateau region  
3. Ridge :  Any point on a ridge can look like peak because movement in all possible 
directions is downward. Hence the algorithm stops when it reaches this state.  
To overcome Ridge :  In this ki nd of obstacle, use two or more rules before testing. It 
implies moving in several directions at once.  
 
Best First Search (Informed Search)  
 
In BFS and DFS, when we are at a node, we can consider any of the adjacent as next 
node. So both BFS and DFS blindl y explore paths without considering any cost function. The 
idea of Best First Search is to use an evaluation function to decide which adjacent is most 
promising and then explore. Best First Search falls under the category of Heuristic Search or 
Informed Se arch.  
We use a priority queue to store costs of nodes. So the implementation is a variation of BFS, we 
just need to change Queue to PriorityQueue.  
 
Algorithm:  
Best-First-Search (Grah g, Node start)  
    1) Create an empty PriorityQueue  
       PriorityQueue pq; 
    2) Insert "start" in pq.  
       pq.insert(start)  
    3) Until PriorityQueue is empty  
          u = PriorityQueue.DeleteMin  
42 
           If u is the goal  
             Exit 
          Else 
             Foreach neighbor v of u  
                If v "Unvisit ed" 
                    Mark v "Visited"                     
                    pq.insert(v)  
             Mark v "Examined"                     
End procedure  
Let us consider below example.  
 
 
 
 
We start from source "S" and search for  
goal "I" using given costs and Best  
First search.  
 
pq initially contains S  
We remove s from and process unvisited  
neighbors of S to pq.  
pq now contains {A, C, B} (C is put  
before B because C has lesser cost)  
 
We remove A from pq and process unvisited  
neighbors of A to pq.  
pq now contains {C, B, E, D}  
43 
  
We remove C from pq and process unvisite d 
neighbors of C to pq.  
pq now contains {B, H, E, D}  
 
We remove B from pq and process unvisited  
neighbors of B to pq.  
pq now contains {H, E, D, F, G}  
 
We remove H from pq.  Since our goal  
"I" is a neighbor of H, we return.  
Analysis :  
 The worst case time co mplexity for Best First Search is O(n * Log n) where n is number 
of nodes. In worst case, we may have to visit all nodes before we reach goal. Note that 
priority queue is implemented using Min(or Max) Heap, and insert and remove 
operations take O(log n) ti me. 
 Performance of the algorithm depends on how well the cost or evaluation function is 
designed.  
 
A* Search Algorithm  
 
A* is a type of search algorithm. Some problems can be solved by representing the world in the 
initial state, and then for each action w e can perform on the world we generate states for what the 
world would be like if we did so. If you do this until the world is in the state that we specified as 
a solution, then the route from the start to this goal state is the solution to your problem.  
 
In this tutorial I will look at the use of state space search to find the shortest path between two 
points (pathfinding), and also to solve a simple sliding tile puzzle (the 8 -puzzle). Let's look at 
some of the terms used in Artificial Intelligence when de scribing this state space search.  
 
Some terminology  
 
A node  is a state that the problem's world can be in. In pathfinding a node would be just a 2d 
coordinate of where we are at the present time. In the 8 -puzzle it is the positions of all the tiles.  
Next a ll the nodes are arranged in a  graph  where links between nodes represent valid steps in 
solving the problem. These links are known as  edges . In the 8 -puzzle diagram the edges are 
shown as blue lines. See figure 1 below.  
State space search , then, is solving  a problem by beginning with the start state, and then for each 
node we expand all the nodes beneath it in the graph by applying all the possible moves that can 
be made at each point.  
 
Heuristics and Algorithms  
 
At this point we introduce an important conc ept, the  heuristic . This is like an algorithm, but with 
a key difference. An algorithm is a set of steps which you can follow to solve a problem, which 
always works for valid input. For example you could probably write an algorithm yourself for 
44 
 multiplying  two numbers together on paper. A heuristic is not guaranteed to work but is useful in 
that it may solve a problem for which there is no algorithm.  
We need a heuristic to help us cut down on this huge search problem. What we need is to use our 
heuristic at  each node to make an estimate of how far we are from the goal. In pathfinding we 
know exactly how far we are, because we know how far we can move each step, and we can 
calculate the exact distance to the goal.  
But the 8 -puzzle is more difficult. There is no known algorithm for calculating from a given 
position how many moves it will take to get to the goal state. So various heuristics have been 
devised. The best one that I know of is known as the Nilsson score which leads fairly directly to 
the goal most o f the time, as we shall see.  
 
Cost 
 
When looking at each node in the graph, we now have an idea of a heuristic, which can estimate 
how close the state is to the goal. Another important consideration is the cost of getting to where 
we are. In the case of pa thfinding we often assign a movement cost to each square. The cost is 
the same then the cost of each square is one. If we wanted to differentiate between terrain types 
we may give higher costs to grass and mud than to newly made road. When looking at a nod e we 
want to add up the cost of what it took to get here, and this is simply the sum of the cost of this 
node and all those that are above it in the graph.  
 
8 Puzzle  
Let's look at the 8 puzzle in more detail. This is a simple sliding tile puzzle on a 3*3 g rid where 
one tile is missing and you can move the other tiles into the gap until you get the puzzle into the 
goal position. See figure 1.  
 
Figure 1 : The 8 -Puzzle state space for a very simple example   
 
There are 362,880 different states that the puzzle can be in, and to find a solution the search has 
to find a route through them. From most positions of the search the number of edges (that's the 
45 
 blue lines) is two. That means that the number of nodes you have in each level of the search is 
2^d where d is the depth. If the number of steps to solve a particular state is 18, then that �s 
262,144 nodes just at that level.  
 
The 8 puzzle game state is as simple as representing a list of the 9 squares and what's in them. 
Here are two states for example; the last one is the GOAL state, at which point we've found the 
solution. The first is a jumbled up example that you may start from.  
 
Start state SPACE, A, C, H, B, D, G, F, E  
Goal state A, B, C, H, SPACE, D, G, F, E  
The rules that you can apply to the puzzle are also simple. If there is a blank tile above, below, to 
the left or to the right o f a given tile, then you can move that tile into the space. To solve the 
puzzle you need to find the path from the start state, through the graph down to the goal state.  
 
There is example code to to solve the 8 -puzzle on the  github  site. 
 
 
Pathfinding  
In a video game, or some other pathfinding scenario, you want to search a state space and find 
out how to get from somewhere you are to somewhere you want to be, without bu mping into 
walls or going too far. For reasons we will see later, the A* algorithm will not only find a path, if 
there is one, but it will find the shortest path. A state in pathfinding is simply a position in the 
world. In the example of a maze game like Pacman you can represent where everything is using 
a simple 2d grid. The start state for a ghost say, would be the 2d coordinate of where the ghost is 
at the start of the search. The goal state would be where pacman is so we can go and eat him. 
There is al so example code to do pathfinding on the  github  site. 
 
Figure 2 : The first three steps of  a pathfinding state space  
 
 
46 
 Implementing A*  
We are now ready to look at the operation of the A* algorithm. What we need to do is start with 
the goal state and then generate the graph downwards from there. Let's take the 8 -puzzle in 
figure 1. We ask how ma ny moves can we make from the start state? The answer is 2, there are 
two directions we can move the blank tile, and so our graph expands.  
If we were just to continue blindly generating successors to each node, we could potentially fill 
the computer's memo ry before we found the goal node. Obviously we need to remember the best 
nodes and search those first. We also need to remember the nodes that we have expanded 
already, so that we don't expand the same state repeatedly.  
Let's start with the OPEN list. This  is where we will remember which nodes we haven't yet 
expanded. When the algorithm begins the start state is placed on the open list, it is the only state 
we know about and we have not expanded it. So we will expand the nodes from the start and put 
those o n the OPEN list too. Now we are done with the start node and we will put that on the 
CLOSED list. The CLOSED list is a list of nodes that we have expanded.  
 
f = g + h  
 
Using the OPEN and CLOSED list lets us be more selective about what we look at next in t he 
search. We want to look at the best nodes first. We will give each node a score on how good we 
think it is. This score should be thought of as the cost of getting from the node to the goal plus 
the cost of getting to where we are. Traditionally this has  been represented by the letters f, g and 
h. 'g' is the sum of all the costs it took to get here, 'h' is our heuristic function, the estimate of 
what it will take to get to the goal. 'f' is the sum of these two. We will store each of these in our 
nodes.  
Using the f, g and h values the A* algorithm will be directed, subject to conditions we will look 
at further on, towards the goal and will find it in the shortest route possible.  
 
So far we have looked at the components of the A*, let's see how they all fit together to make the 
algorithm :  
 
Pseudocode  
 
 
Hopefully the ideas we looked at in the preceding paragraphs will now click into place as we 
look at the A* algorithm pseudocode. You may find it helpful t o print this out or leave the 
window open while we discuss it.  
 
To help make the operation of the algorithm clear we will look again at the 8 -puzzle problem in 
figure 1 above. Figure 3 below shows the f,g and h scores for each of the tiles.  
47 
 
 
Figure 3 : 8 -Puzzle state space showing f,g,h scores   
 
First of all look at the g score for each node. This is the cost of what it took to get from the start 
to that node. So in the picture the  center number is g. As you can see it increases by one at each 
level. In some problems the cost may vary for different state changes. For example in 
pathfinding there is sometimes a type of terrain that costs more than other types.  
Next look at the last n umber in each triple. This is h, the heuristic score. As I mentioned above I 
am using a heuristic known as Nilsson's Sequence, which converges quickly to a correct solution 
in many cases. Here is how you calculate this score for a given 8 -puzzle state :  
 
Advantages:  
  
It is complete and optimal.  
  
It is the best one from other techniques. It is used to solve very complex problems.  
  
It is optimally efficient, i.e. there is no other optimal algorithm guaranteed to expand fewer nodes 
than A*.  
  
Disadvantages:  
  
This algorithm is complete if the branching factor is finite and every action has fixed cost.  
  
  
  
The speed execution of A* search is highly dependant on the accuracy of the heuristic algorithm 
that is used to compute h (n).  
48 
  
 
AO* Search: (And -Or) Gra ph 
  
The Depth first search and Breadth first search given earlier for OR trees or graphs can be easily 
adopted by AND -OR graph. The main difference lies in the way termination conditions are 
determined, since all goals following an AND nodes must be reali zed; where as a single goal 
node following an OR node will do. So for this purpose we are using AO* algorithm.  
  
Like A* algorithm here we will use two arrays and one heuristic function.  
  
OPEN:  
  
It contains the nodes that has been traversed but yet not b een marked solvable or unsolvable.  
  
CLOSE : 
  
It contains the nodes that have already been processed.  
  
6 7:The distance from current node to goal node.  
  
  
  
Algorithm:  
  
Step 1:  Place the starting node into OPEN.  
  
Step 2:  Compute the most promising solu tion tree say T0.  
  
Step 3:  Select a node n that is both on OPEN and a member of T0. Remove it from OPEN and 
place it in  
  
CLOSE  
  
Step 4:  If n is the terminal goal node then leveled n as solved and leveled all the ancestors of n 
as solved.  If the starting  node is marked as solved then success and exit.  
  
Step 5:  If n is not a solvable node, then mark n as unsolvable. If starting node is marked as 
unsolvable,  then return failure and exit.  
  
Step 6:  Expand n. Find all its successors and find their h (n) valu e, push them into OPEN.  
  
Step 7:  Return to Step 2.  
  
Step 8:  Exit.  
  
49 
 Implementation:  
  
Let us take the following example to implement the AO* algorithm.  
 
  
Step 1:  
  
In the above graph, the solvable nodes are A, B, C, D, E, F and the unsolvable nodes are G, H. 
Take A as the starting node. So place A into OPEN.  
  
 
 
 
50 
 
 
 
51 
 
 
 
52 
 
 
 
 
  
Advantages:  
  
It is an optimal algorithm.  
  
If traverse according to the ordering of nodes. It can be used for both OR and AND graph.  
  
Disadvantages:  
  
Sometimes for unsolvable nodes, it can’t find the optimal path. Its complexity is than other 
algorithms.  
 
PROBLEM REDUCTION   
 
Problem Reduction with AO* Algorithm.  
 
When a problem can be divided into a set of sub problems, where each sub problem can be 
solved separately and a combination of these will be a solution, AND -OR graphs or AND - OR 
trees are used for representing the solution. The decomposition of the problem or problem 
reduction generates AND arcs. One AND are may point to any number of successor nodes. All 
53 
 these must be solved so that the arc will rise to many arcs, indicating several po ssible solutions. 
Hence the graph is known as AND - OR instead of AND. Figure shows an AND - OR graph.  
 
 
An algorithm to find a solution in an AND - OR graph must handle AND area appropriately. A* 
algorithm can not search AND - OR graphs efficiently. Thi s can be understand from the give 
figure.  
 
FIGURE : AND - OR graph  
 
In figure (a) the top node A has been expanded producing two area one leading to B and leading 
to C-D . the numbers at each node represent the value of f ' at that node (cost of getting to the 
goal state from current state). For simplicity, it is assumed that every operation(i.e. applying a 
rule) has unit cost, i.e., each are with single successor will have a cost of 1 and each of its 
components. With the available information till now , it appears that C is the most promising 
node to expand since its f ' = 3 , the lowest but going through B would be better since to use C 
we must also use D' and the cost would be 9(3+4+1+1). Through B it would be 6(5+1).  
 
Thus the choice of the next node t o expand depends not only n a value but also on whether that 
node is part of the current best path form the initial mode. Figure (b) makes this clearer. In figure 
the node G appears to be the most promising node, with the least f ' value. But G is not on t he 
current beat path, since to use G we must use GH with a cost of 9 and again this demands that 
arcs be used (with a cost of 27). The path from A through B, E -F is better with a total cost of 
(17+1=18). Thus we can see that to search an AND -OR graph, the following three things must 
be done.  
1. traverse the graph starting at the initial node and following the current best path, and 
accumulate the set of nodes that are on the path and have not yet been expanded.  
 
2. Pick one of these unexpanded nodes and exp and it. Add its successors to the graph and 
computer f ' (cost of the remaining distance) for each of them.  
 
54 
 3. Change the f ' estimate of the newly expanded node to reflect the new information produced 
by its successors. Propagate this change backward thr ough the graph. Decide which of the 
current best path.  
 
The propagation of revised cost estimation backward is in the tree is not necessary in A* 
algorithm. This is because in AO* algorithm expanded nodes are re -examined so that the current 
best path can b e selected. The working of AO* algorithm is illustrated in figure as follows:  
 
Referring the figure. The initial node is expanded and D is Marked initially as promising node. D 
is expanded producing an AND arc E -F. f ' value of D is updated to 10. Going backwards we can 
see that the AND arc B -C is better . it is now marked as current best path. B and C have to be 
expanded next. This process continues until a solution is found or all paths have led to dead ends, 
indicating that there is no solution. An A* algorithm the path from one node to the other is 
always that of the lowest cost and it is independent of the paths through other nodes.  
 
The algorithm for performing a heuristic search of an AND - OR graph is given below. Unlike 
A* algorithm which used two  lists OPEN and CLOSED, the AO* algorithm uses a single 
structure G. G represents the part of the search graph generated so far. Each node in G points 
down to its immediate successors and up to its immediate predecessors, and also has with it the 
value of h' cost of a path from itself to a set of solution nodes. The cost of getting from the start 
nodes to the current node "g" is not stored as in the A* algorithm. This is because it is not 
possible to compute a single such value since there may be many paths  to the same state. In AO* 
algorithm serves as the estimate of goodness of a node. Also a there should value called 
FUTILITY is used. The estimated cost of a solution is greater than FUTILITY then the search is 
abandoned as too expansive to be practical.  
For representing above graphs AO* algorithm is as follows  
 
AO* ALGORITHM:  
1. Let G consists only to the node representing the initial state call this node INTT. Compute   
    h' (INIT).  
 
2. Until INIT is labeled SOLVED or hi (INIT) becomes greater than FUTIL ITY, repeat the   
    following procedure.  
55 
  
(I)     Trace the marked arcs from INIT and select an unbounded node NODE.  
 
(II)  Generate the successors of NODE . if there are no successors then assign FUTILITY as   
        h' (NODE). This means that NODE is no t solvable. If there are successors then for each 
one    
        called SUCCESSOR, that is not also an ancester of NODE do the following  
 
 
            (a) add SUCCESSOR to graph G  
 
            (b) if successor is not a terminal node, mark it solved and ass ign zero to its h ' value.  
 
            (c) If successor is not a terminal node, compute it h' value.  
 
(III) propagate the newly discovered information up the graph by doing the following . let S be a   
        set of nodes that have been marked SOLVED. Ini tialize S to NODE. Until S is empty 
repeat   
        the following procedure;  
 
           (a) select a node from S call if CURRENT and remove it from S.  
 
          (b) compute h' of each of the arcs emerging from CURRENT , Assign minimum h' to      
               CURRENT.  
 
          (c) Mark the minimum cost path a s the best out of CURRENT.  
 
          (d) Mark CURRENT SOLVED if all of the nodes connected to it through the new marked   
               are have been labeled SOLVED.  
 
          (e) If CURRENT has been marked SOLVED or its h ' has just changed, its new status 
must   
               be propagate backwards up the graph . hence all the ancestors of CURRENT are added   
               to S. 
(Refered From Artificial Intelligence TMH)  
 
AO*  Search Procedure.  
 
1. Place the start node on open.  
 
2. Using the search tree, compute the most promising solution tree TP .  
 
3. Select node n that is both on open and a part of tp, remove n from open and place it no closed.  
 
4. If n is a goal node, label n as solved. If th e start node is solved, exit with success where tp is 
the solution tree, remove all nodes from open with a solved ancestor.  
56 
  
5. If n is not solvable node, label n as unsolvable. If the start node is labeled as unsolvable, exit 
with failure. Remove all node s from open ,with unsolvable ancestors.  
 
6. Otherwise, expand node n generating all of its successor compute the cost of for each newly 
generated node and place all such nodes on open.  
 
7. Go back to step(2)  
 
Note: AO* will always find minimum cost solutio n. 
 
CONSTRAINT SATISFACTION: - 
 
Many problems in AI can be considered as problems of constraint satisfaction, in which the goal 
state satisfies a given set of constraint. constraint satisfaction problems can be solved by using 
any of the search strategies. The general form of the constraint satisfaction procedure is as 
follows:  
 
Until a complete solution is found or until all paths have led to lead ends, do  
 
1. select an unexpanded node of the search graph.  
 
2. Apply the constraint inference rules to the sel ected node to generate all possible new 
constraints.  
 
3. If the set of constraints contains a contradiction, then report that this path is a dead end.  
 
4. If the set of constraints describes a complete solution then report success.  
 
5. If neither a constra int nor a complete solution has been found then apply the rules to generate 
new partial solutions. Insert these partial solutions into the search graph.  
 
Example: consider the crypt arithmetic problems.  
 
    SEND  
+ MORE  
----------  
MONEY  
----------  
 
Assign decimal digit to each of the letters in such a way that the answer to the problem is correct 
to the same letter occurs more than once , it must be assign the same digit each time . no two 
different letters may be assigned the same digit. Consider the crypt  arithmetic problem.  
 
 
 
57 
    SEND  
+ MORE  
-----------  
MONEY  
-----------  
 
CONSTRAINTS: - 
 
1. no two digit can be assigned to same letter.  
 
2. only single digit number can be assign to a letter.  
 
1. no two letters can be assigned same digit.  
 
2. Assumption can b e made at various levels such that they do not contradict each other.  
 
3. The problem can be decomposed into secured constraints. A constraint satisfaction approach 
may be used.  
 
4. Any of search techniques may be used.  
 
5. Backtracking may be performed as  applicable us applied search techniques.  
 
6. Rule of arithmetic may be followed.  
 
Initial state of problem.  
D=? 
E=? 
Y=? 
N=? 
R=? 
O=? 
S=? 
M=?  
C1=?  
C2=?  
C1 ,C 2, C3 stands for the carry variables respectively.  
 
Goal State: the digits to the letters must be a ssigned in such a manner so that the sum is satisfied.  
 
 
Solution Process:  
 
We are following the depth -first method to solve the problem.  
 
1. initial guess m=1 because the sum of two single digits can generate at most a carry '1'.  
58 
  
2. When n=1 o=0 or 1 bec ause the largest single digit number added to m=1 can generate the 
sum of either 0 or 1 depend on the carry received from the carry sum. By this we conclude that 
o=0 because m is already 1 hence we cannot assign same digit another letter(rule no.)  
 
3. We h ave m=1 and o=0 to get o=0 we have s=8 or 9, again depending on the carry received 
from the earlier sum.  
 
The same process can be repeated further. The problem has to be composed into various 
constraints. And each constraints is to be satisfied by guessing  the possible digits that the letters 
can be assumed that the initial guess has been already made . rest of the process is being shown 
in the form of a tree, using depth -first search for the clear understandability of the solution 
process.  
59 
  
 
            D>6(Controduction)  
60 
 
 
  
    
 
 
61 
 MEANS - ENDS ANALYSIS: - 
 
Most of the search strategies either reason forward of backward however, often a  mixture o the 
two directions is appropriate. Such mixed strategy would make it possible to solve the major 
parts of problem first and solve the smaller problems the arise when combining them together. 
Such a technique is called "Means - Ends Analysis".  
 
The means -ends analysis process centers around finding the difference between current state and 
goal state. The problem space of means - ends analysis has an initial state and one or more goal 
state, a set of operate with a set of preconditions their appli cation and difference functions that 
computes the difference between two state a(i) and s(j). A problem is solved using means - ends 
analysis by  
 
 
1. Computing the current state s1 to a goal state s2 and computing their difference D12.  
 
2. Satisfy the prec onditions for some recommended operator op is selected, then to reduce the 
difference D12.  
 
3. The operator OP is applied if possible. If not the current state is solved a goal is created and 
means - ends analysis is applied recursively to reduce the sub go al. 
 
4. If the sub goal is solved state is restored and work resumed on the original problem.  
 
( the first AI program to use means - ends analysis was the GPS General problem solver)  
 
means - ends analysis I useful for many human planning activities. Consid er the example of 
planing for an office worker. Suppose we have a different table of three rules:  
 
1. If in out current state we are hungry , and in our goal state we are not hungry , then either the 
"visit hotel" or "visit Canteen " operator is recommende d. 
 
2. If our current state we do not have money , and if in your goal state we have money, then the 
"Visit our bank" operator or the "Visit secretary" operator is recommended.  
 
3. If our current state we do not know where something is , need in our goal s tate we do know, 
then either the "visit office enquiry" , "visit secretary" or "visit co worker " operator is 
recommended.  
 
 
 
 
 
 
 
 
62 
 KNOWLEDGE REPRESENTATION  
 
KNOWLEDGE REPRESENTATION: - 
 
For the purpose of solving complex problems c \encountered in AI, we nee d both a large amount 
of knowledge and some mechanism for manipulating that knowledge to create solutions to new 
problems. A variety of ways of representing knowledge (facts) have been exploited in AI 
programs. In all variety of knowledge representations ,  we deal with two kinds of entities.  
 
A. Facts: Truths in some relevant world. These are the things we want to represent.  
 
B. Representations of facts in some chosen formalism . these are things we will  
 
actually be able to manipulate.  
 
One way to think of  structuring these entities is at two levels : (a) the knowledge level, at which 
facts are described, and (b) the symbol level, at which representations of objects at the 
knowledge level are defined in terms of symbols that can be manipulated by programs.  
 
The facts and representations are linked with two -way mappings. This link is called 
representation mappings. The forward representation mapping maps from facts to 
representations. The backward representation mapping goes the other way, from representation s 
to facts.  
 
One common representation is natural language (particularly English) sentences. Regardless of 
the representation for facts we use in a program , we may also need to be concerned with an 
English representation of those facts in order to facilit ate getting information into and out of the 
system. We need mapping functions from English sentences to the representation we actually use 
and from it back to sentences.  
Representations and Mappings  
 In order to solve complex problems encountered in artific ial intelligence, one needs both 
a large amount of knowledge and some mechanism for manipulating that knowledge to 
create solutions.  
 Knowledge and Representation are two distinct entities. They play central but 
distinguishable roles in the intelligent syst em. 
 Knowledge is a description of the world. It determines a system’s competence by what it 
knows.  
 Moreover, Representation is the way knowledge is encoded. It defines a system’s 
performance in doing something.  
 Different types of knowledge require differen t kinds of representation.  
63 
 
 
Fig: Mapping between Facts and Representations  
The Knowledge Representation m odels/mechanisms are often based on:  
 Logic  
 Rules  
 Frames  
 Semantic Net  
Knowledge is categorized into two major types:  
1. Tacit corresponds to “informal” or “implicit“  
 Exists within a human being;  
 It is embodied.  
 Difficult to articulate formally.  
 Difficult to co mmunicate or share.  
 Moreover, Hard to steal or copy.  
 Drawn from experience, action, subjective insight  
2. Explicit formal type of knowledge, Explicit  
 Explicit knowledge  
 Exists outside a human being;  
 It is embedded.  
 Can be articulated formally.  
 Also, Can be sh ared, copied, processed and stored.  
 So, Easy to steal or copy  
 Drawn from the artifact of some type as a principle, procedure, process, concepts.  
A variety of ways of representing knowledge have been exploited in AI programs.  
There are two different kinds o f entities, we are dealing with.  
1. Facts: Truth in some relevant world. Things we want to represent.  
2. Also, Representation of facts in some chosen formalism. Things we will actually be able 
to manipulate.  
These entities structured at two levels:  
1. The knowledge  level, at which facts described.  
2. Moreover, The symbol level, at which representation of objects defined in terms of 
symbols that can manipulate by programs  
Framework of Knowledge Representation  
 The computer requires a well -defined problem description to p rocess and provide a well -
defined acceptable solution.  
64 
  Moreover, To collect fragments of knowledge we need first to formulate a description in 
our spoken language and then represent it in formal language so that computer can 
understand.  
 Also, The computer can then use an algorithm to compute an answer.  
So, This process illustrated as,  
 
Fig: Knowledge  Representation Framework  
The steps are:  
 The informal formalism of the problem takes place first.  
 It then represented formally and the computer produces an output.  
 This output can then represented in an informally described solution that user understands 
or checks for consistency.  
The Problem solving requires,  
 Formal knowledge representation, and  
 Moreover, Conversion of informal knowledge to a formal knowledge that is the 
conversion of implicit knowledge to explicit knowledge.  
Mapping between Facts and Repr esentation  
 Knowledge is a collection of facts from some domain.  
 Also, We need a representation of “facts“ that can manipulate by a program.  
 Moreover, Normal English is insufficient, too hard currently for a computer program to 
draw inferences in natural la nguages.  
 Thus some symbolic representation is necessary.  
A good knowledge representation enables fast and accurate access to knowledge and 
understanding of the content.  
A knowledge representation system should have following properties.  
1. Representational Ad equacy  
 The ability to represent all kinds of knowledge that are needed in that domain.  
2. Inferential Adequacy  
 Also, The ability to manipulate the representational structures to derive new 
structures corresponding to new knowledge inferred from old.  
3. Inferenti al Efficiency  
 The ability to incorporate additional information into the knowledge structure that 
can be used to focus the attention of the inference mechanisms in the most 
promising direction.  
4. Acquisitional Efficiency  
 Moreover, The ability to acquire new knowledge using automatic methods 
wherever possible rather than reliance on human intervention.  
65 
 Knowledge Representation Schemes  
Relational Knowledge  
 The simplest way to represent declarative facts is a set of relations of the same sort used 
in the databas e system.  
 Provides a framework to compare two objects based on equivalent attributes. o Any 
instance in which two different objects are compared is a relational type of knowledge.  
 The table below shows a simple way to store facts.  
 Also, The facts about a s et of objects are put systematically in columns.  
 This representation provides little opportunity for inference.  
 
 Given the facts, it is not possible to answer a simple question such as: “Who is the 
heaviest player?”  
 Also, But if a procedure for finding the heaviest player is provided, then these facts will 
enable that procedure to compute an answer.  
 Moreover, We can  ask things like who “bats – left” and “throws – right”.  
Inheritable Knowledge  
 Here the knowledge elements inherit attributes from their parents.  
 The knowledge embodied in the design hierarchies found in the functional, physical and 
process domains.  
 Within  the hierarchy, elements inherit attributes from their parents, but in many cases, not 
all attributes of the parent elements prescribed to the child elements.  
 Also, The inheritance is a powerful form of inference, but not adequate.  
 Moreover, The basic KR ( Knowledge Representation) needs to augment with inference 
mechanism.  
 Property inheritance: The objects or elements of specific classes inherit attributes and 
values from more general classes.  
 So, The classes organized in a generalized hierarchy.  
66 
 
 
 Boxed nodes — objects and values of attributes of objects.  
 Arrows — the point from object to its value.  
 This structure is  known as a slot and filler structure, semantic network or a collection of 
frames.  
The steps to retrieve a value for an attribute of an instance object:  
1. Find the object in the knowledge base  
2. If there is a value for the attribute report it  
3. Otherwise look fo r a value of an instance, if none fail  
4. Also, Go to that node and find a value for the attribute and then report it  
5. Otherwise, search through using is until a value is found for the attribute.  
Inferential Knowledge  
 This knowledge generates new information f rom the given information.  
 This new information does not require further data gathering form source but does 
require analysis of the given information to generate new knowledge.  
 Example: given a set of relations and values, one may infer other values or re lations. A 
predicate logic (a mathematical deduction) used to infer from a set of attributes. 
Moreover, Inference through predicate logic uses a set of logical operations to relate 
individual data.  
 Represent knowledge as formal logic: All dogs have tails   ∀x: dog(x) → hastail(x)  
 Advantages:  
 A set of strict rules.  
 Can use to derive more facts.  
 Also, Truths of new statements can be verified.  
 Guaranteed correctness.  
 So, Many inference procedures available to implement standard rules of logic popular in 
AI syst ems. e.g Automated theorem proving.  
67 
 Procedural Knowledge  
 A representation in which the control information, to use the knowledge, embedded in the 
knowledge itself. For example, computer programs, directions, and recipes; these indicate 
specific use or impl ementation;  
 Moreover, Knowledge encoded in some procedures, small programs that know how to do 
specific things, how to proceed.  
 Advantages:  
 Heuristic or domain -specific knowledge can represent.  
 Moreover, Extended logical inferences, such as default reasoni ng facilitated.  
 Also, Side effects of actions may model. Some rules may become false in time. 
Keeping track of this in large systems may be tricky.  
 Disadvantages:  
 Completeness — not all cases may represent.  
 Consistency — not all deductions may be correct. e.g If we know that Fred is a 
bird we might deduce that Fred can fly. Later we might discover that Fred is an 
emu.  
 Modularity sacrificed. Changes in knowledge base might have far -reaching 
effects.  
 Cumbersome control information.  
 
USING PREDICATE LOGIC   
Representation of Simple Facts in Logic  
Propositional logic is useful because it is simple to deal with and a decision procedure for it 
exists.  
Also, In order to draw conclusions, facts are represented in a more convenient way as,  
1. Marcus is a man.  
 man(Marcus)  
2. Plato is a man.  
 man(Plato)  
3. All men are mortal.  
 mortal(men)  
But propositional logic fails to capture the relationship between an individual being a man and 
that individual being a mortal.  
 How can these sentences be represented so that we can infer the thir d sentence from the 
first two?  
 Also, Propositional logic commits only to the existence of facts that may or may not be 
the case in the world being represented.  
 Moreover, It has a simple syntax and simple semantics. It suffices to illustrate the process 
of inference.  
 Propositional logic quickly becomes impractical, even for very small worlds.  
Predicate logic  
First-order Predicate logic (FOPL) models the world in terms of  
 Objects, which are things with individual identities  
 Properties of objects that distingu ish them from other objects  
 Relations that hold among sets of objects  
68 
  Functions, which are a subset of relations where there is only one “value” for any given 
“input”  
First-order Predicate logic (FOPL) provides  
 Constants: a, b, dog33. Name a specific objec t. 
 Variables: X, Y. Refer to an object without naming it.  
 Functions: Mapping from objects to objects.  
 Terms: Refer to objects  
 Atomic Sentences: in(dad -of(X), food6) Can be true or false, Correspond to propositional 
symbols P, Q.  
A well -formed formula ( wff) is a sentence containing no “free” variables. So, That is, all 
variables are “bound” by universal or existential quantifiers.  
(∀x)P(x, y) has x bound as a universally quantified variable, but y is free.  
Quantifiers  
Universal quantification  
 (∀x)P(x) means that P holds for all values of x in the domain associated with that variable  
 E.g., (∀x) dolphin(x)  →  mammal(x)  
Existential quantification  
 (∃ x)P(x) means that P holds for some value of x in the domain associated with that 
variable  
 E.g., (∃ x) mammal(x)  ∧  lays-eggs(x)  
Also, Consider the following example that shows the use of predicate logic as a way of 
representing knowledge.  
1. Marcus was a man.  
2. Marcus was a Pompeian.  
3. All Pompeians were Romans.  
4. Caesar was a ruler.  
5. Also, All Pompeians were either loyal to Ca esar or hated him.  
6. Everyone is loyal to someone.  
7. People only try to assassinate rulers they are not loyal to.  
8. Marcus tried to assassinate Caesar.  
The facts described by these sentences can be represented as a set of well -formed formulas ( wffs) 
as follows:  
1. Marcus was a man.  
 man(Marcus)  
2. Marcus was a Pompeian.  
 Pompeian(Marcus)  
3. All Pompeians were Romans.  
 ∀x: Pompeian(x)  → Roman(x)  
4. Caesar was a ruler.  
 ruler(Caesar)  
5. All Pompeians were either loyal to Caesar or hated him.  
  inclusive -or 
 ∀x: Roman(x)  → loyalto(x, Ca esar)  ∨ hate(x, Caesar)  
 exclusive -or 
 ∀x: Roman(x)  → (loyalto(x, Caesar)  ∧¬ hate(x, Caesar))  ∨ 
 (¬loyalto(x, Caesar)  ∧ hate(x, Caesar))  
69 
 6. Everyone is loyal to someone.  
 ∀x: ∃y: loyalto(x, y)  
7. People only try to assassinate rulers they are not loyal to.  
 ∀x: ∀y: person(x)  ∧ ruler(y)  ∧ tryassassinate(x, y)  
 →¬loyalto(x, y)  
8. Marcus tried to assassinate Caesar.  
 tryassassinate(Marcus, Caesar)  
Now suppose if we want to use these statements to answer the question:  Was Marcus loyal to 
Caesar?  
Also, Now let’s try to produce a formal proof, reasoning backward from the desired goal: ¬ 
Ioyalto(Marcus, Caesar)  
In order to prove the goal, we need to use the rules of inference to transform it into another goal 
(or possibly a set of goals) that can, in turn, transformed, and so on, until there are no unsatisfied 
goals remaining.  
 
Figure: An attempt to prove ¬loyalto(Marcus, Caesar).   
 The prob lem is that, although we know that Marcus was a man, we do not have any way 
to conclude from that that Marcus was a person. Also, We need to add the representation 
of another fact to our system, namely : ∀ man(x) → person(x)  
 Now we can satisfy the last goal  and produce a proof that Marcus was not loyal to 
Caesar.  
 Moreover, From this simple example, we see that three important issues must be 
addressed in the process of converting English sentences into logical statements and then 
using those statements to ded uce new ones:  
1. Many English sentences are ambiguous (for example, 5, 6, and 7 above). 
Choosing the correct interpretation may be difficult.  
2. Also, There is often a choice of how to represent the knowledge. Simple 
representations are desirable, but they may e xclude certain kinds of reasoning.  
3. Similalry, Even in very simple situations, a set of sentences is unlikely to contain 
all the information necessary to reason about the topic at hand. In order to be able 
to use a set of statements effectively. Moreover, I t is usually necessary to have 
access to another set of statements that represent facts that people consider too 
obvious to mention.  
 
 
70 
 Representing Instance and ISA Relationships  
 
 Specific attributes  instance  and isa play an important role particularly in a useful form of 
reasoning called property inheritance.  
 The predicates instance and isa explicitly captured the relationships they used to express, 
namely class membership and class inclusion.  
 4.2 shows the first five sentences of the last section represen ted in logic in three different 
ways.  
 The first part of the figure contains the representations we have already discussed. In 
these representations, class membership represented with unary predicates (such as 
Roman), each of which corresponds to a class.  
 Asserting that P(x) is true is equivalent to asserting that x is an instance (or element) of P.  
 The second part of the figure contains representations that use the  instance  predicate 
explicitly.  
 
Figure: Three ways of representing class membership:  ISA Relationships  
 The predicate  instance  is a binary one, whose first argument is an object and wh ose 
second argument is a class to which the object belongs.  
 But these representations do not use an explicit  isa predicate.  
 Instead, subclass relationships, such as that between Pompeians and Romans, described 
as shown in sentence 3.  
 The implication rule s tates that if an object is an instance of the subclass Pompeian then it 
is an instance of the superclass Roman.  
 Note that this rule is equivalent to the standard set -theoretic definition of the subclass -
superclass relationship.  
 The third part contains repr esentations that use both the  instance  and isa predicates 
explicitly.  
 The use of the  isa predicate simplifies the representation of sentence 3, but it requires that 
one additional axiom (shown here as number 6) be provided.  
71 
 Computable Functions and Predica tes 
 To express simple facts, such as the following greater -than and less -than relationships: 
gt(1,O) It(0,1) gt(2,1)   It(1,2) gt(3,2)   It( 2,3)  
 It is often also useful to have computable functions as well as computable predicates. 
Thus we might want to be able to evaluate the truth of   gt(2 + 3,1)  
 To do so requires that we first compute the value of the plus function given the arguments 
2 and 3, and then send the arguments 5 and 1 to gt.  
Consider the following set of facts, again involving Marcus:  
1) Marcus  was a man.  
            man(Marcus)  
2) Marcus was a Pompeian.  
            Pompeian(Marcus)  
3) Marcus was born in 40 A.D.  
            born(Marcus, 40)  
4) All men are mortal.  
            x: man(x) → mortal(x)  
5) All Pompeians died when the volcano erupted in 79 A.D.  
         erupted(volcano, 79) ∧ ∀ x : [Pompeian(x) → died(x, 79)]  
6) No mortal lives longer than 150 years.  
             x: t1: At2:  mortal(x)   born(x, t1)   gt(t2 – t1,150) → died(x, t2)  
7) It is now 1991.  
            now = 1991  
So, Above example shows how these ideas of computable functions and predicates can be useful.  
It also makes use of the notion of equality and allows equal objects to be substituted for each 
other whenever it appears helpful to do so during a proof.  
 So, Now suppose we want to answer the question “Is Marcus alive?”  
 The statements suggested here, there may be two ways of deducing an answer.  
 Either we can show that Marcus is dead because he was ki lled by the volcano or we can 
show that he must be dead because he would otherwise be more than 150 years old, 
which we know is not possible.  
 Also, As soon as we attempt to follow either of those paths rigorously, however, we 
discover, just as we did in th e last example, that we need some additional knowledge. For 
example, our statements talk about dying, but they say nothing that relates to being alive, 
which is what the question is asking.  
So we add the following facts:  
8) Alive means not dead.  
            x: t: [alive(x, t) → ¬ dead(x, t)]  [¬ dead(x, t) → alive(x, t)]  
9) If someone dies, then he is dead at all later times.  
            x: t1: At2 : died(x, t1)   gt(t2, t1) → dead(x, t2)  
So, Now let’s attempt to answer the question “Is Marcus alive?” by prov ing: ¬ alive(Marcus, 
now)  
 
 
 
 
 
72 
  
Resolution  
Propositional Resolution  
1. Convert all the propositions of F to clause form.  
2. Negate P and convert the result to clause form. Add it to the set of clauses obtained in 
step 1.  
3. Repeat until either a contradiction is fo und or no progress can be made:  
1. Select two clauses. Call these the parent clauses.  
2. Resolve them together. The resulting clause, called the resolvent, will be the 
disjunction of all of the literals of both of the parent clauses with the following 
exception:  If there are any pairs of literals L and  ¬ L such that one of the parent 
clauses contains  L and the other contains  ¬L, then select one such pair and 
eliminate both  L and ¬ L from the resolvent.  
3. If the resolvent is the empty clause, then a contradiction ha s been found. If it is 
not, then add it to the set of classes available to the procedure.  
The Unification Algorithm  
 In propositional logic, it is easy to determine that two literals cannot both be true at the 
same time.  
 Simply look for L and ¬L in predicat e logic, this matching process is more complicated 
since the arguments of the predicates must be considered.  
 For example, man(John) and ¬man(John) is a contradiction, while the man(John) and 
¬man(Spot) is not.  
 Thus, in order to determine contradictions, we  need a matching procedure that compares 
two literals and discovers whether there exists a set of substitutions that makes them 
identical.  
 There is a straightforward recursive procedure, called the unification algorithm, that does 
it. 
Algorithm: Unify(L1, L2) 
1. If L1 or L2 are both variables or constants, then:  
1. If L1 and L2 are identical, then return NIL.  
2. Else if L1 is a variable, then if L1 occurs in L2 then return {FAIL}, else return 
(L2/L1).  
3. Also, Else if L2 is a variable, then if L2 occurs in L1 then retu rn {FAIL}, else 
return (L1/L2). d. Else return {FAIL}.  
2. If the initial predicate symbols in L1 and L2 are not identical, then return {FAIL}.  
3. If LI and L2 have a different number of arguments, then return {FAIL}.  
4. Set SUBST to NIL. (At the end of this procedu re, SUBST will contain all the 
substitutions used to unify L1 and L2.)  
5. For I ← 1 to the number of arguments in L1 :  
1. Call Unify with the ith argument of L1 and the ith argument of L2, putting the 
result in S.  
2. If S contains FAIL then return {FAIL}.  
3. If S is n ot equal to NIL then:  
2. Apply S to the remainder of both L1 and L2.  
3. SUBST: = APPEND(S, SUBST).  
6. Return SUBST.  
73 
 Resolution in Predicate Logic  
We can now state the resolution algorithm for predicate logic as follows, assuming a set of given 
statements F and a st atement to be proved P:  
Algorithm: Resolution  
1. Convert all the statements of F to clause form.  
2. Negate P and convert the result to clause form. Add it to the set of clauses obtained in 1.  
3. Repeat until a contradiction found, no progress can make, or a predete rmined amount of 
effort has expanded.  
1. Select two clauses. Call these the parent clauses.  
2. Resolve them together. The resolvent will the disjunction of all the literals of both 
parent clauses with appropriate substitutions performed and with the following 
exception: If there is one pair of literals T1 and ¬T2 such that one of the parent 
clauses contains T2 and the other contains T1 and if T1 and T2 are unifiable, then 
neither T1 nor T2 should appear in the resolvent. We call T1 and T2 
Complementary literals. Use the substitution produced by the unification to create 
the resolvent. If there is more than one pair of complementary literals, only one 
pair should omit from the resolvent.  
3. If the resolvent is an empty clause, then a contradiction has found. Moreover,  If it 
is not, then add it to the set of classes available to the procedure.  
 
Resolution Procedure  
 Resolution is a procedure, which gains its efficiency from the fact that it operates on 
statements that have been converted to a very convenient standard for m. 
 Resolution produces proofs by refutation.  
 In other words,  to prove a statement (i.e., to show that it is valid), resolution attempts to 
show that the negation of the statement produces a contradiction with the known 
statements (i.e., that it is unsatisf iable).  
 The resolution procedure is a simple iterative process: at each step, two clauses, called 
the parent clauses, are compared (resolved), resulting in a new clause that has inferred 
from them. The new clause represents ways that the two parent clauses  interact with each 
other. Suppose that there are two clauses in the system:  
winter  V summer  
             ¬ winter  V cold 
 Now we observe that precisely one of winter and ¬ winter will be true at any point.  
 If winter is true, then cold must be true to guara ntee the truth of the second clause. If ¬ 
winter is true, then summer must be true to guarantee the truth of the first clause.  
 Thus we see that from these two clauses we can deduce   summer V cold  
 This is the deduction that the resolution procedure will mak e. 
 Resolution operates by taking two clauses that each contains the same literal, in this 
example,  winter . 
 Moreover, The literal must occur in the positive form in one clause and in negative form 
in the other. The resolvent obtained by combining all of the  literals of the two parent 
clauses except the ones that cancel.  
 If the clause that produced is the empty clause, then a contradiction has found.  
For example, the two clauses  
            winter  
74 
             ¬ winter  
will produce the empty clause.  
 
Natural D eduction Using Rules  
 
Testing whether a proposition is a tautology by testing every possible truth assignment is 
expensive —there are exponentially many. We need a  deductive system , which will allow us to 
construct proofs of tautologies in a step -by-step fa shion.  
The system we will use is known as  natural deduction . The system consists of a set of  rules of 
inference  for deriving consequences from premises. One builds a proof tree whose root is the 
proposition to be proved and whose leaves are the initial ass umptions or axioms (for proof trees, 
we usually draw the root at the bottom and the leaves at the top).  
For example, one rule of our system is known as  modus ponens . Intuitively, this says that if we 
know P is true, and we know that P implies Q, then we ca n conclude Q.  
P P ⇒ Q 
Q 
 (modus ponens)  
The propositions above the line are called  premises ; the proposition below the line is 
the conclusion . Both the premises and the conclusion may contain metavariables (in this case, P 
and Q) representing arbitrary propositions. When  an inference rule is used as part of a proof, the 
metavariables are replaced in a consistent way with the appropriate kind of object (in this case, 
propositions).  
Most rules come in one of two flavors:  introduction  or elimination  rules. Introduction rules  
introduce the use of a logical operator, and elimination rules eliminate it. Modus ponens is an 
elimination rule for  ⇒. On the right -hand side of a rule, we often write the name of the rule. This 
is helpful when reading proofs. In this case, we have written (modus ponens). We could also 
have written  (⇒-elim)  to indicate that this is the elimination rule for  ⇒. 
Rules for Conjunction  
Conjunction  (∧) has an introduction rule and two elimination rules:  
P Q 
P ∧ Q 
 (∧-intro)  P ∧ Q 
P 
 (∧-elim-left) P ∧ Q 
Q 
 (∧-elim-right)  
Rule for T  
The simplest introduction rule is the one for T. It is called "unit". Because it has no pr emises, this 
rule is an  axiom : something that can start a proof.  
  
T 
 (unit)  
Rules for Implication  
In natural deduction, to prove an implication of the form P ⇒ Q, we assume P, then reason under 
that assumption to try to derive Q. If we are successful, then we can conclude that P ⇒ Q. 
In a proof, we are always allowed to introduce a new assumption P, then reason under that 
assumption. We must give the assumption a name; we have used the name x in the example 
below. Each distinct assumption must have a di fferent name.  
  
[x : P]  
 (assum)  
75 
 Because it has no premises, this rule can also start a proof. It can be used as if the proposition P 
were proved. The name of the assumption is also indicated here.  
However, you do not get to make assumptions for free! T o get a complete proof, all assumptions 
must be eventually  discharged . This is done in the implication introduction rule. This rule 
introduces an implication P ⇒ Q by discharging a prior assumption [x : P]. Intuitively, if Q can 
be proved under the assumption P, then the implication P ⇒ Q holds without any assumptions. 
We write x in the rule name to show which assumption is discharged. This rule and modus 
ponens are the introduction and elimination rules for implications.  
[x : P]  
⋮ 
Q 
P ⇒ Q 
 (⇒-intro/x)  P P ⇒ Q 
Q 
 (⇒-elim, modus ponens)  
A proof is valid only if every assumption is eventually discharged. This must happen in the proof 
tree below the assumption. The same assumption can be used more than once.  
Rules for Disjunction  
P 
P ∨ Q 
 (∨-intro -
left) Q 
P ∨ Q 
 (∨-intro -
right)  P ∨ Q P ⇒ R Q ⇒ R 
R 
 (∨-
elim)  
Rules for Negation  
A negation ¬P can be considered an abbreviation for P ⇒ ⊥: 
P ⇒ ⊥ 
¬P 
 (¬-intro)  ¬P 
P ⇒ ⊥ 
 (¬-elim)  
Rules for Falsity  
[x : ¬P]  
⋮ 
⊥ 
P 
 (reductio ad absurdum, RAA/x)  ⊥ 
P 
 (ex falso quodlibet, EFQ)  
Reductio ad absurdum  (RAA) is an interesting rule. It embodies proofs by contradiction. It says 
that if by assuming that P is false we  can derive a contradiction, then P must be true. The 
assumption x is discharged in the application of this rule. This rule is present in classical logic 
but not in  intuitionistic  (constructive) logic. In intuitionistic logic, a proposition is not 
consider ed true simply because its negation is false.  
Excluded Middle  
Another classical tautology that is not intuitionistically valid is the  the law of the excluded 
middle , P ∨ ¬P. We will take it as an axiom in our system. The Latin name for this rule 
is tertium non datur , but we will call it  magic . 
  
P ∨ ¬P 
 (magic)  
Proofs  
A proof of proposition P in natural deduction starts from axioms and assumptions and derives P 
with a ll assumptions discharged. Every step in the proof is an instance of an inference rule with 
metavariables substituted consistently with expressions of the appropriate syntactic class.  
Example  
76 
 For example, here is a proof of the proposition  (A ⇒ B ⇒ C) ⇒ (A ∧ B ⇒ C). 
 
The final step in the proof is to derive  (A ⇒ B ⇒ C) ⇒ (A ∧ B ⇒ C) from  (A ∧ B ⇒ C), which is 
done usin g the rule  (⇒-intro), discharging the assumption [x :  A ⇒ B ⇒ C]. To see how this rule 
generates the proof step, substitute for the metavariables P, Q, x in the rule as follows: P =  (A ⇒ 
B ⇒ C), Q =  (A ∧ B ⇒ C), and x = x. The immediately previous step use s the same rule, but with 
a different substitution: P =  A ∧ B, Q = C, x = y.  
The proof tree for this example has the following form, with the proved proposition at the root 
and axioms and assumptions at the leaves.  
 
A proposition that has a complete proof in a deductive system is called a  theorem  of that system.  
Soundness and Completeness  
A measure of a deductive system's  power is whether it is powerful enough to prove all true 
statements. A deductive system is said to be  complete  if all true statements are theorems (have 
proofs in the system). For propositional logic and natural deduction, this means that all 
tautologies must have natural deduction proofs. Conversely, a deductive system is 
called  sound  if all theorems are true. The proof rules we have given above are in fact sound and 
complete for propositional logic: every theorem is a tautology, and every tautology is a theorem.  
Finding a proof for a given tautology can be difficult. But once the proof is found, checking that 
it is indeed a proof is completely mechanical, requiring no intelligence or insight whatsoever. It 
is therefore a very strong argument that the thin g proved is in fact true.  
We can also make writing proofs less tedious by adding more rules that provide reasoning 
shortcuts. These rules are sound if there is a way to convert a proof using them into a proof using 
the original rules. Such added rules are called  admissible . 
 
Procedural versus Declarative Knowledge  
We have discussed various search techniques in previous units. Now we would consider a set of 
rules that represent,  
1. Knowledge about relationships in the world and  
2. Knowledge about how to solve the problem using the content of the rules.  
Procedural vs Declarative Knowledge  
Procedural Knowledge  
77 
  A representation in which the control information that is necessary to use the knowledge 
is embedded in the knowledge itself for e.g. computer programs, direct ions, and recipes; 
these   indicate specific use or implementation;  
 The real difference between declarative and procedural views of knowledge lies in where 
control information reside.  
For example, consider the following  
Man (Marcus)  
Man (Caesar)  
Person (Cle opatra)    
∀x: Man(x) →  Person(x)  
Now, try to answer the question.   ?Person(y)   
The knowledge base justifies any of the following answers.  
Y=Marcus   
Y=Caesar  
Y=Cleopatra  
 We get more than one value that satisfies the predicate.  
 If only one value needed, then  the answer to the question will depend on the order in 
which the assertions examined during the search for a response.  
 If the assertions declarative then they do not themselves say anything about how they will 
be examined. In case of procedural representa tion, they say how they will examine.  
Declarative   Knowledge  
 A statement in which knowledge specified, but the use to which that knowledge is to be 
put is not given.  
 For example, laws, people’s name; these are the facts which can stand alone, not 
dependent  on other knowledge;  
 So to use declarative representation, we must have a program that explains what is to do 
with the knowledge and how.  
 For example, a set of logical assertions can combine with a resolution theorem prover to 
give a complete program for s olving problems but in some cases, the logical assertions 
can view as a program rather than data to a program.  
 Hence the implication statements define the legitimate reasoning paths and automatic 
assertions provide the starting points of those paths.  
 These  paths define the execution paths which is similar to the ‘if then else “in traditional 
programming.  
 So logical assertions can view as a procedural representation of knowledge.  
Logic Programming – Representing Knowledge Using Rules  
 Logic programming is a p rogramming paradigm in which logical assertions viewed as 
programs.  
 These are several logic programming systems, PROLOG is one of them.  
 A PROLOG program consists of several logical assertions where each is a horn clause 
i.e. a clause with at most one posit ive literal.  
 Ex :  P,    P V Q, P  → Q  
 The facts are represented on Horn Clause for two reasons.  
1. Because of a uniform representation, a simple and efficient interpreter can write.  
2. The logic of Horn Clause decidable.  
78 
  Also, The first two differences are the f act that PROLOG programs are actually sets of 
Horn clause that have been transformed as follows: - 
1. If the Horn Clause contains no negative literal then leave it as it is.  
2. Also, Otherwise rewrite the Horn clauses as an implication, combining all of the 
negat ive literals into the antecedent of the implications and the single positive 
literal into the consequent.  
 Moreover, This procedure causes a clause which originally consisted of a disjunction of 
literals (one of them was positive) to be transformed into a s ingle implication whose 
antecedent is a conjunction universally quantified.  
 But when we apply this transformation, any variables that occurred in negative literals 
and so now occur in the antecedent become existentially quantified, while the variables in 
the consequent are still universally quantified.  
For example the PROLOG clause P(x): – Q(x, y) is equal to logical expression ∀x: ∃y: Q (x, 
y) → P(x).  
 The difference between the logic and PROLOG representation is that the PROLOG 
interpretation has a fixed control strategy. And so, the assertions in the PROLOG 
program define a particular search path to answer any question.  
 But, th e logical assertions define only the set of answers but not about how to choose 
among those answers if there is more than one.  
Consider the following example:  
1. Logical representation  
                        ∀x : pet(x)   ۸  small (x) →  apartmentpet(x)  
                        ∀x : cat(x)  ۸ dog(x) →  pet(x)  
                        ∀x : poodle (x)   → dog (x)  ۸ small (x)   
                         poodle (fluffy)  
2. Prolog representation  
                   apartmentpet (x)   :  pet(x), small (x)  
                   pet (x): cat (x)  
                   pet (x): dog(x)  
                  dog(x): poodle (x)  
                 small (x): poodle(x)  
                poodle (fluffy)  
 
Forward versus Backward Reasoning  
Forward versus Backward Reasoning  
A search procedure must fi nd a path between initial and goal states.  
There are two directions in which a search process could proceed.  
The two types of search are:  
1. Forward search which starts from the start state  
2. Backward search that starts from the goal state  
The production system  views the forward and backward as symmetric processes.  
Consider a game of playing 8 puzzles. The rules defined are  
Square 1 empty and square 2 contains tile n.  → 
79 
  Also, Square 2 empty and square 1 contains the tile n.  
Square 1 empty Square 4 contains tile n. → 
 Also, Square 4 empty and Square 1 contains tile n.  
We can solve the problem in 2 ways:  
1. Reason forward from the initial state  
 Step 1. Begin building a tree of move sequences by starting with the initial configuration 
at the root of the tree.  
 Step 2.  Generate the next level of the tree by finding all rules  whose left -hand side 
matches  against the root node. The right -hand side is used to create new configurations.  
 Step 3. Generate the next level by considering the nodes in the previous level and 
apply ing it to all rules whose left -hand side match.  
2. Reasoning backward from the goal states:  
 Step 1. Begin building a tree of move sequences by starting with the goal node 
configuration at the root of the tree.  
 Step 2. Generate the next level of the tree by  finding all rules  whose right -hand side 
matches  against the root node. The left -hand side used to create new configurations.  
 Step 3. Generate the next level by considering the nodes in the previous level and 
applying it to all rules whose right -hand side match.  
 So, The same rules can use in both cases.  
 Also, In forwarding reasoning, the left -hand sides of the rules matched against the current 
state and right sides used to generate the new state.  
 Moreover, In backward reasoning, the right -hand sides of the rules matched against the 
current state and left sides are used to generate the new state.  
There are four factors influencing the type of reasoning. They are,  
1. Are there more possible start or goal state? We move from smaller set of sets to the 
length.  
2. In what direction is the branching factor greater? We proceed in the direction with the 
lower branching factor.  
3. Will the program be asked to justify its reasoning process to a user? If, so then it is 
selected since it is very close to the way in which the user  thinks.  
4. What kind of event is going to trigger a problem -solving episode? If it is the arrival of a 
new factor, the forward reasoning makes sense. If it is a query to which a response is 
desired, backward reasoning is more natural.  
Example 1 of  Forward ve rsus Backward Reasoning  
 It is easier to drive from an unfamiliar place from home, rather than from home to an 
unfamiliar place. Also, If you consider a home as starting place an unfamiliar place as a 
goal then we have to backtrack from unfamiliar place to home.  
Example 2 of  Forward versus Backward Reasoning  
 Consider a problem of symbolic integration. Moreover, The problem space is a set of 
formulas, which contains integral expressions. Here START is equal to the given formula 
with some integrals. GOAL is eq uivalent to the expression of the formula without any 
integral. Here we start from the formula with some integrals and proceed to an integral 
free expression rather than starting from an integral free expression.  
Example 3 of  Forward versus Backward Reason ing 
80 
  The third factor is nothing but deciding whether the reasoning process can justify its 
reasoning. If it justifies then it can apply. For example, doctors are usually unwilling to 
accept any advice from diagnostics process because it cannot explain its reasoning.  
Example 4 of  Forward versus Backward Reasoning  
 Prolog is an example of backward chaining rule system. In Prolog rules restricted to Horn 
clauses. This allows for rapid indexing because all the rules for deducing a given fact 
share the same rule head. Rules matched with unification procedure. Unification tries to 
find a set of bindings for variables to equate a sub -goal with the head of some rule. Rules 
in the Prolog program matched in the order in which they appear.  
Combining Forward and Backward  Reasoning  
 Instead of searching either forward or backward, you can search both simultaneously.  
 Also, That is, start forward from a starting state and backward from a goal state 
simultaneously until the paths meet.  
 This strategy called Bi -directional searc h. The following figure shows the reason for a 
Bidirectional search to be ineffective.  
Forward versus Backward Reaso ning 
 Also, The two searches may pass each other resulting in more work.  
 Based on the form of the rules one can decide whether the same rules can apply to both 
forward and backward reasoning.  
 Moreover, If left -hand side and right of the rule contain pure as sertions then the rule can 
reverse.  
 And so the same rule can apply to both types of reasoning.  
 If the right side of the rule contains an arbitrary procedure then the rule cannot reverse.  
 So, In this case, while writing the rule the commitment to a directio n of reasoning must 
make.  
 
Symbolic Reasoning Under Uncertainty  
Symbolic Reasoning  
 The reasoning is the act of deriving a conclusion from certain properties using a given 
methodology.  
 The reasoning is a process of thinking; reasoning is logically arguing; reasoning is 
drawing the inference.  
 When a system is required to do something, that it has not been explicitly told how to do, 
it must reason. It must figure out what it needs to know from what it already knows.  
81 
  Many types of Reasoning have been identified  and recognized, but many questions 
regarding their logical and computational properties still remain controversial.  
 The popular methods of Reasoning include abduction, induction, model -based, 
explanation and confirmation. All of them are intimately relate d to problems of belief 
revision and theory development, knowledge absorption, discovery, and learning.  
Logical Reasoning  
 Logic is a language for reasoning. It is a collection of rules called Logic arguments, we 
use when doing logical reasoning.  
 The logic reasoning is the process of drawing conclusions from premises using rules of 
inference.  
 The study of logic divided into formal and informal logic. The formal logic is sometimes 
called symbolic logic.  
 Symbolic logic is the study of symbolic abstractions (co nstruct) that capture the formal 
features of logical inference by a formal system.  
 The formal system consists of two components, a formal language plus a set of inference 
rules.  
 The formal system has axioms. Axiom is a sentence that is always true within t he system.  
 Sentences derived using the system’s axioms and rules of derivation called theorems.  
 The Logical Reasoning is of our concern in AI.  
Approaches to Reasoning  
 There are three different approaches to reasoning under uncertainties.  
1. Symbolic reasoning  
2. Statistical reasoning  
3. Fuzzy logic reasoning  
Symbolic Reasoning  
 The basis for intelligent mathematical software is the integration of the “power of 
symbolic mathematical tools” with the suitable “proof technology”.  
 Mathematical reasoning enjoys a property called monotonicity, that says, “If a conclusion 
follows from given premises A, B, C… then it also follows from any larger set of 
premises, as long as the original premises A, B, C.. included.”  
 Moreover, Human reasoning is not monotonic.  
 People arrive at c onclusions only tentatively; based on partial or incomplete information, 
reserve the right to retract those conclusions while they learn new facts. Such reasoning 
non-monotonic, precisely because the set of accepted conclusions have become smaller 
when the  set of premises expanded.  
Formal Logic  
Moreover, The Formal logic is the study of inference with purely formal content, i.e. where 
content made explicit.  
Examples – Propositional logic and Predicate logic.  
 Here the logical arguments are a set of rules for  manipulating symbols. The rules are of 
two types,  
1. Syntax rules: say how to build meaningful expressions.  
2. Inference rules: say how to obtain true formulas from other true formulas.  
 Moreover, Logic also needs semantics, which says how to assign meaning to e xpressions.  
Uncertainty in Reasoning  
82 
  The world is an uncertain place; often the Knowledge is imperfect which causes 
uncertainty.  
 So, Therefore reasoning must be able to operate under uncertainty.  
 Also, AI systems must have the ability to reason under condi tions of uncertainty.  
Monotonic Reasoning  
 A reasoning process that moves in one direction only.  
 Moreover, The number of facts in the knowledge base is always increasing.  
 The conclusions derived are valid deductions and they remain so.  
A monotonic logic can not handle  
1. Reasoning by default: because consequences may derive only because of lack of evidence 
to the contrary.  
2. Abductive reasoning: because consequences only deduced as most likely explanations.  
3. Belief revision: because new knowledge may contradict old  beliefs.  
 
Introduction to Nonmonotonic Reasoning  
Non-monotonic Reasoning  
The definite clause logic is  monotonic  in the sense that anything that could be concluded before 
a clause is added can still be concluded after it is added; adding knowledge does not  reduce the 
set of propositions that can be derived.  
A logic is  non-monotonic  if some conclusions can be invalidated by adding more knowledge. 
The logic of definite clauses with negation as failure is non -monotonic. Non -monotonic 
reasoning is useful for re presenting defaults. A  default  is a rule that can be used unless it 
overridden by an exception.  
For example, to say that  b is normally true if  c is true, a knowledge base designer can write a rule 
of the form  
b ←c ∧ ∼ aba. 
where  aba is an atom that means a bnormal with respect to some aspect  a. Given  c, the agent can 
infer  bunless it is told  aba. Adding  aba to the knowledge base can prevent the conclusion of  b. 
Rules that imply  abacan be used to prevent the default under the conditions of the body of the 
rule. 
Example 5.27:  Suppose the purchasing agent is investigating purchasing holidays. A resort may 
be adjacent to a beach or away from a beach. This is not symmetric; if the resort was adjacent to 
a beach, the knowledge provider would specify this. Thus, it is reasonable to have the clause  
away_from_beach ← ∼ on_beach.  
This clause enables an agent to infer that a resort is away from the beach if the agent is not told it 
is adjacent to a beach.  
A cooperative system  tries to not mislead. If we are told the reso rt is on the beach, we would 
expect that resort users would have access to the beach. If they have access to a beach, we would 
expect them to be able to swim at the beach. Thus, we would expect the following defaults:  
beach_access ←on_beach ∧ ∼ abbeach_acc ess.  
swim_at_beach ←beach_access ∧ ∼ abswim_at_beach . 
A cooperative system would tell us if a resort on the beach has no beach access or if there is no 
swimming. We could also specify that, if there is an enclosed bay and a big city, then there is no 
swim ming, by default:  
abswim_at_beach  ←enclosed_bay ∧big_city ∧ ∼ abno_swimming_near_city . 
We could say that British Columbia is abnormal with respect to swimming near cities:  
83 
 abno_swimming_near_city  ←in_BC ∧ ∼ abBC_beaches . 
Given only the preceding rules, an agent infers  away_from_beach . If it is then told  on_beach , it 
can no longer infer  away_from_beach , but it can now infer  beach_access  and swim_at_beach . If 
it is also told  enclosed_bay  and big_city , it can no longer infer  swim_at_beach . However, if it is 
then told  in_BC , it can then infer  swim_at_beach . 
By having defaults of what is normal, a user can interact with the system by telling it what is 
abnormal, which allows for economy in communication. The user does not have to state the 
obvious.  
One way to thi nk about non -monotonic reasoning is in terms of  arguments . The rules can be 
used as components of arguments, in which the negated abnormality gives a way to undermine 
arguments. Note that, in the language presented, only positive arguments exist that can b e 
undermined. In more general theories, there can be positive and negative arguments that attack 
each other.  
 
Implementation Issues  
 
Weak Slot and Filler Structures  
 
Evolution Frames  
 As seen in the previous example, there are certain problems which are dif ficult to solve 
with Semantic Nets.  
 Although there is no clear distinction between a semantic net and frame system, more 
structured the system is, more likely it is to be termed as a frame system.  
 A frame is a collection of attributes (called slots) and as sociated values that describe 
some entities in the world. Sometimes a frame describes an entity in some absolute sense;  
 Sometimes it represents the entity from a particular point of view only.  
 A single frame taken alone is rarely useful; we build frame sys tems out of collections of 
frames that connected to each other by virtue of the fact that the value of an attribute of 
one frame may be another frame.  
Frames as Sets and Instances  
 The set theory is a good basis for understanding frame systems.  
 Each frame r epresents either a class (a set) or an instance (an element of class)  
 Both  isa and instance  relations have inverse attributes, which we call subclasses & all 
instances.  
 As a class represents a set, there are 2 kinds of attributes that can be associated wit h it. 
1. Its own attributes &  
2. Attributes that are to be inherited by each element of the set.  
84 
 
 
Frames as Sets and In stances  
 Sometimes, the difference between a set and an individual instance may not be clear.  
 Example: Team India is an instance of the class of Cricket Teams and can also think of as 
the set of players.  
 Now the problem is if we present Team India as a subc lass of Cricket teams, then Indian 
players automatically become part of all the teams, which is not true.  
 So, we can make Team India a subclass of class called Cricket Players.  
 To do this we need to differentiate between regular classes and meta -classes.  
 Regular Classes are those whose elements are individual entities whereas Meta -classes 
are those special classes whose elements are themselves, classes.  
 The most basic meta -class is the class  CLASS . 
 It represents the set of all classes.  
 All classes are insta nces of it, either directly or through one of its subclasses.  
 The class  CLASS  introduces the attribute cardinality, which is to inherited by all instances 
of CLASS. Cardinality stands for the number.  
Other ways of Relating Classes to Each Other  
 We have dis cussed that a class1 can be a subset of class2.  
 If Class2 is a meta -class then Class1 can be an instance of Class2.  
 Another way is the  mutually -disjoint -with relationship, which relates a class to one or 
more other classes that guaranteed to have no elemen ts in common with it.  
 Another one is,  is-covered -by which relates a class to a set of subclasses, the union of 
which is equal to it.  
 If a class is -covered -by a set S of mutually disjoint classes, then S called a partition of the 
class.  
Slots as Full -Fledge d Objects (Frames)  
Till now we have used attributes as slots, but now we will represent attributes explicitly and 
describe their properties.  
Some of the properties we would like to be able to represent and use in reasoning include,  
 The class to which the a ttribute can attach.  
 Constraints on either the type or the value of the attribute.  
 A default value for the attribute. Rules for inheriting values for the attribute.  
 To be able to represent these attributes of attributes, we need to describe attributes (slo ts) 
as frames.  
85 
  These frames will organize into an  isa hierarchy, just as any other frames, and that 
hierarchy can then used to support inheritance of values for attributes of slots.  
 Now let us formalize what is a slot. A slot here is a relation.  
 It maps fr om elements of its domain (the classes for which it makes sense) to elements of 
its range (its possible values).  
 A relation is a set of ordered pairs.  
 Thus it makes sense to say that relation R1 is a subset of another relation R2.  
 In that case, R1 is a spe cialization of R2. Since a slot is a set, the set of all slots, which 
we will call SLOT, is a meta -class.  
 Its instances are slots, which may have sub -slots.  
Frame Example  
In this example, the frames Person, Adult -Male, ML -Baseball -Player (corresponding to major 
league baseball players), Pitcher, and ML -Baseball -Team (for major league baseball team) are all 
classes.  
 
 The fra mes Pee -Wee-Reese and Brooklyn -Dodgers are instances.  
 The isa relation that we have been using without a precise definition is, in fact, the subset 
relation. The set of adult males is a subset of the set of people.  
 The set of major league baseball players subset of the set of adult males, and so forth.  
 Our instance relation corresponds to the relation element -of Pee Wee Reese is an element 
of the set of fielders.  
 Thus he is also an element of all of the supersets of fielders, including major league 
baseball  players and people. The transitivity of  isa follows directly from the transitivity 
of the subset relation.  
86 
  Both the isa  and instance relations have inverse attributes, which we call subclasses and 
all instances.  
 Because a class represents a set, there are  two kinds of attributes that can associate with 
it. 
 Some attributes are about the set itself, and some attributes are to inherited by each 
element of the set.  
 We indicate the difference between these two by prefixing the latter with an asterisk (*).  
 For e xample, consider the class ML -Baseball -Player, we have shown only two properties 
of it as a set: It a subset of the set of adult males. And it has cardinality 624.  
 We have listed five properties that all major league baseball players have (height, bats, 
batting average, team, and uniform -color), and we have specified default values for the 
first three of them.  
 By providing both kinds of slots, we allow both classes to define a set of objects and to 
describe a prototypical object of the set.  
 Frames are usefu l for representing objects that are typical of stereotypical situations.  
 The situation like the structure of complex physical objects, visual scenes, etc.  
 A commonsense knowledge can represent using default values if no other value exists. 
Commonsense is g enerally used in the absence of specific knowledge.  
Semantic Nets  
 Inheritance property can represent using  isa and instance  
 Monotonic Inheritance can perform substantially more efficiently with such structures 
than with pure logic, and non -monotonic inheri tance is also easily supported.  
 The reason that makes Inheritance easy is that the knowledge in slot and filler systems is 
structured as a set of entities and their attributes.  
These structures turn out to be useful as,  
 It indexes assertions by the entitie s they describe. As a result, retrieving the value for an 
attribute of an entity is fast.  
 Moreover, It makes easy to describe properties of relations. To do this in a purely logical 
system requires higher -order mechanisms.  
 It is a form of object -oriented p rogramming and has the advantages that such systems 
normally include modularity and ease of viewing by people.  
Here we would describe two views of this kind of structure – Semantic Nets & Frames.  
Semantic Nets   
 There are different approaches to knowledge r epresentation include semantic net, frames, 
and script.  
 The semantic net describes both objects and events.  
 In a semantic net, information represented as a set of nodes connected to each other by a 
set of labeled arcs, which represents relationships among the nodes.  
 It is a directed graph consisting of vertices which represent concepts and edges which 
represent semantic relations between the concepts.  
 It is also known as associative net due to the association of one node with other.  
 The main idea is that th e meaning of the concept comes from the ways in which it 
connected to other concepts.  
 We can use inheritance to derive additional relations.  
87 
 
 
Figure: A Semantic Network  
Intersection Search  Semantic Nets  
 We try to find relationships among objects by spreading activation out from each of two 
nodes. And seeing where the activation meets.  
 Using this we can answer the questions like, what is the relation between India and Blue.  
 It takes advantage of the entity -based organization of knowledge that slot and filler 
representation provides.  
Representing Non -binary Predicates  Semantic Nets  
 Simple binary predicates like isa(Person, Mam mal) can represent easily by semantic nets 
but other non -binary predicates can also represent by using general -purpose predicates 
such as  isa and instance . 
 Three or even more place predicates can also convert to a binary form by creating one 
new object rep resenting the entire predicate statement and then introducing binary 
predicates to describe a relationship to this new object.  
Conceptual Dependency  
Introduction to  Strong Slot and Filler Structures  
 The main problem with semantic networks and frames is tha t they lack formality; there is 
no specific guideline on how to use the representations.  
 In frame when things change, we need to modify all frames that are relevant – this can be 
time-consuming.  
 Strong slot and filler structures typically represent links b etween objects according to 
more rigid rules, specific notions of what types of object and relations between them are 
provided and represent knowledge about common situations.  
 Moreover, We have types of strong slot and filler structures:  
1. Conceptual Depende ncy (CD)  
2. Scripts  
3. Cyc 
Conceptual Dependency (CD)  
Conceptual Dependency originally developed to represent knowledge acquired from natural 
language input.  
The goals of this theory are:  
 To help in the drawing of the inference from sentences.  
 To be independent of the words used in the original input.  
 That is to say: For any 2 (or more) sentences that are identical in meaning there should be 
only one representation of that meaning.  
Moreover, It has used by many programs that portend to understand English (MARGIE,  SAM, 
PAM).  
88 
 Conceptual Dependency (CD) provides:  
 A structure into which nodes representing information can be placed.  
 Also, A specific set of primitives.  
 A given level of granularity.  
Sentences are represented as a series of diagrams depicting actions usin g both abstract and real 
physical situations.  
 The agent and the objects represented.  
 Moreover, The actions are built up from a set of primitive acts which can modify by 
tense.  
CD is based on events and actions. Every event (if applicable) has:  
 an ACTOR o a n ACTION performed by the Actor  
 Also, an OBJECT that the action performs on  
 A DIRECTION in which that action is oriented  
These are represented as slots and fillers. In English sentences, many of these attributes left out.  
A Simple Conceptual Dependency Rep resentation  
For the sentences, “I have a book to the man” CD representation is as follows:  
 
Where the symbols have  the following meaning.  
 Arrows indicate directions of dependency.  
 Moreover, The double arrow indicates the two -way link between actor and action.  
 O — for the object case relation  
 R – for the recipient case relation  
 P – for past tense  
 D – destination  
 Primi tive Acts of Conceptual Dependency Theory  
ATRANS  
 Transfer of an abstract relationship (i.e. give)  
PTRANS  
 Transfer of the physical location of an object (e.g., go)  
PROPEL  
 Also, Application of physical force to an object (e.g. push)  
MOVE  
 Moreover, Movement o f a body part by its owner (e.g. kick)  
GRASP  
 Grasping of an object by an action (e.g. throw)  
INGEST  
 Ingesting of an object by an animal (e.g. eat)  
EXPEL  
 Expulsion of something from the body of an animal (e.g. cry)  
MTRANS  
 Transfer of mental information (e.g . tell)  
MBUILD  
 Building new information out of old (e.g decide)  
SPEAK  
89 
  Producing of sounds (e.g. say)  
ATTEND  
 Focusing of a sense organ toward a stimulus (e.g. listen)  
There are four conceptual categories. These are,  
ACT  
 Actions {one of the CD primitives}  
PP 
 Also, Objects {picture producers}  
AA 
 Modifiers of actions {action aiders}  
PA 
 Modifiers of PP’s {picture aiders}  
Advantages of  Conceptual Dependency  
 Using these primitives involves fewer inference rules.  
 So, Many inference rules already represented in CD s tructure.  
 Moreover, The holes in the initial structure help to focus on the points still to established.  
Disadvantages of  Conceptual Dependency  
 Knowledge must decompose into fairly low -level primitives.  
 Impossible or difficult to find the correct set of pr imitives.  
 Also, A lot of inference may still require.  
 Representations can be complex even for relatively simple actions.  
 Consider: Dave bet Frank five pounds that Wales would win the Rugby World Cup.  
 Moreover, Complex representations require a lot of stora ge. 
Scripts  
Scripts Strong Slot  
 A script is a structure that prescribes a set of circumstances which could be expected to 
follow on from one another.  
 It is similar to a thought sequence or a chain of situations which could be anticipated.  
 It could be consi dered to consist of a number of slots or frames but with more specialized 
roles.  
Scripts are beneficial because:  
 Events tend to occur in known runs or patterns.  
 Causal relationships between events exist.  
 Entry conditions exist which allow an event to take place  
 Prerequisites exist for events taking place. E.g. when a student progresses through a 
degree scheme or when a purchaser buys a house.  
Script Components  
Each script contains the following main components.  
 Entry Conditions: Must be satisfied before eve nts in the script can occur.  
 Results: Conditions that will be true after events in script occur.  
 Props: Slots representing objects involved in the events.  
 Roles: Persons involved in the events.  
 Track: the Specific variation on the more general pattern in t he script. Different tracks 
may share many components of the same script but not all.  
90 
  Scenes: The sequence of events that occur. Events represented in conceptual dependency 
form.  
 Advantages and Disadvantages of Script  
Advantages  
 Capable of predicting impl icit events  
 Single coherent interpretation may be build up from a collection of observations.  
Disadvantage  
 More specific (inflexible) and less general than frames.  
 Not suitable to represent all kinds of knowledge.  
To deal with inflexibility, smaller module s called memory organization packets (MOP) 
can combine in a way that appropriates for the situation.  
Script Example  
 
 It must activate based on its significance.  
 If the topic important, then the script should open.  
 If a topic just mentioned, then a pointer to that script could hold.  
 For example, given “John enjoyed t he play in theater”, a script “Play in Theater” 
suggested above invoke.  
 All implicit questions can  answer correctly.  
Here the significance of this script is high.  
 Did John go to the theater?  
 Also, Did he buy the ticket?  
 Did he have money?  
If we have a sent ence like “John went to the theater to pick his daughter”, then invoking this 
script will lead to many wrong answers.  
 Here significance of the script theater is less.  
91 
 Getting significance from the story is not straightforward. However, some heuristics can apply to 
get the value.  
 
CYC  
What is CYC?  
 An ambitious attempt to form a very large knowledge base aimed at capturing 
commonsense reasoning.  
 Initial goals to capture knowledge from a hundred randomly selected articles in the 
Encyclopedia Britannica.  
 Also, Both Implicit and Explicit knowledge encoded.  
 Moreover, Emphasis on study of underlying information (assumed by the authors but not 
needed to tell to the readers.  
Example: Suppose we read that Wellington learned of Napoleon’s death  Then we (humans) 
can c onclude Napoleon never new that Wellington had died.  
How do we do this?  
So, We require special implicit knowledge or commonsense such as:  
 We only die once.  
 You stay dead.  
 Moreover, You cannot learn anything when dead.  
 Time cannot go backward.  
Why build lar ge knowledge bases:  
1. Brittleness  
 Specialised knowledge bases are brittle. Hard to encode new situations and non -
graceful degradation in performance. Commonsense based knowledge bases 
should have a firmer foundation.  
2. Form and Content  
 Moreover, Knowledge repr esentation may not be suitable for AI. Commonsense 
strategies could point out where difficulties in content may affect the form.  
3. Shared Knowledge  
 Also, Should allow greater communication among systems with common bases 
and assumptions.  
How is CYC coded?  
 By hand.  
 Special CYCL language:  
 LISP -like. 
 Frame -based  
 Multiple inheritances  
 Slots are fully fledged objects.  
 Generalized inheritance — any link not just  isa and instance . 
 
 
 
Module 2  
Game Playing:  
92 
 Game Playing  
 Charles Babbage, the nineteenth -century compute r architect thought about programming 
his analytical engine to play chess and later of building a machine to play tic -tac-toe. 
 There are two reasons that games appeared to be a good domain.  
1. They provide a structured task in which it is very easy to measure  success or 
failure.  
2. They are easily solvable by straightforward search from the starting state to a 
winning position.  
 The first is true is for all games bust the second is not true for all, except simplest games.  
 For example, consider chess.  
 The average b ranching factor is around 35. In an average game, each player might make 
50. 
 So in order to examine the complete game tree, we would have to examine 35100 
 Thus it is clear that a simple search is not able to select even its first move during the 
lifetime o f its opponent.  
 It is clear that to improve the effectiveness of a search based problem -solving program 
two things can do.  
1. Improve the generate procedure so that only good moves generated.  
2. Improve the test procedure so that the best move will recognize and  explored first.  
 If we use legal -move generator then the test procedure will have to look at each of them 
because the test procedure must look at so many possibilities, it must be fast.  
 Instead of the legal -move generator, we can use plausible -move generat or in which only 
some small numbers of promising moves generated.  
 As the number of lawyers available moves increases, it becomes increasingly important 
in applying heuristics to select only those moves that seem more promising.  
 The performance of the overa ll system can improve by adding heuristic knowledge into 
both the generator and the tester.  
 In game playing, a goal state is one in which we win but the game like chess. It is not 
possible. Even we have good plausible move generator.  
 The depth of the resul ting tree or graph and its branching factor is too great.  
 It is possible to search tree only ten or twenty moves deep then in order to choose the best 
move. The resulting board positions must compare to discover which is most 
advantageous.  
 This is done usi ng static evolution function, which uses whatever information it has to 
evaluate individual board position by estimating how likely they are to lead eventually to 
a win.  
 Its function is similar to that of the heuristic function h’ in the A* algorithm: in t he 
absence of complete information, choose the most promising position.  
MINIMAX Search Procedure  
 The minimax search is a depth -first and depth limited procedure.  
 The idea is to start at the current position and use the plausible -move generator to 
generate the set of possible successor positions.  
93 
  Now we can apply the static evolution function to those positions and simply choose the 
best one.  
 After doing so, we can back that value up to the starting position to represent our 
evolution of it.  
 Here we assume t hat static evolution function returns larger values to indicate good 
situations for us.  
 So our goal is to maximize the value of the static evaluation function of the next board 
position.  
 The opponents’ goal is to minimize the value of the static evaluation  function.  
 The alternation of maximizing and minimizing at alternate ply when evaluations are 
to be pushed back up corresponds to the opposing strategies of the two players is 
called MINIMAX.  
 It is the recursive procedure that depends on two procedures  
 MOV EGEN(position, player) — The plausible -move generator, which returns a 
list of nodes representing the moves that can make by Player in Position.  
 STATIC(position, player) – static evaluation function, which returns a number 
representing the goodness of Positi on from the standpoint of Player.  
 With any recursive program, we need to decide when recursive procedure should stop.  
 There are the variety of factors that may influence the decision they are,  
 Has one side won?  
 How many plies have we already explored? Or h ow much time is left?  
 How stable is the configuration?  
 We use DEEP -ENOUGH which assumed to evaluate all of these factors and to return 
TRUE if the search should be stopped at the current level and FALSE otherwise.  
 It takes two parameters, position, and dep th, it will ignore its position parameter and 
simply return TRUE if its depth parameter exceeds a constant cut off value.  
 One problem that arises in defining MINIMAX as a recursive procedure is that it needs to 
return not one but two results.  
 The backed -up value of the path it chooses.  
 The path itself. We return the entire path even though probably only the first 
element, representing the best move from the current position, actually needed.  
 We assume that MINIMAX returns a structure containing both results  and we have two 
functions, VALUE and PATH that extract the separate components.  
 Initially, It takes three parameters, a board position, the current depth of the search, and 
the player to move,  
 MINIMAX(current,0,player -one) If player –one is to move  
 MINIMA X(current,0,player -two) If player –two is to move  
Adding alpha -beta cutoffs  
 Minimax procedure is a depth -first process. One path is explored as far as time allows, 
the static evolution function is applied to the game positions at the last step of the path.  
 The efficiency of the depth -first search can improve by branch and bound technique in 
which partial solutions that clearly worse than known solutions can abandon early.  
 It is necessary to modify the branch and bound strategy to include two bounds, one for  
each of the players.  
 This modified strategy called alpha -beta pruning.  
94 
  It requires maintaining of two threshold values, one representing a lower bound on that a 
maximizing node may ultimately assign (we call this alpha).  
 And another representing an upper bound on the value that a minimizing node may assign 
(this we call beta).  
 Each level must receive both the values, one to use and one to pass down to the next level 
to use.  
 The MINIMAX procedure as it stands does not need to treat maximizing and minimizing  
levels differently. Since it simply negates evaluation each time it changes levels.  
 Instead of referring to alpha and beta, MINIMAX uses two values, USE -THRESH and 
PASSTHRESH.  
 USE -THRESH used to compute cutoffs. PASS -THRESH passed to next level as its 
USE THRESH.  
 USE -THRESH must also pass to the next level, but it will pass as PASS -THRESH so that 
it can be passed to the third level down as USE -THRESH again, and so forth.  
 Just as values had to negate each time they passed across levels.  
 Still, there is no di fference between the code required at maximizing levels and that 
required at minimizing levels.  
 PASS -THRESH should always the maximum of the value it inherits from above and the 
best move found at its level.  
 If PASS -THRESH updated the new value should prop agate both down to lower levels. 
And back up to higher ones so that it always reflects the best move found anywhere in the 
tree. 
 The MINIMAX -A-B requires five arguments, position, depth, player, Use -thresh, and 
passThresh.  
 MINIMAX -A-B(current,0,player -one,maximum value static can compute, minimum 
value static can compute).  
Iterative Deepening Search(IDS) or Iterative Deepening Depth First Search(IDDFS)  
There are two common ways to traverse a graph,  BFS and DFS. Considering a Tree (or Graph) 
of huge height and width, both BFS and DFS are not very efficient due to following reasons.  
1. DFS  first traver ses nodes going through one adjacent of root, then next adjacent. The 
problem with this approach is, if there is a node close to root, but not in first few subtrees 
explored by DFS, then DFS reaches that node very late. Also, DFS may not find shortest 
path to a node (in terms of number of edges).  
95 
 
 
2. BFS goes level by level, bu t requires more space. The space required by DFS is O(d) 
where d is depth of tree, but space required by BFS is O(n) where n is number of nodes in 
tree (Why? Note that the last level of tree can have around n/2 nodes and second last 
level n/4 nodes and in BFS we need to have every level one by one in queue).  
IDDFS  combines depth -first search’s space -efficiency and breadth -first search’s fast search (for 
nodes closer to root).  
How does IDDFS work?  
IDDFS calls DFS for different depths starting from an initial  value. In every call, DFS is 
restricted from going beyond given depth. So basically we do DFS in a BFS fashion.  
Algorithm:  
 
 
 
// Returns true if target is reachable from  
// src within max_depth  
bool IDDFS(src, target, max_depth)  
    for limit from  0 to max_depth  
       if DLS(src, target, limit) == true 
           return  true 
    return  false     
 
bool DLS(src, target, limit)  
    if (src == target)  
        return  true; 
 
    // If reached the maximum depth,  
    // stop recursing.  
    if (limit <= 0)  
        return  false ;    
 
    foreach  adjacent i of src  
96 
         if DLS(i, target, limit?1)              
            return  true 
 
    return  false  
An important thing to note is, we visit top level nodes multiple times. The last (or max depth) 
level is visited once , second last level is visited twice, and so on. It may seem expensive, but it 
turns out to be not so costly, since in a tree most of the nodes are in the bottom level. So it does 
not matter much if the upper levels are visited multiple times.  
Planning  
Blocks World Problem  
In order to compare the variety of methods of planning, we should find it useful to look at all of 
them in a single domain that is complex enough that the need for each of the mechanisms is 
apparent yet simple enough that easy -to-follow e xamples can be found.  
 There is a flat surface on which blocks can be placed.  
 There are a number of square blocks, all the same size.  
 They can be stacked one upon the other.  
 There is robot arm that can manipulate the blocks.  
Actions of the robot arm  
1. UNSTACK (A, B): Pick up block A from its current position on block B.  
2. STACK(A, B): Place block A on block B.  
3. PICKUP(A): Pick up block A from the table and hold it.  
4. PUTDOWN(A): Put block A down on the table.  
Notice that the robot arm can hold only one block at a ti me. 
Predicates  
 In order to specify both the conditions under which an operation may be performed and 
the results of performing it, we need the following predicates:  
1. ON(A, B): Block A is on Block B.  
2. ONTABLES(A): Block A is on the table.  
3. CLEAR(A): There is n othing on the top of Block A.  
4. HOLDING(A): The arm is holding Block A.  
5. ARMEMPTY: The arm is holding nothing.  
Robot problem -solving systems (STRIPS)  
 List of new predicates that the operator causes to become true is ADD List  
 Moreover, List of old predicates t hat the operator causes to become false is DELETE List  
 PRECONDITIONS list contains those predicates that must be true for the operator to be 
applied.  
STRIPS style operators for BLOCKs World  
STACK(x, y)  
P: CLEAR(y)^HOLDING(x)  
D: CLEAR(y)^HOLDING(x)  
A: ARMEM PTY^ON(x, y)  
UNSTACK(x, y)  
PICKUP(x)  
P: CLEAR(x) ^ ONTABLE(x) ^ARMEMPTY  
D: ONTABLE(x) ^ ARMEMPTY  
97 
 A: HOLDING(x)  
PUTDOWN(x)  
Goal Stack Planning  
To start with goal stack is simply:  
 ON(C,A)^ON(B,D)^ONTABLE(A)^ONTABLE(D)  
This problem is separate into four sub -problems, one for each component of the goal.  
Two of the sub -problems ONTABLE(A) and ONTABLE(D) are already true in the initial state.  
 
Alternative 1: Goal Stack:  
 ON(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
Alternative 2: Goal stack:  
 ON(B,D)  
 ON(C,A)  
 ON(C,A)^ON(B,D)^OTAD  
Exploring Operators  
 Pursuing alternative 1, we check for operators that could cause ON(C, A)  
 Out of the 4 operators, there is only one STACK. So it yields:  
 STACK(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
 Preconditions for STACK(C, A) should be satisfied, we must establish them as sub -goals:  
 CLEAR(A)  
 HOLDING(C)  
 CLEAR(A)^HOLDING(C)  
 STACK(C,A) o ON(B,D)  
 ON(C,A)^ON(B,D)^O TAD  
 Here we exploit the Heuristic that if HOLDING is one of the several goals to be achieved 
at once, it should be tackled last.  
Goal stack Planning  
 Next, we see if CLEAR(A) is true. It is not. The only operator that could make it true is 
UNSTACK(B, A). Al so, This produces the goal stack:  
 ON(B, A)  
 CLEAR(B)  
 ON(B,A)^CLEAR(B)^ARMEMPTY  
 UNSTACK(B, A)  
 HOLDING(C)  
98 
  CLEAR(A)^HOLDING(C)  
 STACK(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
 We see that we can pop predicates on the stack till we reach HOLDING(C) for which we 
need to find a suitable operator.  
 Moreover, The operators that might make HOLDING(C) true: PICKUP(C) and 
UNSTACK(C, x). Without looking ahead, since we cannot tell which of these operators 
is appropriate. Also, we create two branches of the search tree correspondi ng to the 
following goal stacks:  
 
Complete plan  
1. UNSTACK(C, A)  
2. PUTDOWN(C )  
3. PICKUP(A)  
4. STACK(A, B)  
5. UNSTACK(A, B)  
6. PUT DOWN(A)  
7. PICKUP(B)  
8. STACK(B, C)  
9. PICKUP(A)  
10. STACK(A,B)  
Planning Components  
 Methods which focus on ways of decomposing the original problem into appropriate 
subparts and on ways of recording and handling interactions among the subparts as they 
are detected duri ng the problem -solving process are often called as planning.  
 Planning refers to the process of computing several steps of a problem -solving procedure 
before executing any of them.  
Components of a planning system  
Choose the best rule to apply next, based on  the best available heuristic information.  
 The most widely used technique for selecting appropriate rules to apply is first to isolate 
a set of differences between desired goal state and then to identify those rules that are 
relevant to reduce those differ ences.  
 If there are several rules, a variety of other heuristic information can be exploited to 
choose among them.  
Apply the chosen rule to compute the new problem state that arises from its application.  
99 
  In simple systems, applying rules is easy. Each rule  simply specifies the problem state 
that would result from its application.  
 In complex systems, we must be able to deal with rules that specify only a small part of 
the complete problem state.  
 One way is to describe, for each action, each of the changes it  makes to the state 
description.  
Detect when a solution has found.  
 A planning system has succeeded in finding a solution to a problem when it has found a 
sequence of operators that transform the initial problem state into the goal state.  
 How will it know w hen this has done?  
 In simple problem -solving systems, this question is easily answered by a straightforward 
match of the state descriptions.  
 One of the representative systems for planning systems is predicate logic. Suppose that as 
a part of our goal, we h ave the predicate P(x).  
 To see whether P(x) satisfied in some state, we ask whether we can prove P(x) given the 
assertions that describe that state and the axioms that define the world model.  
Detect dead ends so that they can abandon and the system’s effor t directed in more fruitful 
directions.  
 As a planning system is searching for a sequence of operators to solve a particular 
problem, it must be able to detect when it is exploring a path that can never lead to a 
solution.  
 The same reasoning mechanisms that  can use to detect a solution can often use for 
detecting a dead end.  
 If the search process is reasoning forward from the initial state. It can prune any path that 
leads to a state from which the goal state cannot reach.  
 If search process reasoning backwar d from the goal state, it can also terminate a path 
either because it is sure that the initial state cannot reach or because little progress made.  
Detect when an almost correct solution has found and employ special techniques to make it 
totally correct.  
 The kinds of techniques discussed are often useful in solving nearly decomposable 
problems.  
 One good way of solving such problems is to assume that they are completely 
decomposable, proceed to solve the sub -problems separately. And then check that when 
the sub-solutions combined. They do in fact give a solution to the original problem.  
Goal Stack Planning  
 Methods which focus on ways of decomposing the original problem into appropriate 
subparts and on ways of recording. And handling interactions among the subp arts as they 
are detected during the problem -solving process are often called as planning.  
 Planning refers to the process of computing several steps of a problem -solving procedure 
before executing any of them.  
  
 
Goal Stack Planning Method  
 In this method, t he problem solver makes use of a single stack that contains both goals 
and operators. That have proposed to satisfy those goals.  
100 
  The problem solver also relies on a database that describes the current situation and a set 
of operators described as PRECONDIT ION, ADD and DELETE lists.  
 The goal stack planning method attacks problems involving conjoined goals by solving 
the goals one at a time, in order.  
 A plan generated by this method contains a sequence of operators for attaining the first 
goal, followed by a complete sequence for the second goal etc.  
 At each succeeding step of the problem -solving process, the top goal on the stack will 
pursue.  
 When a sequence of operators that satisfies it, found, that sequence applied to the state 
description, yielding new de scription.  
 Next, the goal that then at the top of the stack explored. And an attempt made to satisfy it, 
starting from the situation that produced as a result of satisfying the first goal.  
 This process continues until the goal stack is empty.  
 Then as one l ast check, the original goal compared to the final state derived from the 
application of the chosen operators.  
 If any components of the goal not satisfied in that state. Then those unsolved parts of the 
goal reinserted onto the stack and the process resume d. 
Nonlinear Planning using Constraint Posting  
 Difficult problems cause goal interactions.  
 The operators used to solve one sub -problem may interfere with the solution to a previous 
sub-problem.  
 Most problems require an intertwined plan in which multiple su b-problems worked on 
simultaneously.  
 Such a plan is called nonlinear plan because it is not composed of a linear sequence of 
complete sub -plans.  
Constraint Posting  
 The idea of constraint posting is to build up a plan by incrementally hypothesizing 
operator s, partial orderings between operators, and binding of variables within operators.  
 At any given time in the problem -solving process, we may have a set of useful operators 
but perhaps no clear idea of how those operators should order with respect to each ot her. 
 A solution is a partially ordered, partially instantiated set of operators to generate an 
actual plan. And we convert the partial order into any number of total orders.  
Constraint Posting versus State Space search  
State Space Search  
 Moves in the space : Modify world state via operator  
 Model of time: Depth of node in search space  
 Plan stored in Series of state transitions  
Constraint Posting Search  
 Moves in the space: Add operators, Oder Operators, Bind variables Or Otherwise 
constrain plan  
 Model of Time:  Partially ordered set of operators  
 Plan stored in Single node  
Algorithm: Nonlinear Planning (TWEAK)  
1. Initialize S to be the set of propositions in the goal state.  
2. Remove some unachieved proposition P from S.  
101 
 3. Moreover, Achieve P by using step addition, prom otion, DE clobbering, simple 
establishment or separation.  
4. Review all the steps in the plan, including any new steps introduced by step addition, to 
see if any of their preconditions unachieved. Add to S the new set of unachieved 
preconditions.  
5. Also, If S i s empty, complete the plan by converting the partial order of steps into a total 
order, instantiate any variables as necessary.  
6. Otherwise, go to step 2.  
Hierarchical Planning  
 In order to solve hard problems, a problem solver may have to generate long plans . 
 It is important to be able to eliminate some of the details of the problem until a solution 
that addresses the main issues is found.  
 Then an attempt can make to fill in the appropriate details.  
 Early attempts to do this involved the use of macro operator s, in which larger operators 
were built from smaller ones.  
 In this approach, no details eliminated from actual descriptions of the operators.  
ABSTRIPS  
A better approach developed in ABSTRIPS systems which actually planned in a hierarchy of 
abstraction spac es, in each of which preconditions at a lower level of abstraction ignored.  
ABSTRIPS approach is as follows:  
 First solve the problem completely, considering only preconditions whose criticality 
value is the highest possible.  
 These values reflect the expect ed difficulty of satisfying the precondition.  
 To do this, do exactly what STRIPS did, but simply ignore the preconditions of lower 
than peak criticality.  
 Once this done, use the constructed plan as the outline of a complete plan and consider 
preconditions at the next -lowest criticality level.  
 Augment the plan with operators that satisfy those preconditions.  
 Because this approach explores entire plans at one level of detail before it looks at the 
lower -level details of any one of them, it has called length -first approach.  
The assignment of appropriate criticality value is crucial to the success of this hierarchical 
planning method.  
Those preconditions that no operator can satisfy are clearly the most critical.  
Example, solving a problem of moving the robot, f or applying an operator, PUSH -THROUGH 
DOOR, the precondition that there exist a door big enough for the robot to get through is of high 
criticality since there is nothing we can do about it if it is not true.  
 
 
 
 
Other Planning Techniques  
Reactive Systems  
102 
  The idea of reactive systems is to avoid planning altogether, and instead, use the 
observable situation as a clue to which one can simply react.  
 A reactive system must have access to a knowledge base of some sort that describes what 
actions should be taken  under what circumstances.  
 A reactive system is very different from the other kinds of planning systems we have 
discussed. Because it chooses actions one at a time.  
 It does not anticipate and select an entire action sequence before it does the first thing.  
 The example is a Thermostat. The job of the thermostat is to keep the temperature 
constant inside a room.  
 Reactive systems are capable of surprisingly complex behaviors.  
 The main advantage reactive systems have over traditional planners is that they opera te 
robustly in domains that are difficult to model completely and accurately.  
 Reactive systems dispense with modeling altogether and base their actions directly on 
their perception of the world.  
 Another advantage of reactive systems is that they are extrem ely responsive since they 
avoid the combinatorial explosion involved in deliberative planning.  
 This makes them attractive for real -time tasks such as driving and walking.  
Other Planning Techniques  
Triangle tables  
 Provides a way of recording the goals that each operator expected to satisfy as well as the 
goals that must be true for it to execute correctly.  
Meta -planning  
 A technique for reasoning not just about the problem solved but also about the planning 
process itself.  
Macro -operators  
 Allow a planner to b uild new operators that represent commonly used sequences of 
operators.  
Case -based planning:  
 Re-uses old plans to make new ones.  
UNDERSTANDING  
Understanding is the simplest procedure of all human beings. Understanding means ability to 
determine some new k nowledge from a given knowledge. For each action of a problem, the 
mapping of some new actions is very necessary. Mapping the knowledge means transferring the 
knowledge from one representation to another representation. For example, if you will say “I 
need  to go to New Delhi” for which you will book the tickets. The system will have 
“understood” if it finds the first available plane to New Delhi. But if you will say the same thing 
to you friends, who knows that your family lives in “New Delhi”, he/she will have “understood” 
if he/she realizes that there may be a problem or occasion in your family. For people, 
understanding applies to inputs from all the senses. Computer understanding has so far been 
applied primarily to images, speech and typed languages. It  is important to keep in mind that the 
success or failure of an “understanding” problem can rarely be measured in an absolute sense but 
must instead be measured with respect to a particular task to be performed. There are some 
factors that contribute to th e difficulty of an understanding problem.  
(a) If the target representation is very complex for which you cannot map from the original 
representation.  
103 
 (b) There are different types of mapping factors may arise like one -to-one, one -to-many and 
many  to many .  
(c) Some noise or disturbing factors are also there.  
(d) The level of interaction of the source components may be complex one.  
(e) The problem solver might be unknown about some more complex problems.  
(f) The intermediary actions may also be unavaila ble.  
 
Consider an example of an English sentence which is being used for communication with a 
keyword  based data retrieval system. Suppose I want to know all about the temples in India. So I 
would need to be translated into a representation such as  The ab ove sentence is a simple sentence 
for which the corresponding representation may be easy to implement. But what for the complex 
queries?  
 
Consider the following query.  
“Ram told Sita he would not eat apple with her. He has to go to the office”.  
 
This ty pe of complex queries can be modeled with the conceptual dependency representation 
which is more complex than that of simple representation. Constructing these queries is very 
difficult since more informationare to be extracted. Extracting more information  will require 
some more knowledge. Also the type of mapping process is not quite easy to the problem solver. 
Understanding is the process of mapping an input from its original form to a more useful one. 
The simplest kind of mapping is “one -toone”.  
In one -to-one mapping each different problems would lead to only one solution. But there are 
very few inputs which are one -to-one. Other mappings are quite difficult to implement. Many -to-
one mappings are frequent is that free variation is often allowed, either b ecause of the physical 
limitations of that produces the inputs or because such variation simply makes the task of 
generating the inputs.  
Many  to one mapping require that the understanding system know about all the ways that a target 
representation can be expressed in the source language. One -to-many mapping requires a great 
deal of domain knowledge in order to make the correct choice among the available target 
representation.  
The mapping process is simplest if each component can be mapped without concern for the other 
components of the statement. If the number of interactions increases, then the complexity of the 
problem will increase. In many understanding situations the input to which meaning should be 
assigned is not always the input that is presented t o the under stander.  
Because of the complex environment in which understanding usually occurs, other things often 
interfere with the basic input before it reaches the under stander. Hence the understanding will be 
more complex if there will be some sort o f noise on the inputs.  
 
 
 
 
Natural Language Processing  
Introduction to  Natural Language Processing   
104 
  Language meant for communicating with the world.  
 Also, By studying language, we can come to understand more about the world.  
 If we can succeed at building c omputational mode of language, we will have a powerful 
tool for communicating with the world.  
 Also, We look at how we can exploit knowledge about the world, in combination with 
linguistic facts, to build computational natural language systems.  
Natural Lang uage Processing (NLP) problem can divide into two tasks:  
1. Processing written text, using lexical, syntactic and semantic knowledge of the language 
as well as the required real -world information.  
2. Processing spoken language, using all the information needed a bove plus additional 
knowledge about phonology as well as enough added information to handle the further 
ambiguities that arise in speech.  
Steps in Natural Language Processing  
Morphological Analysis  
 Individual words analyzed into their components and non -word tokens such as 
punctuation separated from the words.  
Syntactic Analysis  
 Linear sequences of words transformed into structures that show how the words relate to 
each other.  
 Moreover, Some word sequences may reject if they violate the language’s rule for  how 
words may combine.  
Semantic Analysis  
 The structures created by the syntactic analyzer assigned meanings.  
 Also, A mapping made between the syntactic structures and objects in the task domain.  
 Moreover, Structures for which no such mapping possible may reject.  
Discourse integration  
 The meaning of an individual sentence may depend on the sentences that precede it. And 
also, may influence the meanings of the sentences that follow it.  
Pragmatic Analysis  
 Moreover, The structure representing what said reinter preted to determine what was 
actually meant.  
Summary  
 Results of each of the main processes combine to form a natural language system.  
 All of the processes are important in a complete natural language understanding system.  
 Not all programs are written with exactly these components.  
 Sometimes two or more of them collapsed.  
 Doing that usually results in a system that is easier to build for restricted subsets of 
English but one that is harder to extend to wider coverage.  
 
 
Steps Natural Language Processing  
Morp hological Analysis  
 Suppose we have an English interface to an operating system and the following sentence 
typed: I want to print Bill’s .init file.  
 The morphological analysis must do the following things:  
105 
  Pull apart the word “Bill’s” into proper noun “Bill ” and the possessive suffix “’s”  
 Recognize the sequence “.init” as a file extension that is functioning as an adjective in the 
sentence.  
 This process will usually assign syntactic categories to all the words in the sentence.  
Syntactic Analysis  
 A syntactic analysis must exploit the results of the morphological analysis to build a 
structural description of the sentence.  
 The goal of this process, called parsing, is to convert the flat list of words that form the 
sentence into a structure that defines the units  that represented by that flat list.  
 The important thing here is that a flat sentence has been converted into a hierarchical 
structure. And that the structure corresponds to meaning units when a semantic analysis 
performed.  
 Reference markers (set of entiti es) shown in the parenthesis in the parse tree.  
 Each one corresponds to some entity that has mentioned in the sentence.  
 These reference markers are useful later since they provide a place in which to 
accumulate information about the entities as we get it.  
 
Semantic Analysis  
 The semantic analysis must do two important things:  
1. It must map individual words into appropriat e objects in the knowledge base or 
database.  
2. It must create the correct structures to correspond to the way the meanings of the 
individual words combine with each other.  
Discourse Integration  
 Specifically, we do not know whom the pronoun “I” or the proper noun “Bill” refers to.  
 To pin down these references requires an appeal to a model of the current discourse 
context, from which we can learn that the current user is USER068 and that the only 
person named “Bill” about whom we could be talking is USER073.  
 Once the correct referent for Bill known, we can also determine exactly which file 
referred to.  
Pragmatic Analysis  
 The final step toward effective understanding is to decide what to do as a result.  
106 
  One possible thing to do to record what was said as a fact a nd done with it.  
 For some sentences, a whose intended effect is clearly declarative, that is the precisely 
correct thing to do.  
 But for other sentences, including this one, the intended effect is different.  
 We can discover this intended effect by applying a set of rules that characterize 
cooperative dialogues.  
 The final step in pragmatic processing to translate, from the knowledge -based 
representation to a command to be executed by the system.  
Syntactic Processing  
 Syntactic Processing is the step in which a  flat input sentence converted into a 
hierarchical structure that corresponds to the units of meaning in the sentence. This 
process called parsing.  
 It plays an important role in natural language understanding systems for two reasons:  
1. Semantic processing mu st operate on sentence constituents. If there is no syntactic 
parsing step, then the semantics system must decide on its own constituents. If 
parsing is done, on the other hand, it constrains the number of constituents that 
semantics can consider.  
2. Syntacti c parsing is computationally less expensive than is semantic processing. 
Thus it can play a significant role in reducing overall system complexity.  
 Although it is often possible to extract the meaning of a sentence without using 
grammatical facts, it is no t always possible to do so.  
 Almost all the systems that are actually used have two main components:  
1. A declarative representation, called a grammar, of the syntactic facts about the 
language.  
2. A procedure, called parser that compares the grammar against inpu t sentences to 
produce parsed structures.  
Grammars and Parsers  
 The most common way to represent grammars is a set of production rules.  
 The first rule can read as “A sentence composed of a noun phrase followed by Verb 
Phrase”; the Vertical bar is OR; ε represents the empty string.  
 Symbols that further expanded by rules called non -terminal symbols.  
 Symbols that correspond directly to strings that must found in an input sentence called 
terminal symbols.  
 Grammar formalism such as this one underlies many l inguistic theories, which in turn 
provide the basis for many natural language understanding systems.  
 Pure context -free grammars are not effective for describing natural languages.  
 NLPs have less in common with computer language processing systems such as 
compilers.  
 Parsing process takes the rules of the grammar and compares them against the input 
sentence.  
 The simplest structure to build is a Parse Tree, which simply records the rules and how 
they matched.  
 Every node of the parse tree corresponds either to an input word or to a non -terminal in 
our grammar.  
 Each level in the parse tree corresponds to the application of one grammar rule.  
107 
 Example for Syntactic Processing – Augmented Transition 
Network  
Syntactic Processing is the step in which a flat input sente nce is converted into a hierarchical 
structure that corresponds to the units of meaning in the sentence. This process called parsing.  
It plays an important role in natural language understanding systems for two reasons:  
1. Semantic processing must operate on sentence constituents. If there is no syntactic 
parsing step, then the semantics system must decide on its own constituents. If parsing is 
done, on the other hand, it constrains the number of constituents that semantics can 
consider.  
2. Syntactic parsing is c omputationally less expensive than is semantic processing. Thus it 
can play a significant role in reducing overall system complexity.  
Example: A Parse tree for a sentence: Bill Printed the file   
 
The grammar specifies two things about a language:  
1. Its weak generative capacity, by which we mean the set of sentences that contained 
within the language. This set m ade up of precisely those sentences that can completely 
match by a series of rules in the grammar.  
2. Its strong generative capacity, by which we mean the structure to assign to each 
grammatical sentence of the language.  
Augmented Transition Network (ATN)  
 An augmented transition network is a top -down parsing procedure that allows various 
kinds of knowledge to incorporated into the parsing system so it can operate efficiently.  
 ATNs build on the idea of using finite state machines (Markov model) to parse sentenc es. 
 Instead of building an automaton for a particular sentence, a collection of transition 
graphs built.  
 A grammatically correct sentence parsed by reaching a final state in any state graph.  
 Transitions between these graphs simply subroutine calls from one  state to any initial 
state on any graph in the network.  
 A sentence determined to be grammatically correct if a final state reached by the last 
word in the sentence.  
 The ATN is similar to a finite state machine in which the class of labels that can attach to 
the arcs that define the transition between states has augmented.  
Arcs may label with:  
 Specific words such as “in’.  
 Word categories such as noun.  
108 
  Procedures that build structures that will form part of the final parse.  
 Procedures that perform arbitrary tests on current input and sentence components that 
have identified.  
Semantic Analysis   
 The structures created by the syntactic analyzer assigned meanings.  
 A mapping made between the syntactic structures and objects in the task domain.  
 Structures for which  no such mapping is possible may rejected.  
 The semantic analysis must do two important things:  
 It must map individual words into appropriate objects in the knowledge base or 
database.  
 It must create the correct structures to correspond to the way the meani ngs of the 
individual words combine with each other.  Semantic Analysis AI  
 Producing a syntactic parse of a sentence is only the first step toward understanding it.  
 We must produce a representation of the meaning of the sentence.  
 Because understanding is a mapping process, we must first define the language into 
which we are trying to map.  
 There is no single definitive language in which all sentence meaning can describe.  
 The choice of a target language for any particular natural language understanding 
program  must depend on what is to  do with the meanings once they constructed.  
 Choice of the target language in Semantic Analysis AI  
 There are two broad families of target languages that used in NL systems, 
depending on the role that the natural language system pl aying in a larger system:  
 When natural language considered as a phenomenon on its own, as for example 
when one builds a program whose goal is to read the text and then answer 
questions about it. A target language can design specifically to support language  
processing.  
 When natural language used as an interface language to another program (such as 
a db query system or an expert system), then the target language must legal input 
to that other program. Thus the design of the target language driven by the 
backe nd program.  
Discourse and Pragmatic Processing  
To understand a single sentence, it is necessary to consider the discourse and pragmatic context 
in which the sentence was uttered.  
There are a number of important relationships that may hold between phrases a nd parts of their 
discourse contexts, including:  
1. Identical entities. Consider the text:  
 Bill had a red balloon. o John wanted it.  
 The word “it” should identify as referring to the red balloon. These types of 
references called anaphora.  
2. Parts of entities. C onsider the text:  
 Sue opened the book she just bought.  
 The title page was torn.  
 The phrase “title page” should be recognized as part of the book that was just 
bought.  
109 
 3. Parts of actions. Consider the text:  
 John went on a business trip to New York.  
 He left on  an early morning flight.  
 Taking a flight should recognize as part of going on a trip.  
4. Entities involved in actions. Consider the text:  
 My house was broken into last week.  
 Moreover, They took the TV and the stereo.  
 The pronoun “they” should recognize as re ferring to the burglars who broke into 
the house.  
5. Elements of sets. Consider the text:  
 The decals we have in stock are stars, the moon, item and a flag.  
 I’ll take two moons.  
 Moons mean moon decals.  
6. Names of individuals:  
 Dev went to the movies.  
7. Causal chain s 
 There was a big snow storm yesterday.  
 So, The schools closed today.  
8. Planning sequences:  
 Sally wanted a new car  
 She decided to get a job.  
9. Implicit presuppositions:  
 Did Joe fail CS101?  
The major focus is on using following kinds of knowledge:  
 The current f ocus of the dialogue.  
 Also, A model of each participant’s current beliefs.  
 Moreover, The goal -driven character of dialogue.  
 The rules of conversation shared by all participants.  
 
Statistical Natural Language Processing  
Formerly, many language -processing ta sks typically involved the direct hand coding of 
rules,  which is not in general robust to natural -language variation. The machine -learning 
paradigm calls instead for using  statistical inference  to automatically learn such rules through the 
analysis of large  corpora  of typical real -world examples (a  corpus  (plural, "corpora") is a set of 
documents, possibly with human or computer annotations).  
Many different classes of machine learning algorithms have been applied to natural -language 
processing tasks. These algorithms take as input a large set of "features" that are generated from 
the input d ata. Some of the earliest -used algorithms, such as  decision trees , produced systems of 
hard if -then rules similar to the systems of hand -written rules that were then common. 
Increasingly, however, research has focused on  statistical models , which make 
soft, probabil istic decisions based on attaching  real-valued  weights to each input feature. Such 
models have the advantage that they can express the relative  certainty of many different possible  
answers rather than only one, producing more reliable results when such a model is included as a 
component of a larger system.  
Systems based on machine -learning algorithms have many advantages over hand -produced rules:  
110 
  The learning procedures used during machine learning automatically focus on the most 
common cases, whereas when writing rules by hand it is often not at all obvious where 
the effort should be directed.  
 Automatic learning procedures can make use of statistical inference algorithms to 
produce models that are robust to unfamiliar input (e.g. containing words or structures 
that have not been seen before) and to erroneous input (e.g. with misspelled words or 
words accidentally omitted). Generally, handling such input gracefully with hand -written 
rules—or more generally, creating systems of hand -written rules that make soft 
decisions —is extremely difficult, error -prone and time -consuming.  
 Systems based on automatically learning the rules can be made more accurate simply by 
supplying more input data.  However, systems based on hand -written rules can only be 
made more accurate by increasing the complexity of the rules, which is a much more 
difficult task. In particular, there is a limit to the complexity of systems based on hand -
crafted rules, beyond wh ich the systems become more and more unmanageable. 
However, creating more data to input to machine -learning systems simply requires a 
corresponding increase in the number of man -hours worked, generally without significant 
increases in the complexity of the  annotation process.  
Spell Checking  
Spell checking is one of the applications of natural language processing that impacts billions of 
users daily. A good introduction to spell checking can be found on Peter Norvig’s webpage. The 
article introduces a simple  21-line spell checker implementation in Python combining simple 
language and error models to predict the word a user intended to type.  The language model 
estimates how likely a given word `c` is in the language for which the spell checker is 
designed, thi s can be written as `P(C)`. The error model estimates the probability `P(w|c)` 
of typing the misspelled version `w` conditionally to the intention of typing the correctly 
spelled word `c`. The spell checker then returns word `c` corresponding to the highest  value of 
`P(w|c)P(c)` among all possible words in the language.  
 
 
 
 
 
 
 
 Module 3   
LEARNING  
Learning is the improvement of performance with experience over time.  
Learning element is the portion of a learning AI system that decides how to modify the 
perfor mance element and implements those modifications.  
We all learn new knowledge through different methods, depending on the type of material to be 
learned, the amount of relevant knowledge we already possess, and the environment in which the 
learning takes pl ace. There are five methods of learning . They are,  
1. Memorization (rote learning)  
2. Direct instruction (by being told)  
3. Analogy  
4. Induction  
111 
 5. Deduction  
Learning by memorizations is the simplest from of le4arning. It requires the least amount of 
inference and is accomplished by simply copying the knowledge in the same form that it will be 
used directly into the knowledge base.  
Example: - Memorizing multiplication tables, formulate , etc.  
Direct instruction is a complex form of learning. This type of le arning requires more inference 
than role learning since the knowledge must be transformed into an operational form before 
learning when a teacher presents a number of facts directly to us in a well organized manner.  
Analogical learning is the process of le arning a new concept or solution through the use of 
similar known concepts or solutions. We use this type of learning when solving problems on an 
exam where previously learned examples serve as a guide or when make frequent use of 
analogical learning. This  form of learning requires still more inferring than either of the previous 
forms. Since difficult transformations must be made between the known and unknown situations.  
Learning by induction is also one that is used frequently by humans . it is a powerful  form of 
learning like analogical learning which also require s more inferring than the first two methods. 
This learning re  quires the use of inductive inference, a form of invalid but useful inference. We 
use inductive learning  ofinstances  of examples  of the concept.  For example  we learn the  
concepts of  color or sweet taste after experiencing the sensations associated with several 
examples of colored objects or sweet foods.  
Deductive learning is accomplished through a sequence of deductive inference steps using 
known facts. From the known facts, new facts or relationships are logically derived. Deductive 
learning usually requires more inference than the other methods.  
Review Questions: - 
1. what is perception ?  
2. How do we overcome the Perceptual Problems?  
3. Explain in detail the constraint satisfaction waltz algorithm?  
4. What is learning ?  
5. What is Learning element ?  
6. List and explain the methods of learning?  
Types of learning :- Classification or taxonomy of learning types serves as a guide in studyin g or 
comparing a differences among them. One can develop learning taxonomies based on the type of 
knowledge representation used (predicate calculus , rules, frames), the type of knowledge 
learned (concepts, game playing, problem solving), or by the area of  application(medical 
diagnosis , scheduling , prediction and so on).  
The classification is intuitively more appealing and is one which has become popular among 
machine learning researchers . it is independent of the knowledge domain and the representation 
scheme is used. It is based on the type of inference strategy employed or the methods used in the 
learning process. The five different learning methods under this taxonomy are:  
Memorization (rote learning)  
Direct instruction(by being told)  
Analogy  
Inductio n 
Deduction  
Learning by memorization is the simplest form of learning. It requires the least5 amount of 
inference and is accomplished by simply copying the knowledge in the same form that it will be 
112 
 used directly into the knowledge base. We use this type o f learning when we memorize 
multiplication tables ,  
for example.  
A slightly more complex form of learning is by direct instruction. This type of learning requires 
more understanding and inference than role learning since the knowledge must be transformed 
into an operational form before being integrated into the knowledge base. We use this type of 
learning when a teacher presents a number of facts directly to us in  a well  organized manner.  
The third type listed, analogical learning, is the process of learnin g an ew concept or solution 
through the use of similar known concepts or solutions. We use this type of learning when 
solving problems on an examination where previously learned examples serve as a guide or 
when we learn to drive a truck using our knowledg e of car driving. We make frewuence use of 
analogical learning. This form of learning requires still more inferring than either of the previous 
forms, since difficult transformations must be made between the known and unknown situations. 
This is a kind of application of knowledge in a new situation.  
The fourth type of learning is also one that is used frequency by humans. It is a powerful form of 
learning which, like analogical learning, also requires more inferring than the first two methods. 
This form of learning requires the use of inductive inference, a form of invalid but useful 
inference. We use inductive learning when wed formulate a general concept after seeing a 
number of instance or examples of the concept. For example, we learn the concepts of col or 
sweet taste after experiencing the sensation associated with several examples of colored objects 
or sweet foods.  
The final type of acquisition is deductive learning. It is accomplished through a sequence of 
deductive inference steps using known facts. F rom the known facts, new facts or relationships 
are logically  derived. Deductive learning usually requires more inference than the other methods. 
The inference method used is, of course , a deductive type, which is a valid from of inference.  
In addition to  the above classification, we will sometimes refer to learning methods as wither 
methods or knowledge -rich methods. Weak methods are general purpose methods in which little 
or no initial knowledge is available. These methods are more mechanical than the cl assical AI 
knowledge – rich methods. They often rely on a form of heuristics search in the learning process.  
 
Rote Learning  
Rote learning is the basic learning activity. Rote learning  is a memorization  technique based 
on repetition . It is also called memorization because the  knowledge , without any modification is, 
simply copied into the knowledge base. As computed values are stored, this technique can save a 
significant amount of time.  
Rote learning technique can also be used in complex  learning sy stems  provided sophisticated 
techniques are employed to use the stored values faster and there is a generalization to keep the 
number of stored information down to a manageable level. Checkers -playing  program,  for ex   
The idea is that one will be able to q uickly  recall  the meaning of the material the more one 
repeats it. Some of the alternatives to rote learning include  meaningful learning , associative 
learning , and  active learning . ample, uses this technique to learn the board positions it evaluates 
in its  look-ahead search.  
113 
 Learning By Taking Advice .  
 
This is a simple form of learning. Suppose a programmer writes a set of instructions to instruct 
the computer what to do, the programmer is a teacher and the computer is a student. Once 
learned (i.e. program med), the system will be in a position to do new things.  
 
The advice may come from many sources: human experts, internet to name a few. This type of 
learning requires more inference than rote learning. The knowledge must be transformed into an 
operational form before stored in the knowledge base. Moreover the reliability of the source of 
knowledge should be considered.  
The system should ensure that the new knowledge is conflicting with the existing knowledge. 
FOO (First Operational Operationaliser), for exa mple, is a learning system which is used to learn 
the game of Hearts. It converts the advice which is in the form of principles, problems, and 
methods into effective executable (LISP) procedures (or knowledge). Now this knowledge is 
ready to use.  
 
General Learning Model.  
General Learning Model: - AS noted earlier, learning can be accomplished using a number of 
different methods, such as by memorization facts, by being told, or by studying examples like 
problem solution. Learning requires that new knowledge structures be created from some form of 
input stimulus. This new knowledge must then be assimilated into a knowledge base and be 
tested in some way for its utility. Testing means that the knowledge should be used in 
performance of some task from which mean ingful feedback can be obtained, where the feedback 
provides some measure of the accuracy and usefulness of the newly acquired knowledge.  
General Learning Model  
 
general learning model is depicted in figure 4.1 where the environment has been incl uded as a 
part of the overall learner system. The environment may be regarded as either a form of nature 
which produces random stimuli or as a more organized training source such as a teacher which 
provides carefully selected training examples for the lear ner component. The actual form of 
environment used will depend on the particular learning paradigm. In any case, some 
representation language must be assumed for communication between the environment and the 
learner. The language may be the same representa tion scheme as that used in the knowledge base 
(such as a form of predicate calculus). When they are hosen to be the same, we say the single 
representation trick is being used. This usually results in a simpler implementation since it is not 
necessary to t ransform between two or more different representations.  
114 
  
For some systems the environment may be a user working at a keyboard . Other systems will use 
program modules to simulate a particular environment. In even more realistic cases the system 
will have r eal physical sensors which interface with some world environment.  
 
Inputs to the learner component may be physical stimuli of some type or descriptive , symbolic 
training examples. The information conveyed to the learner component is used to create and 
modify knowledge structures in the knowledge base. This same knowledge is used by the 
performance component to carry out some tasks, such as solving a problem playing a game, or 
classifying instances of some concept.  
 
 given a task, the performance component produces a response describing its action in 
performing the task. The critic module then evaluates this response relative to an optimal 
response.  
 
Feedback , indicating whether or not the performance was acceptable , is then sent by the critic 
module to th e learner component for its subsequent use in modifying the structures in the 
knowledge base. If proper learning was accomplished, the system’s performance will have 
improved with the changes made to the knowledge base.  
 
The cycle described above may be re peated a number of times until the performance of the 
system has reached some acceptable level, until a known learning goal has been reached, or until 
changes ceases to occur in the knowledge base after some chosen number of training examples 
have been obs erved.  
 
There are several important factors which influence a system’s ability to learn in addition to the 
form of representation used. They include the types of training provided, the form and extent of 
any initial background knowledge , the type of feedb ack provided, and the learning algorithms 
used.   
 
The type of training used in a system can have a strong effect on performance, much the same as 
it does for humans. Training may consist of randomly selected instance or examples that have 
been carefully se lected and ordered for presentation. The instances may be positive examples of 
some concept or task a being learned, they may be negative, or they may be mixture of both 
positive and negative. The instances may be well focused using only relevant informati on, or 
they may contain a variety of facts and details including irrelevant data.  
 
There are Many forms of learning can be characterized as a search through a space of possible 
hypotheses or solutions. To make learning more efficient. It is necessary to co nstrain this search 
process or reduce the search space. One method of achieving this is through the use of 
background knowledge which can be used to constrain the search space or exercise control 
operations which limit the search process.  
 
Feedback is esse ntial to the learner component since otherwise it would never know if the 
knowledge structures in the knowledge base were improving or if they were adequate for the 
performance of the given tasks. The feedback may be a simple yes or no type of evaluation, or it 
115 
 may contain more useful information describing why a particular action was good or bad. Also , 
the feedback may be completely reliable, providing an accurate assessment of the performance or 
it may contain noise, that is the feedback may actually be incorrect some of the time. Intuitively , 
the feedback must be accurate more than 50% of the time; otherwise the system carries useful 
information, the learner should also to build up a useful corpus of knowledge quickly. On the 
other hand, if the feedback  is noisy or unreliable, the learning process may be very slow and the 
resultant knowledge incorrect.  
 
Learning Neural Network  
Perceptron   
 The perceptron an invention of (1962) Rosenblatt was one of the earliest neural network 
models.  
 Also, It models a neu ron by taking a weighted sum of its inputs and sending the output 1 
if the sum is greater than some adjustable threshold value (otherwise it sends 0).  
 
Figure: A neuron & a Perceptron  
 
Figure: Perceptron with adjustable threshold  
 In case of zero with two inputs g(x) = w0 + w1x1 + w2x2 = 0  
116 
  x2 =  -(w1/w2)x1   – (w0/w2)   → equation for a line  
 the location of the line is determined  by the weight w0 w1 and w2  
 if an input vector lies on one side of the line, the perceptron will output 1  
 if it lies on the other side, the perception will output 0  
 Moreover, Decision surface: a line that correctly separates the training instances 
correspo nds to a perfectly function perceptron.  
Perceptron Learning Algorithm  
Given: A classification problem with n input feature (x 1, x2, …., x n) and two output classes.  
Compute A set of weights (w 0, w1, w2,….,w n) that will cause a perceptron to fire whenever th e 
input falls into the first output class.  
1. Create a perceptron with n+ 1 input and n+ 1 weight, where the x 0 is always set to 1.  
2. Initialize the weights (w 0, w1,…., w n) to random real values.  
3. Iterate through the training set, collecting all examples  misclas sified  by the current set of 
weights.  
4. If all examples are classified correctly, output the weights and quit.  
5. Otherwise, compute the vector sum S of the misclassified input vectors where each vector 
has the form (x 0, x1, …, Xn). In creating the sum, add to S a vector x if x  is an input for 
which the perceptron incorrectly fails to fire, but – x  if x  is an input for which the 
perceptron incorrectly fires. Multiply sum by a scale factor η.  
6. Moreover, Modify the weights (w 0, w1, …, w n) by adding the elements of the vector S to 
them.  
7. Go to step 3.  
 The perceptron learning algorithm is a search algorithm. It begins with a random initial 
state and finds a solution state. The search space is simply all possible assignments of real 
values to the weights of the perce ption, and the search strategy is gradient descent.  
 The perceptron learning rule is guaranteed to converge to a solution in a finite number of 
steps, so long as a solution exists.  
 Moreover, This brings us to an important question. What problems can a perce ptron 
solve? Recall that a single -neuron perceptron is able to divide the input space into two 
regions.  
 Also, The perception can be used to classify input vectors that can be separated by a 
linear boundary. We call such vectors linearly separable.  
 Unfortun ately, many problems are not linearly separable. The classic example is the XOR 
gate. It was the inability of the basic perceptron to solve such simple problems that are 
not linearly separable or non -linear.  
Genetic Learning  
Supervised Learning   
Supervised  learning is the machine learning task of inferring a function from labeled training 
data.  
Moreover, The training data consist of a set of training examples.  
In supervised learning, each example a pair consisting of an input object (typically a vector) and  
the desired output value (also called the supervisory signal).  
Training set   
A training set a set of data used in various areas of information science to discover potentially 
predictive relationships.  
117 
 Training sets used in artificial intelligence, machine  learning, genetic programming, intelligent 
systems, and statistics.  
In all these fields, a training set has much the same role and often used in conjunction with a test 
set. 
Testing set   
A test set  is a set of data used in various areas of information sci ence to assess the strength and 
utility of a predictive relationship.  
Moreover, Test sets are used in artificial intelligence, machine learning, genetic programming, 
and statistics. In all these fields, a test set has much the same role.  
Accuracy of classi fier: Supervised learning   
In the fields of science, engineering, industry, and statistics. The accuracy of a measurement 
system is the degree of closeness of measurements of a quantity to that quantity’s actual (true) 
value.  
Sensitivity analysis:  Supervis ed learning   
Similarly, Local Sensitivity as correlation coefficients and partial derivatives can only use, if the 
correlation between input and output is linear.  
Regression: Supervised learning   
In statistics,  regression analysis  is a statistical process for estimating the relationships among 
variables.  
Moreover, It includes many techniques for modeling and analyzing several variables. When the 
focus on the relationship between a dependent variable and one or more independent variables.  
More specifically, regression analysis helps one understand how the typical value of the 
dependent variable (or ‘criterion variable’) changes when any one of the independent variables 
varied. Moreover,   While the other independent variables held fixed.  
 
Expert systems : 
 
Expe rt system  = knowledge  + problem -solving methods . ... A  knowledge  base that captures 
the domain -specific  knowledge  and an inference engine that consists of algorithms for 
manipulating the  knowledge represented  in the  knowledge  base to solve a problem presen ted to 
the system.  
Expert systems (ES) are one of the prominent research domains of AI. It is introduced by the 
researchers at Stanford University, Computer Science Department.  
 
What are Expert Systems?  
 
The expert systems are the computer applications dev eloped to solve complex problems in a 
particular domain, at the level of extra -ordinary human intelligence and expertise.  
 
Characteristics of Expert Systems  
 High performance  
 Understandable  
 Reliable  
 Highly responsive  
 
118 
 Capabilities of Expert Systems  
The expe rt systems are capable of −  
 Advising  
 Instructing and assisting human in decision making  
 Demonstrating  
 Deriving a solution  
 Diagnosing  
 Explaining  
 Interpreting input  
 Predicting results  
 Justifying the conclusion  
 Suggesting alternative options to a problem  
They  are incapable of −  
 Substituting human decision makers  
 Possessing human capabilities  
 Producing accurate output for inadequate knowledge base  
 Refining their own knowledge  
Components of Expert Systems  
The components of ES include −  
 Knowledge Base  
 Inference E ngine  
 User Interface  
Let us see them one by one briefly −  
 
Knowledge Base  
It contains domain -specific and high -quality knowledge. Knowl edge is required to exhibit 
intelligence. The success of any ES majorly depends upon the collection of highly accurate and 
precise knowledge.  
119 
 What is Knowledge?  
The data is collection of facts. The information is organized as data and facts about the task 
domain.  Data, information,  and past experience  combined together are termed as knowledge.  
Components of Knowledge Base  
The knowledge base of an ES is a store of both, factual and heuristic knowledge.  
 Factual Knowledge  − It is the information widely accepted by the Knowledge Engineers 
and scholars in the task domain.  
 Heuristic Knowledge  − It is about practice, accurate judgement, one’s ability of 
evaluation, and guessing.  
Knowledge representation  
It is the method used to organize and formalize the knowledge in the knowledge base. It is in the 
form of IF -THEN -ELSE rules.  
Knowledge Acquisition  
The success of any expert system majorly depends on the quality, completeness, and accuracy of 
the information sto red in the knowledge base.  
The knowledge base is formed by readings from various experts, scholars, and the  Knowledge 
Engineers. The knowledge engineer is a person with the qualities of empathy, quick learning, 
and case analyzing skills.  
He acquires inform ation from subject expert by recording, interviewing, and observing him at 
work, etc. He then categorizes and organizes the information in a meaningful way, in the form of 
IF-THEN -ELSE rules, to be used by interference machine. The knowledge engineer also 
monitors the development of the ES.  
Inference Engine  
Use of efficient procedures and rules by the Inference Engine is essential in deducting a correct, 
flawless solution.  
In case of knowledge -based ES, the Inference Engine acquires and manipulates the know ledge 
from the knowledge base to arrive at a particular solution.  
In case of rule based ES, it −  
 Applies rules repeatedly to the facts, which are obtained from earlier rule application.  
 Adds new knowledge into the knowledge base if required.  
 Resolves rules  conflict when multiple rules are applicable to a particular case.  
To recommend a solution, the Inference Engine uses the following strategies −  
 Forward Chaining  
 Backward Chaining  
Forward Chaining  
It is a strategy of an expert system to answer the question , “What can happen next?”  
Here, the Inference Engine follows the chain of conditions and derivations and finally deduces 
the outcome. It considers all the facts and rules, and sorts them before concluding to a solution.  
This strategy is followed for workin g on conclusion, result, or effect. For example, prediction of 
share market status as an effect of changes in interest rates.  
120 
 
 
Backw ard Chaining  
With this strategy, an expert system finds out the answer to the question,  “Why this happened?”  
On the basis of what has already happened, the Inference Engine tries to find out which 
conditions could have happened in the past for this result.  This strategy is followed for finding 
out cause or reason. For example, diagnosis of blood cancer in humans.  
 
User Interface  
User interface provides interaction between user of the ES and the ES itself. It is generally 
Natural Language Processing so as to be used by the user who is well -versed in the task domain. 
The user of the ES need not be necessarily an expert in Artificial Inte lligence.  
It explains how the ES has arrived at a particular recommendation.  The explanation may appear 
in the following forms −  
 Natural language displayed on screen.  
 Verbal narrations in natural language.  
 Listing of rule numbers displayed on the screen.  
The user interface makes it easy to trace the credibility of the deductions.  
Requirements of Efficient ES User Interface  
 It should help users to accomplish their goals in shortest possible way.  
 It should be designed to work for user’s existing or desired work practices.  
 Its technology should be adaptable to user’s requirements; not  the other way round.  
 It should make efficient use of user input.  
Expert Systems Limitations  
No technology can offer easy and complete solution. Large systems are costly, require 
significant development time, and computer resources. ESs have their limitati ons which include 
− 
 Limitations of the technology  
121 
  Difficult knowledge acquisition  
 ES are difficult to maintain  
 High development costs  
Applications of Expert System  
The following table shows where ES can be applied.  
Application  Description  
Design Domain  Camera lens design, automobile design.  
Medical Domain  Diagnosis Systems to deduce cause of disease from observed 
data, conduction medical operations on humans.  
Monitoring Systems  Comparing data continuously with observed system or with 
prescribed behavior such as leakage monitoring in long 
petroleum pipeline.  
Process Control Systems  Controlling a physical process based on monitoring.  
Knowledge Domain  Finding out faults in vehicles, computers.  
Finance/Commerce  Detection of possible fraud, suspicious trans actions, stock 
market trading, Airline scheduling, cargo scheduling.  
Expert System Technology  
There are several levels of ES technologies available. Expert systems technologies include −  
 Expert System Development Environment  − The ES development environme nt includes 
hardware and tools. They are −  
o Workstations, minicomputers, mainframes.  
o High level Symbolic Programming Languages such as  LISt Programming (LISP) 
and PROgrammation en  LOGique (PROLOG).  
o Large databases.  
 Tools  − They reduce the effort and cost in volved in developing an expert system to large 
extent.  
o Powerful editors and debugging tools with multi -windows.  
o They provide rapid prototyping  
o Have Inbuilt definitions of model, knowledge representation, and inference 
design.  
 Shells  − A shell is nothing bu t an expert system without knowledge base. A shell 
provides the developers with knowledge acquisition, inference engine, user interface, and 
explanation facility. For example, few shells are given below −  
o Java Expert System Shell (JESS) that provides fully  developed Java API for 
creating an expert system.  
o Vidwan , a shell developed at the National Centre for Software Technology, 
Mumbai in 1993. It enables knowledge encoding in the form of IF -THEN rules.  
Development of Expert Systems: General Steps  
The proces s of ES development is iterative. Steps in developing the ES include −  
Identify Problem Domain  
122 
  The problem must be suitable for an expert system to solve it.  
 Find the experts in task domain for the ES project.  
 Establish cost -effectiveness of the system.  
Design the System  
 Identify the ES Technology  
 Know and establish the degree of integration with the other systems and databases.  
 Realize how the concepts can represent the domain knowledge best.  
Develop the Prototype  
From Knowledge Base: The knowledge enginee r works to −  
 Acquire domain knowledge from the expert.  
 Represent it in the form of If -THEN -ELSE rules.  
Test and Refine the Prototype  
 The knowledge engineer uses sample cases to test the prototype for any deficiencies in 
performance.  
 End users test the prot otypes of the ES.  
Develop and Complete the ES  
 Test and ensure the interaction of the ES with all elements of its environment, including 
end users, databases, and other information systems.  
 Document the ES project well.  
 Train the user to use ES.  
Maintain th e ES 
 Keep the knowledge base up -to-date by regular review and update.  
 Cater for new interfaces with other information systems, as those systems evolve.  
Benefits of Expert Systems  
 Availability  − They are easily available due to mass production of software.  
 Less Production Cost  − Production cost is reasonable. This makes them affordable.  
 Speed  − They offer great speed. They reduce the amount of work an individual puts in.  
 Less Error Rate  − Error rate is low as compared to human errors.  
 Reducing Risk  − They ca n work in the environment dangerous to humans.  
 Steady response  − They work steadily without getting motional, tensed or fatigued.  
 
Expert System.  
 
DEFINITION - An ex pert system is a computer program that simulates the judgement and 
behavior of a human or an organization that has expert knowledge and experience in a particular 
field. Typically, such a system contains a knowledge base containing accumulated experience 
and a set of rules for applying the knowledge base to each particular situation that is described to 
the program. Sophisticated expert systems can be enhanced with additions to the knowledge base 
or to the set of rules.  
 
Among the best -known expert systems have been those that play chess and that assist in medical 
diagnosis.  
 
An expert system  is software  that attempts to provide an answer to a problem, or clarify 
uncertainties where normally  one or more human  experts  would need to be consulted. Expert 
systems are most common in a specific  problem domai n, and is a traditional application and/or 
123 
 subfield of  artificial intelligence  (AI). A wide variety of methods can be used to simulate the 
performance of the expert; however, common to most or all are: 1) the creation of a  knowledge 
base which uses some  knowledge representation  structure to capture the knowledge of 
the Subject Matter Expert  (SME); 2) a process of gathering that knowledge from the SME and 
codifying it according to the structure, which is called  knowledge engineering ; and 3) once the 
system is developed, it is placed in the same real world  problem solving  situation as the human 
SME, typically as an aid to human workers or as a supplement to some information system. 
Expert systems may or may not have learning components.  
  
factors  
 
 
The MYCIN rule-based expert system introduced a quasi -probabilistic approach called certainty 
factors, whose rationale is explained below.  
 
A human, when reasoning, does not always make statements with 100% confidence: he might 
venture, "If Fritz is green, then he i s probably a frog" (after all, he might be a chameleon). This 
type of reasoning can be imitated using numeric values called confidences. For example, if it is 
known that Fritz is green, it might be concluded with 0.85 confidence that he is a frog; or, if i t is 
known that he is a frog, it might be concluded with 0.95 confidence that he hops. These certainty 
factor (CF) numbers quantify uncertainty in the degree to which the available evidence supports 
a hypothesis. They represent a degree of confirmation, an d are not probabilities in a Bayesian 
sense. The CF calculus, developed by Shortliffe & Buchanan, increases or decreases the CF 
associated with a hypothesis as each new piece of evidence becomes available. It can be mapped 
to a probability update, although  degrees of confirmation are not expected to obey the laws of 
probability. It is important to note, for example, that evidence for hypothesis H may have nothing 
to contribute to the degree to which Not_h is confirmed or disconfirmed (e.g., although a fever  
lends some support to a diagnosis of infection, fever does not disconfirm alternative hypotheses) 
and that the sum of CFs of many competing hypotheses may be greater than one (i.e., many 
hypotheses may be well confirmed based on available evidence).  
 
The CF approach to a rule -based expert system design does not have a widespread following, in 
part because of the difficulty of meaningfully assigning CFs a priori. (The above example of 
green creatures being likely to be frogs is excessively naive.) Alternati ve approaches to quasi -
probabilistic reasoning in expert systems involve fuzzy logic, which has a firmer mathematical 
foundation. Also, rule -engine shells such as Drools and Jess do not support probability 
manipulation: they use an alternative mechanism ca lled salience, which is used to prioritize the 
order of evaluation of activated rules.  
 
In certain areas, as in the tax -advice scenarios discussed below, probabilistic approaches are not 
acceptable. For instance, a 95% probability of being correct means a 5% probability of being 
wrong. The rules that are defined in such systems have no exceptions: they are only a means of 
achieving software flexibility when external circumstances change frequently. Because rules are 
stored as data, the core software does no t need to be rebuilt each time changes to federal and 
state tax codes are announced.  
 
124 
  
Chaining  
 
 
Two methods of reasoning when using inference rules are forward chaining and backward 
chaining.  
 
Forward chaining starts with the data available and uses the inference rules to extract more data 
until a desired goal is reached. An inference engine using forward chaining searches the 
inference rules until it finds one in which the if clause is known to be true. It then concludes the 
then clause and adds this inf ormation to its data. It continues to do this until a goal is reached. 
Because the data available determines which inference rules are used, this method is also 
classified as data driven.  
 
Backward chaining starts with a list of goals and works backwards t o see if there is data which 
will allow it to conclude any of these goals. An inference engine using backward chaining would 
search the inference rules until it finds one which has a then clause that matches a desired goal. If 
the if clause of that inferen ce rule is not known to be true, then it is added to the list of goals.  
 
SW Architecture.  
 
The following general points about expert systems and their architecture have been outlined:  
 
1. The sequence of steps taken to reach a conclusion is dynamically syn thesized with each new 
case. The sequence is not explicitly programmed at the time that the system is built.  
 
2. Expert systems can process multiple values for any problem parameter. This permits more 
than one line of reasoning to be pursued and the result s of incomplete (not fully determined) 
reasoning to be presented.  
 
3. Problem solving is accomplished by applying specific knowledge rather than specific 
technique. This is a key idea in expert systems technology. It reflects the belief that human 
experts do not process their knowledge differently from others, but they do possess different 
knowledge. With this philosophy, when one finds that their expert system does not produce the 
desired results, work begins to expand the knowledge base, not to re -program  the procedures.  
  
End user  
 
There are two styles of user -interface design followed by expert systems. In the original style of 
user interaction, the software takes the end -user through an interactive dialog. In the following 
example, a backward -chaining s ystem seeks to determine a set of restaurants to recommend:  
 
Q. Do you know which restaurant you want to go to?  
 
A. No  
 
125 
 Q. Is there any kind of food you would particularly like?  
 
A. No  
 
Q. Do you like spicy food?  
 
A. No  
 
Q. Do you usually drink wine with m eals?  
 
A. Yes  
 
Q. When you drink wine, is it French wine?  
 
A. Yes  
  
  
Participants  
  
There are generally three individuals having an interaction in an expert system. Primary among 
these is the end -user, the individual who uses the system for its problem so lving assistance. In 
the construction and maintenance of the system there are two other roles: the problem domain 
expert who builds the system and supplies the knowledge base, and a knowledge engineer who 
assists the experts in determining the representati on of their knowledge, enters this knowledge 
into an explanation module and who defines the inference technique required to solve the 
problem. Usually the knowledge engineer will represent the problem solving activity in the form 
of rules. When these rules  are created from domain expertise, the knowledge base stores the rules 
of the expert system.  
 
 Inference rule  
 
An understanding of the "inference rule" concept is important to understand expert systems. An 
inference rule is a conditional statement with tw o parts: an if clause and a then clause. This rule 
is what gives expert systems the ability to find solutions to diagnostic and prescriptive problems. 
An example of an inference rule is:  
 
If the restaurant choice includes French and the occasion is romanti c, 
 
Then the restaurant choice is definitely Paul Bocuse.  
  
Procedure node interface  
 
The function of the procedure node interface is to receive information from the procedures 
coordinator and create the appropriate procedure call. The ability to call a pr ocedure and receive 
information from that procedure can be viewed as simply a generalization of input from the 
external world. In some earlier expert systems external information could only be obtained in a 
126 
 predetermined manner, which only allowed certain information to be acquired. Through the 
knowledge base, this expert system disclosed in the cross -referenced application can invoke any 
procedure allowed on its host system. This makes the expert system useful in a much wider class 
of knowledge domains tha n if it had no external access or only limited external access.  
 
In the area of machine diagnostics using expert systems, particularly self -diagnostic applications, 
it is not possible to conclude the current state of "health" of a machine without some info rmation. 
The best source of information is the machine itself, for it contains much detailed information 
that could not reasonably be provided by the operator.  
 
The knowledge that is represented in the system appears in the rulebase. In the rulebase 
descri bed in the cross -referenced applications, there are basically four different types of objects, 
with the associated information:  
 
1. Classes: Questions asked to the user.  
 
2. Parameters: Place holders for character strings which may be variables that can be  inserted 
into a class question at the point in the question where the parameter is positioned.  
 
3. Procedures: Definitions of calls to external procedures.  
 
3. Rule nodes: Inferences in the system are made by a tree structure which indicates the 
rules or log ic mimicking human reasoning. The nodes of these trees are called rule nodes. 
There are several different types of rule nodes.  
 
 
Expert Systems /Shells . The E.S  shell  simplifies the process of creating a knowledge base. It is 
the shell  that actually process es the information entered by a user relates it to the concepts 
contained in the knowledge base and provides an assessment or solution for a particular problem.  
 
 
Knowledge Acquisition  
Knowledge acquisition  is the process used to define the rules and ontologies req uired for 
a knowledge -based system . The phrase was first used in conjunction with  expert systems  to 
describe the initial tasks associated with developing an expert system, namely finding and 
interviewing  domain  experts and capturing their knowledge via  rules, objects , and  frame -
based  ontologies . 
127 
 Expert systems were one of the first successful applications of  artificial intelligence  technology 
to real world business problems . Researchers at  Stanford  and other AI laboratories worked with 
doctors and other hig hly skilled experts to develop systems that could automate complex tasks 
such as  medical diagnosis . Until this point computers had mostly been used to automate highly 
data intensive tasks but not for complex reasoning. Technologies such as  inference 
engine s allowed developers for the first time to tackle more complex problems.  
As expert systems scaled up from demonstration prototypes to industrial strength applications it 
was soon realized that the acquisition of domain expert knowledge was one of if not th e most 
critical task in the  knowledge engineering  process. This knowledge acquisition process became 
an intense area of research on its own. One of the earlier works  on the topic used Batesonian 
theories of learning to guide the process.  
One approach to kn owledge acquisition investigated was to use  natural language parsing  and 
generation to facilitate knowledge acquisition. Natural language parsing could be performed on 
manuals and other expert documents and an initial first pass at the rules and objects co uld be 
developed automatically. Text generation was also extremely useful in generating explanations 
for system behavior. This greatly facilitated the development and maintenance of expert systems.   
A more recent approach to knowledge acquisition is a re -use based approach. Knowledge can be 
developed in  ontologies  that conform to standards such as the  Web Ontology Language 
(OWL) . In this way knowledge can be standardized and shared across a broad community of 
knowledge workers. One example domain where this  approach has been successful 
is bioinformatics . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
128 
 Refferences  
 
1. Elaine Rich, Kevin Knight, & Shivashankar B Nair, Artificial Intelligence,  McGraw Hill, 3rd ed.,2009  
References:  
1) Introduction to Artificial Intelligence & Expert S ystems, Dan W Patterson,  PHI.,2010  
2) S Kaushik, Artificial Intelligence, Cengage Learning, 1st ed.2011  
 
 


MACHINE LEARNING   
[R17A0534 ] 
LECTURE NOTES  
 
B.TECH IV YEAR – I SEM(R17 ) 
(2020 -21) 
 
 
 
 
 
 
DEPARTMENT OF 
COMPUTER SCIENCE AND ENGINEERING  
MALLA REDDY COLLEGE OF ENGINEERING & 
TECHNOLOGY  
(Autonomous Institution – UGC, Govt. of India)  
Recognized under 2(f) and  12 (B) of UGC ACT 1956  
(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified)  
Maisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India  
 

IV Year B. T ech. CSE –II Sem                        L   T/P/D   C  
  4   1/ - / -   3  
(R17A0534) Machine Learning  
Objectives:  
 Acquire theoretical Knowledge on setting hypothesis for pattern recognition . 
 Apply suitable machine learning techniques for data handling and  to gain knowledge  from it . 
 Evaluate the performance of algorithms and to provide solution for various real  world 
applications . 
 
UNIT I:  
Introduction to Machine Learning  
Introduction  ,Components of Learning , Learning Models , Geometric Models, Probabilis tic 
Models, Logic Models, G roupin g and Grading, D esign ing a Learning System, Types of 
Learning, Supervised, Unsupervised,  Reinforcement , Perspectives and  Issues, Version Spaces, 
PAC Learning, VC Dimension.   
 
UNIT II:  
Supervised and Unsupervised Learning  
Decision Trees: ID3, Classification and Regression Trees, Regression: Linear Regression, Multiple Linear 
Regression, Logistic Regression, Neural Networks: Introduction, Perception, Multilayer Perception, 
Support Vector  Machines: Linear and Non -Linear, Kernel Functions, K  Nearest Neighbors .  
Introduction to clustering,  K-means clustering, K -Mode  Clustering . 
 
UNIT III:  
Ensemble and Probabilistic Learning  
Model Combination Schemes, Voting, Error -Correcting Output Codes, B agging: Random  Forest Trees, 
Boosting: Adaboost, Stacking . 
Gaussian mixture models  - The Expectation -Maximization (EM) Algorithm , Information Criteria , Nearest 
neighbour methods  - Nearest Neighbour Smoothing , Efficient Distance Computations: the KD -Tree, 
Distance Measures . 
 
 
UNIT IV:  
Reinforcement Learning  and Evaluating Hypotheses  
Introduction, Learning Task, Q Learning, Non deterministic Rewards and actions, temporal -difference 
learning, Relationship to Dynamic Programming, A ctive reinf orcement learning, Generalizat ion in 
reinfor cement learning.  
Motivation, Basics of Sampling Theory: Error Estimation and Estimating Binomial Proportions, The 
Binomial Distribution, Estimators, Bias, and Variance   
 
 
UNIT V:  
Genetic Algorithms:  Motivation, Genetic Algorithms : Representing Hypotheses , Genetic Operator, 
Fitness Function and Selection,  An Illustrative  Example, Hypothesis Space Search, Genetic 
Programming, Models of Evolution and Learning : Lamarkian Evolution, Baldwin Effect , Parallelizing 
Genetic Algorithms . 
 
TEXT BOOKS:  
 
1. Ethem  Alpaydin, ”Introduction to Machine Learning”, MIT Press, Prentice Hall of India, 3rd 
Edition2014.  
2. Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar ” Foundations of Machine Learning”, MIT 
Press,2012.  
3. Tom Mitchell, “Machine Learning”, McGraw Hill, 3rdEdition, 1997.  
4. MACHINE LEARNING - An Algorithmic Perspective, Second Edition , Stephen Marsland , 2015.  
 
REFERENCE BOOKS:  
1. CharuC.Aggarwal,“DataClassificationAlgorithmsandApplications”,CRCPress,2014.  
2. Charu C. Aggarwal, “DATA CLUSTERING Algorithms and Applications”, CRC Press,  
       2014.  
3. Kevin P. Murphy ”Machine Learning: A Probabilistic Perspective”, The MIT Press, 2012  
4. Jiawei Han and  Micheline Kambers and JianPei, “Data Mining Concepts  
      andTechniques”,3rd edition, Morgan Kaufman Publications, 2012.  
 
 
OUTCOMES:  
1. Recognize the characteristics of Machine Learning techniques that enable to solve real world 
problems  
2. Recognize the char acteristics of machine learning strategies  
3. Apply various supervised learning methods to appropriate problems  
4. Identify and integrate more than one techniques to enhance the performance of learning  
5. Create probabilistic and unsupervised learning mode ls for handling unknown pattern  
6. Analyze the co -occurrence of data to find interesting frequent patterns  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
INDEX  
UNIT NO  TOPIC  PAGE NO  
I Introduction  1 
Learning Models  3 
Designing a Learning System  7 
Types of Learning  12 
Perspectives and Issues  13 
Version Spaces  14 
PAC Learning  19 
VC Dimension  21 
II Decision Trees  23 
Classification and Regression Trees  27 
Neural Networks  37 
Support Vector Machines  45 
Introduction to clustering  49 
K-means clustering  52 
III Model Combination Schemes  55 
Voting, Error -Correcting Output Codes  57 
Bagging , Random Fo rest Trees  61 
Boosting , Adaboost  65 
Gaussian mixture models  68 
EM Algorithms  69 
Efficient Distance Computations  73 
IV Reinforcement Learning  78 
Learning Task  79 
Q Learning  82 
Evaluating Hypotheses  86 
Basics of Sampling Theory  88 
V Genetic Algorithms  92 
An Illustrative Example  96 
Hypothesis Space Search  98 
Genetic Programming  101 
Models of Evolution and Learning  104 
Parallelizing Genetic Algorithms . 105 
 
1 
 UNIT I  
Introduction to Machine Learning  
1. Introduction  
 
1.1 What Is Machine Learning?  
Machine learning is programming computers to optimize a performance criterion using example 
data or past experience. We have a model defined up to some parameters, and learning is the 
execution of a computer program to optimize the parameters of the model using the training data or 
past experience. The model may be predictive to make predictions in the future, or descriptive to gain 
knowledge from data, or both.  
Arthur Samuel, an early American leader in the field of computer gaming and artificial intellige nce, 
coined the term “Machine Learning” in 1959 while at IBM. He defined machine learning as “the field of 
study that gives computers the ability to learn without being explicitly programmed.” However, there is 
no universally accepted definition for machin e learning. Different authors define the term differently.  
 
Definition of learning  
Definition  
A computer program is said to learn from experience E with respect to some class of tasks T and 
performance measure P, if its performance at tasks T, as measured by P, improves with experience E.  
 
Examples  
i) Handwriting recognition learning problem  
• Task T: Recognising and classifying handwritten words within images  
• Performance P: Percent of words correctly classified  
• Training experience E: A dataset of handw ritten words with given classifications  
ii) A robot driving learning problem  
• Task T: Driving on highways using vision sensors  
• Performance measure P: Average distance traveled before an error  
• training  experience: A sequence of images and steering commands recorded while  
  observing a human driver  
iii) A chess learning problem  
• Task T: Playing chess  
• Performance measure P: Percent of games won against opponents  
• Training experience E: Playing practi ce games against itself  
Definition  
A computer program which learns from experience is called a machine learning program or 
simply a learning program. Such a program is sometimes also referred to as a learner.  
 
1.2 Components of Learning  
 Basic components o f learning process  
The learning process, whether by a human or a machine, can be divided into four components, 
namely, data storage, abstraction, generalization and evaluation. Figure 1.1 illustrates the 
variouscomponents and the steps involved in the lear ning process.  
 
2 
  
1. Data storage  
Facilities for storing and retrieving huge amounts of data are an important component of the 
learning process. Humans and computers alike utilize data storage as a foundation for advanced 
reasoning.  
• In a human being, the data is stored in the brain and data is retrieved using electrochemical    signals.  
• Computers use hard disk drives, flash memory, random access memory and similar devices    to store 
data and use cables and other technology to retrieve data.  
 
2. Abstract ion 
The second component of the learning process is known as abstraction.  
Abstraction is the process of extracting knowledge about stored data. This involves creating general 
concepts about the data as a whole. The creation of knowledge involves application of known models 
and creation of new models.  
The process of fitting a model to a dataset is known as training. When the model has been trained, the 
data is transformed into an abstract form that summarizes the original information.  
 
3. Generaliz ation  
The third component of the learning process is known as generalisation.  
The term generalization describes the process of turning the knowledge about stored data into a form 
that can be utilized for future action. These actions are to be carried out o n tasks that are similar, but 
not identical, to those what have been seen before. In generalization, the goal is to discover those 
properties of the data that will be most relevant to future tasks.  
 
4. Evaluation  
Evaluation is the last component of the lea rning process.  
It is the process of giving feedback to the user to measure the utility of the learned knowledge. This 
feedback is then utilised to effect improvements in the whole learning process  
 
Applications of machine learning  
Application of machine le arning methods to large databases is called data mining. In data 
mining, a large volume of data is processed to construct a simple model with valuable use, for example, 
having  
high predictive accuracy.  
 
The following is a list of some of the typical applic ations of machine learning.  
1. In retail business, machine learning is used to study consumer behaviour.  
2. In finance, banks analyze their past data to build models to use in credit applications, fraud 
detection, and the stock market.  
3. In manufacturing, learning  models are used for optimization, control, and troubleshooting.  

3 
 4. In medicine, learning programs are used for medical diagnosis.  
5. In telecommunications, call patterns are analyzed for network optimization and maximizing the 
quality of service.  
6. In science, la rge amounts of data in physics, astronomy, and biology can only be analyzed fast 
enough by computers. The World Wide Web is huge; it is constantly growing and searching for 
relevant information cannot be done manually.  
7. In artificial intelligence, it is use d to teach a system to learn and adapt to changes so that the 
system designer need not foresee and provide solutions for all possible situations.  
8. It is used to find solutions to many problems in vision, speech recognition, and robotics.  
9. Machine learning methods are applied in the design of computer -controlled vehicles to steer 
correctly when driving on a variety of roads.  
10. Machine learning methods have been used to develop programmes for playing games such as 
chess, backgammon and Go.  
 
1.3 Learning Models  
Machine learning is concerned with using the right features to build the right models that 
achieve the right tasks.  The basic idea of Learning models has divided into three categories.  
For a given problem, the collection of all possible outcomes represent s the  sample space or instance 
space . 
 
 Using a Logical expression. ( Logical models ) 
 Using the Geometry of the instance space. ( Geometric models)  
 Using Probability to classify the instance space. ( Probabilistic models ) 
 Grouping and Grading  
 
1.3.1  Logical  models  
Logical models  use a logical expression to divide the instance space into segments and hence 
construct grouping models. A  logical expression  is an expression that returns a Boolean value, i.e., a 
True or False outcome. Once the data is grouped usin g a logical expression, the data is divided into 
homogeneous groupings for the problem we are trying to solve.   For example, for a classiﬁcation 
problem, all the instances in the group belong to one class.  
 
There are mainly two kinds of logical models:  Tree models  and Rule models . 
 
Rule models consist of a collection of implications or IF -THEN rules. For tree -based models, the ‘if -part’ 
deﬁnes a segment and the ‘then -part’ deﬁnes the behaviour of the model for this segment. Rule models 
follow the same reaso ning.  
 
Logical models and Concept learning  
To understand logical models further, we need to understand the idea of  Concept Learning . 
Concept Learning involves learning logical expressions or concepts from examples. The idea of Concept 
Learning fits in well  with the idea of Machine learning, i.e., inferring a general function from specific 
training examples. Concept learning forms the basis of both tree -based and rule -based models.   More 
formally, Concept Learning involves acquiring the definition of a gener al category from a given set of 
positive and negative training examples of the category. A Formal Definition for Concept Learning is 
“The inferring of a Boolean -valued function from training examples of its input and output.”  In 
concept learning, we only l earn a description for the positive class and label everything that doesn’t 
satisfy that description as negative.  
 
4 
  
 
 
 
 
 
 
The following  example  explains this idea in more detail.  
 
 
 
A Concept Learning  Task called “Enjoy Sport” as shown above is defined by a set of data from 
some example days. Each data is described by six attributes. The task is to learn to predict the value of 
Enjoy Sport for an arbitrary day based on the values of its attribute values. The problem can be 
represented by a  series of hypotheses . Each hypothesis is described by a conjunction of constraints on 
the attribu tes. The training data represents a set of positive and negative examples of the target 
function. In the example above, each hypothesis is a vector of six constraints, specifying the values of 
the six attributes –  Sky, AirTemp, Humidity, Wind, Water, and Forecast. The training phase involves 
learning the set of days (as a conjunction of attributes) for which Enjoy Sport = yes.  
 
Thus, the problem can be formulated as:  
 
 Given instances X   which represent a set of all possible days, each described by the attr ibutes:  
o Sky – (values: Sunny, Cloudy, Rainy),  
o AirTemp – (values: Warm, Cold),  
o Humidity – (values: Normal, High),  
o Wind – (values: Strong, Weak),  
o Water – (values: Warm, Cold),  
o Forecast – (values: Same, Change).  
 
Try to identify a function that can predict th e target variable Enjoy Sport as yes/no, i.e., 1 or 0.  
 
1.3.2 Geometric models  
In the previous section, we have seen that with logical models, such as decision trees, a logical 
expression is used to partition the instance space. Two instances are similar when they end up in the 
same logical segment. In this section, we consider models that define similarity by considering the 
geometry of the instance space.   In Geometric models, features could be described as points in two 
dimensions ( x- and y-axis) or a three -dimensional space ( x, y, and z). Even when features are not 

5 
 intrinsically geometric, they could be modelled in a geometric manner (for example, temperature as a 
function of time can be modelled in two axes). In geometric models, there are two ways we  could 
impose similarity.  
 We could use geometric concepts like  lines or planes to segment (classify)  the instance space. 
These are called  Linear models . 
 Alternatively, we can use the geometric notion of distance to represent similarity. In this case, if 
two points are close together, they have similar values for features and thus can be classed as 
similar. We call such models as  Distance -based models . 
 
 
Linear models  
Linear models are relatively simple. In this case, the function is represented as a linear 
combination of its inputs. Thus, if  x1 and x2 are two scalars or vectors of the same dimension 
and a and b are arbitrary scalars, then  ax1 + bx2 represents a linear combination of  x1 and x2. In the 
simplest case where  f(x) represents a straight line, we ha ve an equation of the form  f (x) 
= mx + c where  c represents the intercept and  m represents the slope.  
 
 
Linear models are  parametric , which means that they have a ﬁxed  form with a small number of numeric 
parameters that need to be learned from data. For example, in  f (x) = mx + c, m and c are the 
parameters that we are trying to learn from the data. This technique is different from tree or rule 
models, where the structu re of the model (e.g., which features to use in the tree, and where) is  not 
ﬁxed in advance . 
 
Linear models are  stable , i.e., small variations in the t raining data have only a limited impact on the 
learned model. In contrast,  tree models tend to vary more with the training data , as the choice of a 
different split at the root of the tree typically means that the rest of the tree is different as well.   As a 
result of having relatively few parameters, Linear models have  low variance and high bias . This implies 
that Linear models are less likely to overfit  the training data  than some other models. However, they 
are more likely to underfit. For example, if we want to learn the boundaries between countries based 
on labelled data, then linear models are not likely to give a good approximation.  
 
Distance -based models  
Distance -based models  are the second class of Geometric models. Like Linear models, distance -
based  models are based on the geometry of data. As the name implies, distance -based models work on 
the concept of distance.   In the context of Machine learning, the concept of distance is not based on 
merely the physical distance between two points. Instead, we  could think of the distance between two 
points considering the  mode of transport  between two points. Travelling between two cities by plane 

6 
 covers less distance physically than by train because a plane is unrestricted. Similarly, in chess, the 
concept of distance depends on the piece used – for example, a Bishop can move diagonally.    Thus, 
depending on the entity and the mode of travel, the concept of distance can be experienced differently. 
The distance metrics commonly used are  Euclidean , Minkowski , Man hattan , and  Mahalanobis . 
 
 
Distance is applied through the concept of  neighbours and exemplars . Neighbours are points in 
proximity with respect to the distance measure expressed through exemplars. Exemplars are 
either  centroids  that ﬁnd  a centre of mass according to a chosen distance metric or  medoids  that ﬁnd 
the most centrally located data point. The most commonly used centroid is the arithmetic mean, which 
minimises squared Euclidean distance to all other points.  
 
Notes:  
 The centroid  represents the geometric centre of a plane figure, i.e., the arithmetic mean 
position of all the points in the figure from the centroid point. This definition extends to any 
object in  n-dimensional space: its centroid is the mean position of all the points . 
 Medoids  are similar in concept to means or centroids. Medoids are most commonly used on 
data when a mean or centroid cannot be defined. They are used in contexts where the centroid 
is not representative of the dataset, such as in image data.  
 
Examples of  distance -based models include the  nearest -neighbour  models, which use the training data 
as exemplars – for example, in classification. The  K-means clustering  algorithm also uses exemplars to 
create clusters of similar data points.  
 
1.3.3 Probabilistic mod els 
The third family of machine learning algorithms is the probabilistic models. We have seen 
before that the k -nearest neighbour algorithm uses the idea of distance (e.g., Euclidian distance) to 
classify entities, and logical models use a logical expressi on to partition the instance space. In this 
section, we see how the  probabilistic models use the idea of probability to classify new entities.  
 
Probabilistic models see features and target variables as random variables. The process of modelling 
represents and manipulates the level of uncertainty  with respect to these variables. There are two 
types of probabilistic models:  Predictive and Generative . Predictive probability models use the idea of 
a conditional probability  distribution  P (Y |X) from which  Y can be predicted from  X.  Generative models 
estimate the  joint distribution  P (Y, X).  Once we know the joint distribution for the generative models, 
we can derive any conditional or marginal distribution involving the same variables. Thus, the 
generative model is capable of creating new data points and their labels, knowing the joint probability 
distribution. The joint distribution looks for a relationship between two variables. Once this relationship 
is inferred, it is possible to infer new data points.  
Naïve Bayes  is an example of a probabilistic classifier.  
 
We can do this using the  Bayes rule  defined as  

7 
  
 
 
The Naïve Bayes algorithm is based on the idea of  Conditional Probability.   Conditional probability is 
based on finding the  probability that somethi ng will happen,  given that something else  has already 
happened. The task of the algorithm then is to look at the evidence and to determine the likelihood of a 
specific class and assign a label accordingly to each entity.  
 
Some broad categories of models:  
Geometric models  Probabilistic models  Logical models  
E.g. K -nearest neighbors, linear 
regression, support vector 
machine, logistic regression, …  Naïve Bayes, Gaussian process 
regression, conditional random 
field, …  Decision tree, random forest, …  
 
1.3.4 Grouping and Grading  
Grading vs grouping is an orthogonal categorization to geometric -probabilistic -logical -compositional.  
 Grouping models break the instance space up into groups or segments and in each segment 
apply a very simple method (such as majority  class).  
o E.g. decision tree, KNN.  
 Grading models form one global model over the instance space.  
o E.g. Linear classifiers – Neural networks  
1.4 Designing a Learning System  
For any learning system, we must be knowing the three elements — T (Task) , P (Performance 
Measure) , and  E (Training Experience) . At a high level, the process of learning system looks as below.  
 
The learning process starts with task T, performance measure P and training experience E and objective 
are to find an unknown target function. The target function is an exact knowledge to be learned from the 
training experience and its unknown. For example , in a case of credit approval, the learning system will 
have customer application records as experience and task would be to classify whether the given 
customer application is eligible for a loan. So in this case, the training examples can be represented as 

8 
 (x1,y1)(x2,y2)..(xn,yn) where X represents customer application details and y represents the status of 
credit approval.  
With these details, what is that exact knowledge to be learned from the training experience?  
So the target function to be learned in the credit approval learning system is a mapping function f:X →y. 
This function represents the exact knowledge defining the relationship between input variable X and 
output variable y.  
Design of a learning system  
Just now we looked into the learning proce ss and also understood the goal of the learning. When we 
want to design a learning system that follows the learning process, we need to consider a few design 
choices. The design choices will be to decide the following key components:  
1. Type of training exper ience  
2. Choosing the Target Function  
3. Choosing a representation for the Target Function  
4. Choosing an approximation algorithm for the Target Function  
5. The final Design  
 
We will look into the game - checkers learning problem and apply the above design choices. For a 
checkers learning problem, the three elements will be,  
 
1. Task T: To play checkers  
2. Performance measure P: Total percent of the game won in the tournament.  
3. Training experience E: A set of games played against itself  
 
1.4.1 Type of training experience  
During the design of the checker's learning system, the type of training experience available for a 
learning system will have a significant effect on the success or failure of the learning.  
 
1. Direct or Indirect training experience — In the case of direct training experience, an individual board 
states and correct move for each board state are given.  
In case of indirect training experience, the move sequences for a game and the final result (win, loss 
or draw) are given for a number of games. How to assign credit or blame to individual moves is the 
credit assignment problem.  
2. Teacher or Not — Supervised — The training experience will be labeled, which means, all the board 
states will be labeled with the correct move. So the learning takes place in t he presence of a 
supervisor or a teacher.  
Unsupervised — The training experience will be unlabeled, which means, all the board states will not 
have the moves. So the learner generates random games and plays against itself with no supervision 
or teacher inv olvement.  
9 
 Semi -supervised — Learner generates game states and asks the teacher for help in finding the 
correct move if the board state is confusing.  
3. Is the training experience good — Do the training examples represent the distribution of examples 
over whic h the final system performance will be measured? Performance is best when training 
examples and test examples are from the same/a similar distribution.  
 
The checker player learns by playing against oneself. Its experience is indirect. It may not encounter 
moves that are common in human expert play. Once the proper training experience is available, the next 
design step will be choosing the Target Function.  
 
1.4.2 Choosing the Target Function  
When you are playing the checkers game, at any moment of time, you make a decision on 
choosing the best move from different possibilities. You think and apply the learning that you have 
gained from the experience. Here the learning is, for a specific board, you move a checker such that your 
board state tends towards the w inning situation. Now the same learning has to be defined in terms of 
the target function.  
 
Here there are 2 considerations — direct and indirect experience.  
 
 During the direct experience , the checkers learning system, it needs only to learn how to choose 
the best move among some large search space. We need to find a target function that will help 
us choose the best move among alternatives. Let us call this function ChooseMove and use the 
notation  ChooseMove : B →M  to indicate that this function accepts as input any board from the 
set of legal board states B and produces as output some move from the set of legal moves M.  
 When there is an indirect experience , it becomes difficult to learn su ch function. How about 
assigning a real score to the board state.  
 
So the function be  V : B →R  indicating that this accepts as input any board from the set of legal board 
states B and produces an output a real score. This function assigns the higher score s to better board 
states.  
 
 
If the system can successfully learn such a target function V, then it can easily use it to select the best 
move from any board position.  

10 
 Let us therefore define the target value V(b) for an arbitrary board state b in B, as follows:  
1. if b is a final board state that is won, then V(b) = 100  
2. if b is a final board state that is lost, then V(b) = -100 
3. if b is a final board state that is drawn, then V(b) = 0  
4. if b is a not a final state in the game, then V (b) = V (b’), where b’ is the best final board state that can 
be achieved starting from b and playing optimally until the end of the game.  
 
The (4) is a recursive definition and to determine the value of V(b) for a particular board state, it 
performs the search ahead fo r the optimal line of play, all the way to the end of the game. So this 
definition is not efficiently computable by our checkers playing program, we say that it is a 
nonoperational definition.  
 
The goal of learning, in this case, is to discover an operational description of V ; that is, a description 
that can be used by the checkers -playing program to evaluate states and select moves within realistic 
time bounds.  
It may be very difficult in general to learn such an operational form of V perfectly. W e expect learning 
algorithms to acquire only some approximation to the target function ^V.  
 
1.4.3 Choosing a representation for the Target Function  
Now that we have specified the ideal target function V, we must choose a representation that 
the learning pr ogram will use to describe the function ^V that it will learn. As with earlier design 
choices, we again have many options. We could, for example, allow the program to represent using a 
large table with a distinct entry specifying the value for each distinc t board state. Or we could allow it to 
represent using a collection of rules that match against features of the board state, or a quadratic 
polynomial function of predefined board features, or an artificial  
neural network. In general, this choice of repres entation involves a crucial tradeoff. On one hand, we 
wish to pick a very expressive representation to allow representing as close an approximation as 
possible to the ideal target function V.  
 
On the other hand, the more expressive the representation, the  more training data the program 
will require in order to choose among the alternative hypotheses it can represent. To keep the 
discussion brief, let us choose a simple representation:  
for any given board state, the function ^V will be calculated as a linear combination of the following 
board features:  
 x1(b) — number of black pieces on board b  
 x2(b) — number of red pieces on b  
 x3(b) — number of black kings on b  
 x4(b) — number of red kings on b  
 x5(b) — number of red pieces threat ened by black (i.e., which can be taken on black’s next turn)  
 x6(b) — number of black pieces threatened by red  
 
^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · x5(b) + w6 · x6(b)  
 
Where w0 through w6 are numerical coefficients or weights to be obtained by a learning algorithm.  
Weights w1 to w6 will determine the relative importance of different board features.  
 
11 
 Specification of the Machine Learning Problem at this time — Till now we worked on choosing the type 
of training experience, choo sing the target function and its representation. The checkers learning task 
can be summarized as below.  
 Task T : Play Checkers  
 Performance Measure : % of games won in world tournament  
 Training Experience E : opportunity to play against itself  
 Target Functi on : V : Board → R  
 Target Function Representation : ^V = w0 + w1 · x1(b) + w2 · x2(b) + w3 · x3(b) + w4 · x4(b) +w5 · 
x5(b) + w6 · x6(b)  
The first three items above correspond to the specification of the learning task,whereas  the final two 
items constitute design choices for the implementation of the learning program.  
 
1.4.4 Choosing an approximation algorithm for the Target Function  
Generating training data — 
To train our learning program, we need a set of training data, each  describing a specific board state b and 
the training value V_train (b) for b. Each training example is an ordered pair <b,V_train(b)>  
For example, a training example may be <(x1 = 3, x2 = 0, x3 = 1, x4 = 0, x5 = 0, x6 = 0), +100">. This is an 
example wher e black has won the game since x2 = 0 or red has no remaining pieces. However, such clean 
values of V_train (b) can be obtained only for board value b that are clear win, loss or draw.  
In above case, assigning a training value V_train (b) for the specific boards b that are clean win, loss or 
draw is direct as they are direct training experience. But in the case of indirect training experience, 
assigning a training value V_train(b) for the intermediate boards is difficult. In such case, the training 
values are updated using temporal difference learning.  Temporal difference (TD) learning is a concept 
central to reinforcement learning, in which learning happens through the iterative correction of your 
estimated returns towards a more accura te target return.  
Let Successor(b) denotes the next board state following b for which it is again the program’s turn to 
move. ^V is the learner’s current approximation to V. Using these information, assign the training value 
of V_train(b) for any intermedi ate board state b as below :  
V_train(b) ← ^V(Successor(b))  
 
Adjusting the weights  
Now its  time to define the learning algorithm for choosing the weights and best fit the set of 
training examples. One common approach is to define the best hypothesis as that which minimizes the 
squared error E between the training values and the values predicted  by the hypothesis ^V.  
 
 
The learning algorithm should incrementally refine weights as more training examples become available 
and it needs to be robust to errors in training data Least Mean Square (LMS) training rule is the one 
training algorithm that wi ll adjust weights a small amount in the direction that reduces the error.  
 
The LMS algorithm is defined as follows:  
 

12 
  
 
1.4.5 Final Design for Checkers Learning system  
The final design of our checkers learning system can be naturally described by four distinct 
program modules that represent the central components in many learning systems.  
1. The performance System — Takes a new board as input and outputs a trace of the game it played 
against itself.  
2. The Critic — Takes the trace of a game as an input and ou tputs a set of training examples of the 
target function.  
3. The Generalizer — Takes training examples as input and outputs a hypothesis that estimates the 
target function. Good generalization to new cases is crucial.  
4. The Experiment Generator — Takes the curre nt hypothesis (currently learned function) as input and 
outputs a new problem (an initial board state) for the performance system to explore.  
 
 
Final design of the checkers learning program.  
 
1.5 Types of Learning  
In general, machine learning algorithms can be classified into three types.  
 Supervised learning  
 Unsupervised learning  
 Reinforcement learning  
 
1.5.1 Supervised learning  
A training set of examples with the correct responses (targets) is provided and, based on this 
training set, the algorithm generalises to respond correctly to all possible inputs. This is also called 
learning from exemplars. Supervised learning is the machine learning task of learning a function that 
maps an input to an output based on example input -output pairs.  
 
In supervise d learning, each example in the training set is a pair consisting of an input object 
(typically a vector) and an output value. A supervised learning algorithm analyzes the training data and 
produces a function, which can be used for mapping new examples. I n the optimal case, the function 
will correctly determine the class labels for unseen instances. Both classification and regression 

13 
 problems are supervised learning problems. A wide range of supervised learning algorithms are 
available, each with its stren gths and weaknesses. There is no single learning algorithm that works best 
on all supervised learning problems.  
 
 
Figure 1.4: Supervised learning  
 
 
 
Remarks  
A “supervised learning” is so called because the process of an algorithm  learning from the 
training dataset can be thought of as a teacher supervising the learning process. We know the correct 
answers (that is, the correct outputs), the algorithm iteratively makes predictions on the training data 
and is corrected by the teache r. Learning stops when the algorithm achieves an acceptable level of 
performance.  
 
Example  
Consider the following data regarding patients entering a clinic. The data consists of the gender 
and age of the patients and each patient is labeled as “healthy” or  “sick”.  
 
 
 
1.5.2 Unsupervised learning  
Correct responses are not provided, but instead the algorithm tries to identify similarities 
between the inputs so that inputs that have something in common are categorised  together. The 
statistical approach to unsupervised learning is  
known as density estimation.  
 
Unsupervised learning is a type of machine learning algorithm used to draw inferences from 
datasets consisting of input data without labeled responses. In unsuper vised learning algorithms, a 
classification or categorization is not included in the observations. There are no output values and so 
there is no estimation of functions. Since the examples given to the learner are unlabeled, the accuracy 
of the structure t hat is output by the algorithm cannot be evaluated. The most common unsupervised 
learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns 

14 
 or grouping in data.  
 
Example  
Consider the following data regarding pa tients entering a clinic. The data consists of the gender 
and age of the patients.  
 
Based on this data, can we infer anything regarding the patients entering the clinic?  
 
1.5.3 Reinforcement learning  
This is somewhere between supervised and unsupervised l earning. The algorithm gets told 
when the answer is wrong, but does not get told how to correct it. It has to explore and try out different 
possibilities until it works out how to get the answer right. Reinforcement learning is sometime called 
learning wit h a critic because of this monitor that scores the answer, but does not suggest 
improvements.  
 
Reinforcement learning is the problem of getting an agent to act in the world so as to maximize 
its rewards. A learner (the program) is not told what actions to take as in most forms of machine 
learning, but instead must discover which actions yield the most reward by trying them. In the most 
interesting and challenging cases, actions may affect not only the immediate reward but also the next 
situations and, throu gh that, all subsequent rewards.  
 
Example  
Consider teaching a dog a new trick: we cannot tell it what to do, but we can reward/punish it if 
it does the right/wrong thing. It has to find out what it did that made it get the reward/punishment. We 
can use a s imilar method to train computers to do many tasks, such as playing backgammon or chess, 
scheduling jobs, and controlling robot limbs. Reinforcement learning is different from supervised 
learning. Supervised learning is learning from examples provided by a knowledgeable expert.  
 
1.6 PERSPECTIVES AND ISSUES IN MACHINE LEARNING  
 
Perspectives in Machine Learning  
One useful perspective on machine learning is that it involves searching a very large space of 
possible hypotheses to determine one that best fits the observed data and any prior knowledge held by 
the learner.  
For example, consider the space of hypotheses that could in principle be output by the above checkers 
learner. This hypothesis space consists of all evaluation functions that can be represented by some 
choice of values for the weights wo through w6. The learner's task is thus to search through this vast 
space to locate the hypothesis that is most consistent with the available training examples. The LMS 
algorithm for fitting weights achieves this goa l by iteratively tuning the weights, adding a correction to 
each weight each time the hypothesized evaluation function predicts a value that differs from the 
training value. This algorithm works well when the hypothesis representation considered by the lea rner 
defines a continuously parameterized space of potential hypotheses.  
 

15 
 Many of the chapters in this book present algorithms that search a hypothesis space defined by 
some underlying representation (e.g., linear functions, logical descriptions, decision trees, artificial 
neural networks). These different hypothesis representat ions are appropriate for learning different 
kinds of target functions. For each of these hypothesis representations, the corresponding learning 
algorithm takes advantage of a different underlying structure to organize the search through the 
hypothesis spac e.  
 
Throughout this book we will return to this perspective of learning as a search problem in order 
to characterize learning methods by their search strategies and by the underlying structure of the 
search spaces they explore. We will also find this view point useful in formally analyzing the relationship 
between the size of the hypothesis space to be searched, the number of training examples available, 
and the confidence we can have that a hypothesis consistent with the training data will correctly 
genera lize to unseen examples.  
 
Issues in Machine Learning  
Our checkers example raises a number of generic questions about machine learning. The field of 
machine learning, and much of this book, is concerned with answering questions such as the following:  
 
 What algorithms exist for learning general target functions from specific training examples? In 
what settings will particular algorithms converge to the desired function, given sufficient 
training data? Which algorithms perform best for which types of problems and representations?  
 How much training data is sufficient? What general bounds can be found to relate the 
confidence in learned hypotheses to the amount of training experience and the character of the 
learner's hypothesis space?  
 When and how can prior know ledge held by the learner guide the process of generalizing from 
examples? Can prior knowledge be helpful even when it is only approximately correct?  
 What is the best strategy for choosing a useful next training experience, and how does the 
choice of this strategy alter the complexity of the learning problem?  
 What is the best way to reduce the learning task to one or more function approximation 
problems? Put another way, what specific functions should the system attempt to learn? Can 
this process itself be  automated?  
 How can the learner automatically alter its representation to improve its ability to represent 
and learn the target function?  
 
1.7 Version Spaces  
Definition (Version space). A concept is complete if it covers all positive examples.  
 
A concept is consistent if it covers none of the negative examples. The version space is the set of all 
complete and consistent concepts. This set is convex and is fully defined by its least and most general 
elements.  
 
The key idea in the CANDIDATE -ELIMINA TION algorithm is to output a description of the set of all 
hypotheses consistent with the training examples  
 
1.7.1 Representation  
The Candidate – Elimination  algorithm finds all describable hypotheses that are consistent with the 
16 
 observed training exampl es. In order to define this algorithm precisely, we begin with a few basic 
definitions. First, let us say that a hypothesis is consistent with the training examples if it correctly 
classifies these examples.  
 
Definition: A hypothesis h is consistent with a set of training examples D if and only if h(x) = c(x) for 
each example (x, c(x)) in D. 
 
 
 
Note difference between definitions of consistent and satisfies  
 An example x is said to satisfy hypothesis h when h(x) = 1, regardless of whether x is a positive 
or negative example of the target concept.  
 An example x is said to consistent with hypothesis h iff h(x) = c(x)  
 
Definition: version space - The version space, denoted V SH, D with respect to hypothesis space H and 
training examples D, is th e subset of hypotheses from H consistent with the training examples in D  
 
 
 
1.7.2 The LIST -THEN -ELIMINATION algorithm  
The LIST -THEN -ELIMINATE algorithm first initializes the version space to contain all hypotheses in H 
and then eliminates any hypothesis found inconsistent with any training example.  
 
1. VersionSpace c a list containing every hypothesis in H  
2. For each training example, (x, c(x)) remove from VersionSpace any hypothesis h for which h(x) ≠ c(x)  
3. Output the list of hypotheses in VersionSpace  
 
 List-Then -Eliminate works in principle, so long as version space is finite.  
 However, since it requires exhaustive enumeration of all hypotheses in practice it is not feasible.  
 
A More Compact Representation for Version Spaces  
The version space is represen ted by its most general and least general members. These members form general 
and specific boundary sets that delimit the version space within the partially ordered hypothesis space.  
Definition: The general boundary G, with respect to hypothesis space H and training data D, is the set of 
maximally general members of H consistent with D 
 
G {g  H | Consistent (g, D)(g'  H)[(g'  g)  Consistent (g', D )]} 
g 
 
Definition: The specific boundary S, with respect to hypothesis space H and training data D, is the set of 
minimally general (i.e., maximally specific) members of H consistent with D. 
 
S {s  H | Consistent (s, D)(s'  H)[(s  s')  Consistent (s', D)]} 
g 
 
Theorem: Version Space representation theorem  
Theorem: Let X be an arbitrary set of instances and Let H be a set of Boolean -valued hypotheses defined over X. 
Let c: X →{O, 1} be an arbitrary target concept defined over X, and let D be an arbitrary set of training examples 

17 
 {(x, c(x))). For all X, H, c, and D su ch that S and G are well defined,  
 
VS ={ h  H | (s  S ) (g  G ) ( g  h  s )} 
H,D g g 
 
To Prove:  
1. Every h satisfying the right hand side of the above expression is in  VS 
                                                                                                                                            H, D  
2. Every member  of VS satisfies the right -hand side of the  expression  
                                         H, D 
 
Sketch of proof:  
1. let g, h, s be arbitrary members of G, H, S respectively with g g h g s 
 By the definition of S, s must be satisfied by all positive examples in D. Because h g s, 
h must also be satisfied by all positive examples in D.  
 By the definition  of G, g cannot  be satisfied  by any negative  example  in D, and because g g h 
h cannot be satisfied by any negative example in D. Because h is satisfied by all positive 
examples in D and by no negative examples in D, h is consistent with D, and therefore h is a 
member of VSH,D. 
2. It can be proven by assuming some h in VSH,D,that does n ot satisfy the right -hand side of 
the expression, then showing that this leads to an  inconsistency  
1.7.3 CANDIDATE -ELIMINATION Learning Algorithm  
 
The CANDIDATE -ELIMINTION algorithm computes the version space containing all hypotheses 
from H that are consi stent with an observed sequence of training examples.  
 
Initialize G to the set of maximally general hypotheses in H Initialize S to the set of maximally specific 
hypotheses in H For each training example d, do  
• If d is a positive  example  
• Remove from G any hypothesis inconsistent with  d 
• For each hypothesis s in S that is not consistent with  d 
• Remove s from  S 
• Add to S all minimal generalizations h of s such  that 
• h is consistent with d, and some member of G is more general than  h 
• Remove  from  S any hypothesis  that is more  general  than  another  hypothesis  in S 
 
• If d is a negative  example  
• Remove from S any hypothesis inconsistent with  d 
• For each hypothesis g in G that is not consistent with  d 
• Remove g from  G 
18 
 • Add to G all minimal specializations h of g such  that 
• h is consistent with d, and some member of S is more specific than  h 
• Remove from G any hypothesis that is less general than another hypothesis in  G 
CANDIDATE - ELIMINTION algorithm using version spaces  
 
1.7.4 An Illustrative Example  
 
 
Example  Sky AirTemp  Humidity  Wind  Water  Forecast  EnjoySport  
1 Sunny  Warm  Normal  Strong  Warm  Same  Yes 
2 Sunny  Warm  High  Strong  Warm  Same  Yes 
3 Rainy  Cold  High  Strong  Warm  Change  No 
4 Sunny  Warm  High  Strong  Cool  Change  Yes 
 
CANDIDATE -ELIMINTION algorithm begins by initializing the version space to the set of all 
hypotheses in H;  
 
Initializing the G boundary set to contain the most general hypothesis in H  
G0  ?,  ?,  ?,  ?,  ?, ?  
 
Initializing the S boundary set to contain the most specific (least general) hypothesis  
S0  , , , , ,   
 
 When the first training example is presented , the CANDIDATE -ELIMINTION algorithm checks the 
S boundary and finds that it is overly specific and it fails to cover the positive example.  
 The boundary is therefore revised by moving it to the least more general hypothesis that covers 
this new example  
 No update of the G boundary is needed in response to this training example because Go 
correctly covers this  example  
 
 
 
 
 When the second training example is observed,  it has a similar effect of generalizing S  further to S 2, 
leaving G again unchanged i.e., G 2 = G1 = G0 
 

19 
  
 
 
 Consider  the third  training  example . This negative  example  reveals  that the G boundary of 
the version space is overly general, that is, the hypothesis in G incorrectly predicts that this 
new example is a positive  example.  
 The hypothesis in the G boundary must therefore be specialized until it correctly classifies 
this new negative  example . 
 
 
 
Given that there are six attributes that could be specified to specialize G 2, why are there only  three 
new hypotheses in G 3? 
For example, the hypothesis h = (?, ?, Normal, ?, ?, ?) is a minimal specialization of G2 that 
correctly  labels  the new  example  as a negative  example,  but it is not included  in G3. The 
reason this hypothesis is excluded is that it is inconsistent with the previously encountered 
positive examples  
 
Consider the fourth training  example . 
 

20 
  
 
 
 This positive example further generalizes the S boundary of the version space. It also 
results in removing one member of the G boundary, because this member fails to cover 
the new positive  example  
 
After processing these four examples, the boundary sets S 4 and G 4 delimit the version space of all 
hypotheses consistent with the set of incrementally ob served training examples.  
 
 
 
 
 
1.8 Probably approximately correct learning  
 
In computer science, computational learning theory (or just learning theory) is a subfield of 
artificial intelligence devoted to studying the design and analysis of machine learni ng algorithms. In 
computational learning theory, probably approximately correct learning (PAC learning) is a framework 
for mathematical analysis of machine learning algorithms. It was proposed in 1984 by Leslie Valiant.  
 
In this framework, the learner (tha t is, the algorithm) receives samples and must select a 
hypothesis from a certain class of hypotheses. The goal is that, with high probability (the “probably” 
part), the selected hypothesis will have low generalization error (the “approximately correct” pa rt). In 
this section we first give an informal definition of PAC -learnability. After introducing a few nore notions, 
we give a more formal, mathematically oriented, definition of PAC -learnability. At the end, we mention 
one of the applications of PAC -learn ability.  
 
PAC-learnability  
To define PAC -learnability we require some specific terminology and related notations.  

21 
  Let X be a set called the instance space which may be finite or infinite. For example, X may be 
the set of all points in a plane.  
 A concept class C for X is a family of functions c : X  {0; 1}. A member of C is called a concept. 
A concept can also be thought of as a subset of X. If C is a subset of X, it defines a unique 
function µ c : X  {0; 1} as follows:  
 
 
 
 A hypothesis h is als o a function h : X  {0; 1}. So, as in the case of concepts, a hypothesis can 
also be thought of as a subset of X. H will denote a set of hypotheses.  
 We assume that F is an arbitrary, but fixed, probability distribution over X.  
 Training examples are obtain ed by taking random samples from X. We assume that the samples 
are randomly generated from X according to the probability distribution F.  
 
Now, we give below an informal definition of PAC -learnability.  
 
Definition (informal)  
Let X be an instance space, C a  concept class for X, h a hypothesis in C and F an arbitrary, but fixed, 
probability distribution. The concept class C is said to be PAC -learnable if there is an algorithm A which, 
for samples drawn with any probability distribution F and any concept c Є C , will with high probability 
produce a hypothesis h Є C whose error is small.  
 
Examples  
 
To illustrate the definition of PAC -learnability, let us consider some concrete examples . 
 
 
Figure : An axis -aligned rectangle in the Euclidean plane  
 
Example  

22 
  Let the instance space be the set X of all points in the Euclidean plane. Each point is represented 
by its coordinates (x; y). So, the dimension or length of the instances is 2.  
 Let the concept class C be the set of all “axis -aligned rectangles” in the pl ane; that is, the set of 
all rectangles whose sides are parallel to the coordinate axes in the plane (see Figure).  
 Since an axis -aligned rectangle can be defined by a set of inequalities of the following form 
having four parameters  
 
a ≤ x ≤ b,    c ≤ y ≤ d 
 
the size of a concept is 4.  
 We take the set H of all hypotheses to be equal to the set C of concepts, H = C.  
 
Given a set of sample points labeled positive or negative, let L be the algorithm which outputs the 
hypothesis defined by the axis -aligned rect angle which gives the tightest fit to the positive examples 
(that is, that rectangle with the smallest area that includes all of the positive examples and none of the 
negative examples) (see Figure bleow).  
 
 
Figure :  Axis -aligned rectangle which gives the tightest fit to the positive examples  
 
It can be shown that, in the notations introduced above, the concept class C is PAC -learnable by the 
algorithm L using the hypothesis space H of all axis -aligned rectangles.  
 
1.9 Vapnik -Chervonenkis (VC) dimension  
The concepts of Vapnik -Chervonenkis dimension (VC dimension) and probably approximate 
correct (PAC) learning are two important concepts in the mathematical theory of learnability and hence 
are mathematically oriented. T he former is a measure of the capacity (complexity, expressive power, 
richness, or flexibility) of a space of functions that can be learned by a classification algorithm. It was 
originally defined by Vladimir Vapnik and Alexey Chervonenkis in 1971. The lat ter is a framework for the 
mathematical analysis of learning algorithms. The goal is to check whether the probability for a selected 
hypothesis to be approximately correct is very high. The notion of PAC  
learning was proposed by Leslie Valiant in 1984.  
 
V-C dimension  
Let H be the hypothesis space for some machine learning problem. The Vapnik -Chervonenkis dimension 
of H, also called the VC dimension of H, and denoted by V C(H), is a measure of the complexity (or, 
capacity, expressive power, richness, or flex ibility) of the space H. To define the VC dimension we 
require the notion of the shattering of a set of instances.  

23 
  
Shattering of a set  
Let D be a dataset containing N examples for a binary classification problem with class labels 0 and 1. 
Let H be a hypot hesis space for the problem. Each hypothesis h in H partitions D into two disjoint 
subsets as follows:  
 
 
Such a partition of S is called a “dichotomy” in D. It can be shown that there are 2N possible dichotomies 
in D. To each dichotomy of D there is a unique assignment of the labels “1” and “0” to the elements of 
D. Conversely, if S is any subset of D then, S defines a unique hypothesis h as follows:  
 
 
Thus to specify a hypothesis h, we need only specify the set {x Є D |  h(x) = 1}. Figure 3.1 shows al l 
possible dichotomies of D if D has three elements. In the figure, we have shown only one of the two sets 
in a dichotomy, namely the set {x Є D |  h(x) = 1}.The circles and ellipses represent such sets.  
 
 
 
 
Definition  
A set of examples D is said to be shattered by a hypothesis space H if and only if for every dichotomy of 
D there exists some hypothesis in H consistent with the dichotomy of D.  
 
The following example illustrates the concept of Vapnik -Chervonenkis dimension.  
 
Example  
 
In figure ,  we see that an axis -aligned rectangle can shatter four points in two dimensions. Then  VC(H), 
when H is the hypothesis class of axis -aligned rectangles in two dimensions, is four. In calculating the VC 
dimension, it is enough that we find four points that  can be shattered; it is not necessary that we be 
able to shatter any four points in two dimensions.  
 

24 
  
Fig: An axis -aligned rectangle can shattered four points. Only rectangle covering two points are shown.  
 
VC dimension may seem pessimistic. It tells us that using a rectangle as our hypothesis class, we can 
learn only datasets containing four points and not more.  
 
 
 
 
 
 
 
Unit II  
Supervised and Unsupervised Learning  
 
Topics: Decision Trees: ID3, Classification and Regression Trees, Regression: Linear Regres sion, 
Multiple Linear Regression, Logistic Regression, Neural Networks: Introduction, Perception, 
Multilayer Perception, Support Vector Machines: Linear and Non -Linear, Kernel Functions, K 
Nearest Neighbors. Introduction to clustering, K -means clustering, K-Mode Clustering.  
 
2.1. Decision Tree  
Introduction  Decision Trees are a type of Supervised Machine Learning (that is you explain what 
the input is and what the corresponding output is in the training data) where the data is continuously 
split according to a certain parameter. The tree can be explained by two entities, namely decision 
nodes and leaves . The leaves are the decisions or the final outcomes. And the decision nodes are where 
the data is split.  
 
An example of a decision tree can be explained using above binary tree. Let’s say you want to predict 
whether a person is fit given their information like age, eating habit, and physical activity, etc. The 

25 
 decision nodes here are questions like ‘What’s the age?’, ‘Does he exercise?’, and ‘Does he eat a lot of 
pizzas’? And the leaves, which are outcomes lik e either ‘fit’, or ‘unfit’. In this case this was a binary 
classification problem (a yes no type problem). There are two main types of Decision Trees:  
1. Classification trees  (Yes/No types)  
What we have seen above is an example of classification tree, where t he outcome was a variable like 
‘fit’ or ‘unfit’. Here the decision variable is  Categorical . 
 
2. Regression trees  (Continuous data types)  
Here the decision or the outcome variable is  Continuous , e.g. a number like 123.   Working  Now that we 
know what a Decision  Tree is, we’ll see how it works internally. There are many algorithms out there 
which construct Decision Trees, but one of the best is called as  ID3 Algorithm . ID3 Stands for  Iterative 
Dichotomiser 3 . Before discussing the ID3 algorithm, we’ll go through few definitions.  Entropy  Entropy, 
also called as Shannon Entropy is denoted by H(S) for a finite set S, is the measure of the amount of 
uncertainty or randomness in data.  
 
Intuitively, it tells us about the predictability of a certain event. Example, cons ider a coin toss whose 
probability of heads is 0.5 and probability of tails is 0.5. Here the entropy is the highest possible, since 
there’s no way of determining what the outcome might be. Alternatively, consider a coin which has 
heads on both the sides, t he entropy of such an event can be predicted perfectly since we know 
beforehand that it’ll always be heads. In other words, this event has  no randomness  hence it’s entropy 
is zero. In particular, lower values imply less uncertainty while higher values impl y high 
uncertainty.  Information Gain  Information gain is also called as Kullback -Leibler divergence denoted by 
IG(S,A) for a set S is the effective change in entropy after deciding on a particular attribute A. It 
measures the relative change in entropy wit h respect to the independent variables  
 
ASHSHASIG , , 
 
Alternatively,  
 
 
 
where IG(S, A) is the information gain by applying feature A. H(S) is the Entropy of the entire set, while 
the second term calculates the Entropy after applying the feature A, where P(x) is the probability of 
even t x. Let’s understand this with the help of an example Consider a piece of data collected over the 
course of 14 days where the features are Outlook, Temperature, Humidity, Wind and the outcome 
variable is whether Golf was played on the day. Now, our job is  to build a predictive model which takes 
in above 4 parameters and predicts whether Golf will be played on the day. We’ll build a decision tree 
to do that using  ID3 algorithm.  
 
Day Outlook  Temperature  Humidity  Wind  Play Golf  
D1 Sunny  Hot High  Weak  No 
D2 Sunny  Hot High  Strong  No 
D3 Overcast  Hot High  Weak  Yes 

n
ixHxP SHASIG
0,
26 
 D4 Rain  Mild  High  Weak  Yes 
D5 Rain  Cool  Normal  Weak  Yes 
D6 Rain  Cool  Normal  Strong  No 
D7 Overcast  Cool  Normal  Strong  Yes 
D8 Sunny  Mild  High  Weak  No 
D9 Sunny  Cool  Normal  Weak  Yes 
D10 Rain  Mild  Normal  Weak  Yes 
D11 Sunny  Mild  Normal  Strong  Yes 
D12 Overcast  Mild  High  Strong  Yes 
D13 Overcast  Hot Normal  Weak  Yes 
D14 Rain  Mild  High  Strong  No 
 
2.1.1 ID3  
 ID3 Algorithm will perform following tasks recursively  
 
1. Create root node for the tree  
2. If all examples are positive, return leaf node „positive‟  
3. Else if all examples are negative, return leaf node „negative‟  
4. Calculate the entropy of current state H(S)  
5. For each attribute, calculate the entropy with respect to the attribute „x‟ denoted by H(S, x)  
6. Select the attribute which has maximum value of IG(S, x)  
7. Remove the attribute that offers highest IG from the set of attributes  
8. Repeat until we run out of all attributes, or the decision tree has all leaf nodes.  
 
Now we‟ll go ahead and grow the decision tree.  The initial step is to calculate H(S), the Entropy of the current state. 
In the above example, we can see in total there are 5 No‟s and 9 Yes‟s.  
 
Yes No Total  
9 5 14 
 
 
where  „x‟ are the possible values for an attribute. Here,  attribute „Wind‟ takes two possible values in the sample 
data, hence x = {Weak, Strong} we‟ll have to calculate:  
 
 
 
Amongst all the 14 examples we have  8 places where the wind is weak and 6 where the wind is Strong . 
 
Wind = Weak  Wind = Strong  Total  
8 6 14 

27 
  
Now out of the 8 Weak examples, 6 of them were „Yes‟ for Play Golf and 2 of them were „No‟ for „Play Golf‟. So, 
we have,  
 
 
Similarly, out of 6 Strong examples, we have  3 examples where the outcome was „Yes‟ for Play Golf and 3 
where we had „No‟ for Play Golf . 
 
 
 
Remember, here half items belong to one class while other half belong to other. Hence we have perfect randomness. 
Now we have all the pieces required to calculate the Information Ga in, 
 
 
 
Which tells us the Information Gain by considering „Wind‟ as the feature and give us information gain of  0.048 . 
Now we must similarly calculate the Information Gain for all the features.  
 
 
We can clearly see that IG(S, Outlook) has the highest information gain of 0.246,  hence  we chose Outlook 
attribute  as the root node . At this point, the decision tree looks like.  
 
 
 
 
 
 
 
 
 
 
 

28 
 Here we observe that whenever the outlook is Overcast, Play Golf is always ‘Yes’, it’s no coincidence by 
any chance, the simple tree resulted because of  the highest information gain is given by the attribute 
Outlook . Now how do we proceed from this point? We can simply apply  recursion , you might want to 
look at the algorithm steps described earlier. Now that we’ve used Outlo ok, we’ve got three of them 
remaining Humidity, Temperature, and Wind. And, we had three possible values of Outlook: Sunny, 
Overcast, Rain. Where the Overcast node already ended up having leaf node ‘Yes’, so we’re left with 
two subtrees to compute: Sunny a nd Rain.  
 
Table where the value of Outlook is Sunny looks like:  
Temperature  Humidity  Wind  Play Golf  
Hot High  Weak  No 
Hot High  Strong  No 
Mild  High  Weak  No 
Cool  Normal  Weak  Yes 
Mild  Normal  Strong  Yes 
 
 
As we can see the  highest Information Gain is given by Humidity . Proceeding in the same way with  
 
will give us Wind as the one with highest information gain. The final Decision Tree looks something like 
this. The final Decision Tree looks something like this.  
 
 
 
2.1.2. Classification and Regression Trees  
2.1.2.1. Classification Trees  
A classification tree is an algorithm where the target variable is fixed or categorical. The 
algorithm is then used to identify the “class” within which a target variable would most likely fall.  
An example of a classific ation -type problem would be determining who will or will not subscribe to a 
digital platform; or who will or will not graduate from high school.  
These are examples of simple binary classifications where the categorical dependent variable can 
assume only on e of two, mutually exclusive values. In other cases, you might have to predict among a 
number of different variables. For instance, you may have to predict which type of smartphone a 
consumer may decide to purchase.  
In such cases, there are multiple values  for the categorical dependent variable. Here’s what a classic 
classification tree looks like  

29 
  
2.1.2.2. Regression Trees  
A regression tree refers to an algorithm where the target variable is and the algorithm is used to 
predict it’s value. As an example o f a regression type problem, you may want to predict the selling 
prices of a residential house, which is a continuous dependent variable.  
This will depend on both continuous factors like square footage as well as categorical factors like the 
style of home,  area in which the property is located and so on.  
 
 
 
When to use Classification and Regression Trees  
Classification trees are used when the dataset needs to be split into classes which belong to the 
response variable. In many cases, the classes Yes or No.  
In other words, they are just two and mutually exclusive. In some cases, there may be more than two 
classes in which case a variant of the classification tree algorithm is used.  
Regression trees, on the other hand, are used when the response variable is c ontinuous. For instance, if 
the response variable is something like the price of a property or the temperature of the day, a 
regression tree is used.  
In other words, regression trees are used for prediction -type problems while classification trees are 
used  for classification -type problems.  
 
How Classification and Regression Trees Work  
A classification tree splits the dataset based on the homogeneity of data. Say, for instance, 
there are two variables; income and age; which determine whether or not a consume r will buy a 
particular kind of phone.  
If the training data shows that 95% of people who are older than 30 bought the phone, the data gets 
split there and age becomes a top node in the tree. This split makes the data “95% pure”. Measures of 
impurity like e ntropy or Gini index are used to quantify the homogeneity of the data when it comes to 
classification trees.  

30 
 In a regression tree, a regression model is fit to the target variable using each of the independent 
variables. After this, the data is split at se veral points for each independent variable.  
At each such point, the error between the predicted values and actual values is squared to get “A Sum 
of Squared Errors” (SSE). The SSE is compared across the variables and the variable or point which has 
the low est SSE is chosen as the split point. This process is continued recursively.  
 
 
Advantages of Classification and Regression Trees  
The purpose of the analysis conducted by any classification or regression tree is to create a set of if -else 
conditions that allow for the accurate prediction or classification of a case.  
(i) The Results are Simplistic  
The interpretation of results summarized in classification or regression trees is usually fairly simple. The 
simplicity of results helps in the following ways.  
 It allows for the rapid classification of new observations. That’s because it is much simpler to 
evaluate just one or two logical conditions than to compute scores using complex nonlinear 
equations for each group.  
 It can often result in a simpler model which  explains why the observations are either classified 
or predicted in a certain way. For instance, business problems are much easier to explain with 
if-then statements than with complex nonlinear equations.  
(ii) Classification and Regression Trees are Nonpa rametric & Nonlinear  
The results from classification and regression trees can be summarized in simplistic if -then conditions. 
This negates the need for the following implicit assumptions.  
 The predictor variables and the dependent variable are linear.  
 The p redictor variables and the dependent variable follow some specific nonlinear link function.  
 The predictor variables and the dependent variable are monotonic.  
Since there is no need for such implicit assumptions, classification and regression tree methods a re well 
suited to data mining. This is because there is very little knowledge or assumptions that can be made 
beforehand about how the different variables are related.  
As a result, classification and regression trees can actually reveal relationships betwe en these variables 
that would not have been possible using other techniques.  
(iii) Classification and Regression Trees Implicitly Perform Feature Selection  
Feature selection or variable screening is an important part of analytics. When we use decision tree s, 
the top few nodes on which the tree is split are the most important variables within the set. As a result, 
feature selection gets performed automatically and we don’t need to do it again.  
Limitations of Classification and Regression Trees  

31 
 Classification  and regression tree tutorials, as well as classification and regression tree ppts, exist in 
abundance. This is a testament to the popularity of these decision trees and how frequently they are 
used. However, these decision trees are not without their disa dvantages.  
There are many classification and regression trees examples where the use of a decision tree has not 
led to the optimal result. Here are some of the limitations of classification and regression trees.  
(i) Overfitting  
Overfitting occurs when the tree takes into account a lot of noise that exists in the data and 
comes up with an inaccurate result.  
(ii) High variance  
In this case, a small variance in the data can lead to a very high variance in the prediction, 
thereby affecting the stability of the outcome.  
(iii) Low bias  
A decision tree that is very complex usually has a low bias. This makes it very difficult for the 
model to incorporate any new data.  
 
What is a CART in Machine Learning?  
A Classification and Regression Tree (CART) is a predictive algorithm used in  machine learning. It 
explains how a target variable’s values can be predicted based on other values.  
It is a decision tree where each fork is a split in a predictor variable and each node at the end has a 
prediction for the target variabl e. 
The CART algorithm is an important  decision tree algorithm  that lies at the foundation of machine 
learning. Moreover, it is also the basis for other powerful machine learning algorithms like bagged 
decision trees, random forest and boosted decision tree s. 
Summing up  
The Classification and regression tree (CART) methodology is one of the oldest and most fundamental 
algorithms. It is used to predict outcomes based on certain predictor variables.  
They are excellent for data mining tasks because they require  very little data pre -processing. Decision 
tree models are easy to understand and implement which gives them a strong advantage when 
compared to other analytical models.  
 
2.2. Regression  
Regression Analysis in Machine learning  
Regression analysis is a stat istical method to model the relationship between a dependent 
(target) and independent (predictor) variables with one or more independent variables. More 
specifically, Regression analysis helps us to understand how the value of the dependent variable is 
changing corresponding to an independent variable when other independent variables are held fixed. It 
predicts continuous/real values such as  temperature, age, salary, price,  etc. 
 
We can understand the concept of regression analysis using the below example:  
 
Example:  Suppose there is a marketing company A, who does various advertisement every year and get 
sales on that. The below list shows the advertisement made by the company in the last 5 years and the 
corresponding sales:  
32 
  
 
Now, the company wants to do t he advertisement of $200 in the year 2019  and wants to know the 
prediction about the sales for this year . So to solve such type of prediction problems in machine 
learning, we need regression analysis.  
Regression is a  supervised learning technique  which hel ps in finding the correlation between variables 
and enables us to predict the continuous output variable based on the one or more predictor variables. 
It is mainly used for  prediction, forecasting, time series modeling, and determining the causal -effect 
relationship between variables . 
In Regression, we plot a graph between the variables which best fits the given datapoints, using this 
plot, the machine learning model can make predictions about the data. In simple words,  "Regression 
shows a line or curve tha t passes through all the datapoints on target -predictor graph in such a way 
that the vertical distance between the datapoints and the regression line is minimum."  The distance 
between datapoints and line tells whether a model has captured a strong relation ship or not.  
 
Some examples of regression can be as:  
o Prediction of rain using temperature and other factors  
o Determining Market trends  
o Prediction of road accidents due to rash driving.  
 
Terminologies Related to the Regression Analysis:  
o Dependent Variable:  The main factor in Regression analysis which we want to predict or 
understand is called the dependent variable. It is also called  target variable . 
o Independent Variable:  The factors which affect the dependent variables or which are used to 
predict the valu es of the dependent variables are called independent variable, also called as 
a predictor . 
o Outliers:  Outlier is an observation which contains either very low value or very high value in 
comparison to other observed values. An outlier may hamper the result,  so it should be 
avoided.  
o Multicollinearity:  If the independent variables are highly correlated with each other than other 
variables, then such condition is called Multicollinearity. It should not be present in the dataset, 
because it creates problem while  ranking the most affecting variable.  
o Underfitting and Overfitting:  If our algorithm works well with the training dataset but not well 
with test dataset, then such problem is called  Overfitting . And if our algorithm does not 
perform well even with training  dataset, then such problem is called  underfitting . 
 

33 
 Why do we use Regression Analysis?  
As mentioned above, Regression analysis helps in the prediction of a continuous variable. There are 
various scenarios in the real world where we need some future predic tions such as weather condition, 
sales prediction, marketing trends, etc., for such case we need some technology which can make 
predictions more accurately. So for such case we need Regression analysis which is a statistical method 
and used in machine lear ning and data science. Below are some other reasons for using Regression 
analysis:  
o Regression estimates the relationship between the target and the independent variable.  
o It is used to find the trends in data.  
o It helps to predict real/continuous values.  
o By performing the regression, we can confidently determine the  most important factor, the 
least important factor, and how each factor is affecting the other factors . 
 
Types of Regression  
There are various types of regressions which are used in data science an d machine learning. Each type 
has its own importance on different scenarios, but at the core, all the regression methods analyze the 
effect of the independent variable on dependent variables. Here we are discussing some important 
types of regression which are given below:  
o Linear Regression  
o Logistic Regression  
o Polynomial Regression  
o Support Vector Regression  
o Decision Tree Regression  
o Random Forest Regression  
o Ridge Regression  
o Lasso Regression  
 
 
 
2.2.1. Linear Regression:  
o Linear regression is a statistical regression method which is used for predictive analysis.  
o It is one of the very simple and easy algorithms which works on regression and shows the relationship 
between the continuous variables.  
o It is used for solving the regression problem in machine learni ng. 
o Linear regression shows the linear relationship between the independent variable (X -axis) and the 
dependent variable (Y -axis), hence called linear regression.  

34 
 o If there is only one input variable (x), then such linear regression is called  simple linear regression . And if 
there is more than one input variable, then such linear regression is called  multiple linear regression . 
o The relationship between variables in the linear regression model can be explained using the below image. 
Here we are predicting the  salary of an employee on the basis of  the year of experience . 
 
Below is the mathematical equation for Linear regression:  
Y= aX+b   
 
Here, Y = dependent variables (target variables),  
X= Independent variables (predictor variables),  
a and b are the linear co efficients  
 
Some popular applications of linear regression are:  
o Analyzing trends and sales estimates  
o Salary forecasting  
o Real estate prediction  
o Arriving at ETAs in traffic.  
2.2.2. Logistic Regression:  
o Logistic regression is another supervised learning algor ithm which is used to solve the classification 
problems. In  classification problems , we have dependent variables in a binary or discrete format such as 0 
or 1. 
o Logistic regression algorithm works with the categorical variable such as 0 or 1, Yes or No, True or False, 
Spam or not spam, etc.  
o It is a predictive analysis algorithm which works on the concept of probability.  
o Logistic regression is a type of regression, but it is different from the linear regression algorithm in the 
term how they are used.  
o Logistic regression uses  sigmoid function  or logistic function which is a complex cost function. This 
sigmoid function is used to model the data in logistic regression. The function can be represented as:  

35 
  
o f(x)= Output between the 0 and 1 value.  
o x= input t o the function  
o e= base of natural logarithm.  
When we provide the input values (data) to the function, it gives the S -curve as follows:  
 
 
 
 
 
 
 
 
 
o It uses the concept of threshold levels, values above the threshold level are rounded up to 1, and values 
below  the threshold level are rounded up to 0.  
There are three types of logistic regression:  
o Binary(0/1, pass/fail)  
o Multi(cats, dogs, lions)  
o Ordinal(low, medium, high)  
Linear Regression in Machine Learning  
Linear regression is one of the easiest and most popula r Machine Learning algorithms. It is a statistical method that 
is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such 
as sales, salary, age, product price,  etc. 
Linear regression algorithm shows a  linear relationship between a dependent (y) and one or more independent (y) 
variables, hence called as linear regression. Since linear regression shows the linear relationship, which means it 
finds how the value of the dependent variable is changing accor ding to the value of the independent variable.  
The linear regression model provides a sloped straight line representing the relationship between the variables. 
Consider the below image:  

36 
  
Mathematically, we can represent a linear regression as:  
    
   y= a 0+a1x+ ε 
 
Here,  
Y= Dependent Variable (Target Variable)  
X= Independent Variable (predictor Variable)  
a0= intercept of the line (Gives an additional degree of freedom)  
a1 = Linear regression coefficient (scale factor to each input value).  
ε = random error  
The values for x and y variables are training datasets for Linear Regression model representation.  
 
Types of Linear Regression  
 
Linear regression can be further divided into two types of the algorithm:  
o Simple Linear Regression:  
If a single independent variable is used to predict the value of a numerical dependent variable, then such a 
Linear Regression algorithm is called Simple Linear Regression.  
o Multiple Linear regression:  
If more than one independent variable is used to predict the value of a numeric al dependent variable, then 
such a Linear Regression algorithm is called Multiple Linear Regression.  
Linear Regression Line:  
A linear line showing the relationship between the dependent and independent variables is called a  regression line . 
A regression li ne can show two types of relationship:  
o Positive Linear Relationship:  
If the dependent variable increases on the Y -axis and independent variable increases on X -axis, then such a 
relationship is termed as a Positive linear relationship.  

37 
  
o Negative Linear Rel ationship:  
If the dependent variable decreases on the Y -axis and independent variable increases on the X -axis, then 
such a relationship is called a negative linear relationship.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Finding the best fit 
line:  
 
When working with linear regression, our main goal is to find the best fit line that means the error between 
predicted values and actual values should be minimized. The best fit line will have the least error.  
The different values for weights or the coefficient of lines (a 0, a1) gives a different line of regression, so we 
need to calculate the best values for a 0 and a 1 to find the best fit line, so to calculate this we use cost function.  
Cost function - 
o The different values for weights or coefficient of lines (a 0, a1) gives the diff erent line of regression, and the 
cost function is used to estimate the values of the coefficient for the best fit line.  
o Cost function optimizes the regression coefficients or weights. It measures how a linear regression model 
is performing.  
o We can use the  cost function to find the accuracy of the  mapping function , which maps the input variable 
to the output variable. This mapping function is also known as  Hypothesis function . 
For Linear Regression, we use the  Mean Squared Error (MSE)  cost function, which i s the average of 
squared error occurred between the predicted values and actual values. It can be written as:  
 

38 
 For the above linear equation, MSE can be calculated as:  
 
 
 
Where,  
N=Total number of observation  
Yi = Actual value  
(a1x i+a0)= Predicted value.  
 
Residuals:  The distance between the actual value and predicted values is called residual. If the observed points are 
far from the regression line, then the residual will be high, and so cost function will high. If the scatter points are 
close to the regres sion line, then the residual will be small and hence the cost function.  
 
Gradient Descent:  
o Gradient descent is used to minimize the MSE by calculating the gradient of the cost function.  
o A regression model uses gradient descent to update the coefficients of  the line by reducing the cost 
function.  
o It is done by a random selection of values of coefficient and then iteratively update the values to reach the 
minimum cost function.  
Model Performance:  
The Goodness of fit determines how the line of regression fits the set of observations. The process of 
finding the best model out of various models is called  optimization . It can be achieved by below method:  
 
1. R-squared method:  
o R-squared is a statistical method that determines the goodness of fit.  
o It measures the st rength of the relationship between the dependent and independent variables on a scale of 
0-100%.  
o The high value of R -square determines the less difference between the predicted values and actual values 
and hence represents a good model.  
o It is also called a  coefficient of determination,  or coefficient of multiple determination  for multiple 
regression.  
o It can be calculated from the below formula:  
 
Assumptions of Linear Regression  
Below are some important assumptions of Linear Regression. These are some formal checks while building a 
Linear Regression model, which ensures to get the best possible result from the given dataset.  
 
o Linear relationship between the features and target:  
Linear regression assumes the linear relationship between the dependent and independent variables.  
o Small or no multicollinearity between the features:  
Multicollinearity means high -correlation between the independent variables. Due to multicollinearity, it 
may difficult to find the true relationship between the predictors and targe t variables. Or we can say, it is 

39 
 difficult to determine which predictor variable is affecting the target variable and which is not. So, the 
model assumes either little or no multicollinearity between the features or independent variables.  
o Homoscedasticity  Assumption:  
Homoscedasticity is a situation when the error term is the same for all the values of independent variables. 
With homoscedasticity, there should be no clear pattern distribution of data in the scatter plot.  
o Normal distribution of error terms:  
Linear regression assumes that the error term should follow the normal distribution pattern. If error terms 
are not normally distributed, then confidence intervals will become either too wide or too narrow, which 
may cause difficulties in finding coefficie nts. 
It can be checked using the  q-q plot . If the plot shows a straight line without any deviation, which means 
the error is normally distributed.  
o No autocorrelations:  
The linear regression model assumes no autocorrelation in error terms. If there will be any correlation in 
the error term, then it will drastically reduce the accuracy of the model. Autocorrelation usually occurs if 
there is a dependency between residual errors.  
Simple Linear Regression in Machine Learning  
Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent 
variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or 
a sloped straight line, hence it is called Simple Linear Regressio n. 
The key point in Simple Linear Regression is that the  dependent variable must be a continuous/real value . 
However, the independent variable can be measured on continuous or categorical values.  
Simple Linear regression algorithm has mainly two objectives : 
o Model the relationship between the two variables.  Such as the relationship between Income and 
expenditure, experience and Salary, etc.  
o Forecasting new observations.  Such as Weather forecasting according to temperature, Revenue of a 
company according to t he investments in a year, etc.  
Simple Linear Regression Model:  
The Simple Linear Regression model can be represented using the below equation:  
y= a 0+a1x+ ε  
 
 
Where,  
a0= It is the intercept of the Regression line (can be obtained putting x=0)  
a1= It is the  slope of the regression line, which tells whether the line is increasing or decreasing.  
ε = The error term. (For a good model it will be negligible)  
 
2.2.3. Multiple Linear Regressions  
In the previous topic, we have learned about Simple Linear Regression,  where a single 
Independent/Predictor(X) variable is used to model the response variable (Y). But there may be various cases in 
which the response variable is affected by more than one predictor variable; for such cases, the Multiple Linear 
Regression algo rithm is used.  
40 
  
Moreover, Multiple Linear Regression is an extension of Simple Linear regression as it takes more than one 
predictor variable to predict the response variable.  
 
 We can define it as:  
“Multiple Linear Regression is one of the important regre ssion algorithms which models the linear relationship 
between a single dependent continuous variable and more than one independent variable. ” 
 
Example:  
Prediction of CO 2 emission based on engine size and number of cylinders in a car.  
 
Some key points about  MLR:  
o For MLR, the dependent or target variable(Y) must be the continuous/real, but the predictor or independent 
variable may be of continuous or categorical form.  
o Each feature variable must model the linear relationship with the dependent variable.  
o MLR tries to fit a regression line through a multidimensional space of data -points.  
MLR equation:  
In Multiple Linear Regression, the target variable(Y) is a linear combination of multiple predictor variables 
x1, x2, x3, ...,x n. Since it is an enhancement of Simple Linear Regression, so the same is applied for the multiple 
linear regression equation, the equation becomes:  
Y= b<sub>0</sub>+b<sub>1</sub>x<sub>1</sub>+  b<sub>2</sub>x<sub>2</sub>+  b<sub>3</sub>x<sub>
3</sub>+......  bnxn        ...............  (a)  
Where,  
Y= Output/Response variable  
b0, b1, b2, b3 , bn....= Coefficients of the model.  
x1, x2, x3, x4,...= Various Independent/feature variable  
Assumptions for Multiple Linear Regression:  
o A linear relationship  should exist between the Target and predictor v ariables.  
o The regression residuals must be  normally distributed . 
o MLR assumes little or  no multicollinearity  (correlation between the independent variable) in data.  
2.3. Neural Networks (ANN - Artificial Neural Network ) 
 
2.3.1. Introduction  
The term " Artifi cial Neural Network " is derived from Biological neural networks that develop the structure 
of a human brain. Similar to the human brain that has neurons interconnected to one another, artificial neural 
networks also have neurons that are interconnected to one another in various layers of the networks. These neurons 
are known as nodes.  
 
41 
  
 
The given figure illustrates the typical diagram of Biological Neural Network.  
 
The typical Artificial Neural Network looks something like the given figure.  
 
 
Dendrites from Biological Neural Network represent inputs in Artificial Neural Networks, cell nucleus represents 
Nodes, synapse represents Weights, and Axon represents Output.  
 
Relationship between Biological neural network and artificial neural network:  
 
Biological  Neural Network  Artificial Neural Network  
Dendrites  Inputs  
Cell nucleus  Nodes  
Synapse  Weights  
Axon  Output  
 
An Artificial Neural Network  in the field of  Artificial intelligence  where it attempts to mimic the network of 
neurons makes up a human brain so  that computers will have an option to understand things and make decisions in 
a human -like manner. The artificial neural network is designed by programming computers to behave simply like 
interconnected brain cells.  
 
There are around 1000 billion neurons in the human brain. Each neuron has an association point somewhere in the 
range of 1,000 and 100,000. In the human brain, data is stored in such a manner as to be distributed, and we can 

42 
 extract more than one piece of this data when necessary from our memo ry parallelly. We can say that the human 
brain is made up of incredibly amazing parallel processors.  
 
 
We can understand the artificial neural network with an example, consider an example of a digital logic gate that 
takes an input and gives an output. "OR " gate, which takes two inputs. If one or both the inputs are "On," then we 
get "On" in output. If both the inputs are "Off," then we get "Off" in output. Here the output depends upon input. 
Our brain does not perform the same task. The outputs to inputs r elationship keep changing because of the neurons 
in our brain, which are "learning."  
 
The architecture of an artificial neural network:  
 
 
 
Input Layer:  
As the name suggests, it accepts inputs in several different formats provided by the programmer.  
 
Hidden Layer:  
The hidden layer presents in -between input and output layers. It performs all the calculations to find 
hidden features and patterns.  
 
Output Layer:  
The input goes through a series of transformations using the hidden layer, which finally resul ts in output 
that is conveyed using this layer.  
 
The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This 
computation is represented in the form of a transfer function.  
 
 
 
It determines weighted total  is passed as an input to an activation function to produce the output. Activation 
functions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are 
distinctive activation functions available that can be ap plied upon the sort of task we are performing.  
 
Advantages of Artificial Neural Network (ANN)  
 
Parallel processing capability:  
Artificial neural networks have a numerical value that can perform more than one task simultaneously.  
 
Storing data on the entire  network:  
Data that is used in traditional programming is stored on the whole network, not on a database. The 
disappearance of a couple of pieces of data in one place doesn't prevent the network from working.  
 
Capability to work with incomplete knowledge:  
After ANN training, the information may produce output even with inadequate data. The loss of 

43 
 performance here relies upon the significance of missing data.  
 
Having a memory distribution:  
For ANN is to be able to adapt, it is important to determine the examples and to encourage the network 
according to the desired output by demonstrating these examples to the network. The succession of the network is 
directly proportional to the chosen instances, and if the event can't appear to the network in all its as pects, it can 
produce false output.  
 
Having fault tolerance:  
Extortion of one or more cells of ANN does not prohibit it from generating output, and this feature makes 
the network fault -tolerance.  
 
Disadvantages of Artificial Neural Network:  
 
Assurance of p roper network structure:  
There is no particular guideline for determining the structure of artificial neural networks. The appropriate 
network structure is accomplished through experience, trial, and error.  
 
Unrecognized behavior of the network:  
It is the most significant issue of ANN. When ANN produces a testing solution, it does not provide insight 
concerning why and how. It decreases trust in the network.  
 
Hardware dependence:  
Artificial neural networks need processors with parallel processing power, as per their structure. Therefore, 
the realization of the equipment is dependent.  
 
Difficulty of showing the issue to the network:  
ANNs can work with numerical data. Problems must be converted into numerical values before being 
introduced to ANN. The presenta tion mechanism to be resolved here will directly impact the performance of the 
network. It relies on the user's abilities.  
 
The duration of the network is unknown:  
The network is reduced to a specific value of the error, and this value does not give us opt imum results.  
“Science artificial neural networks that have steeped into the world in the mid -20th century are exponentially 
developing. In the present time, we have investigated the pros of artificial neural networks and the issues 
encountered in the cour se of their utilization. It should not be overlooked that the cons of ANN networks, which are 
a flourishing science branch, are eliminated individually, and their pros are increasing day by day. It means that 
artificial neural networks will turn into an ir replaceable part of our lives progressively important. ” 
 
How do artificial neural networks work?  
Artificial Neural Network can be best represented as a weighted directed graph, where the artificial 
neurons form the nodes. The association between the neuron s outputs and neuron inputs can be viewed as the 
directed edges with weights. The Artificial Neural Network receives the input signal from the external source in the 
form of a pattern and image in the form of a vector. These inputs are then mathematically assigned by the notations 
x(n) for every n number of inputs.  
 
44 
  
 
Afterward, each of the input is multiplied by its corresponding weights ( these  weights are the details 
utilized by the artificial neural networks to solve a specific problem ). In general terms, these weights normally 
represent the strength of the interconnection between neurons inside the artificial neural network. All the weighted  
inputs are summarized inside the computing unit.  
 
If the weighted sum is equal to zero, then bias is added to make the output non -zero or something else to 
scale up to the system's response. Bias has the same input, and weight equals to 1. Here the total of weighted inputs 
can be in the range of 0 to positive infinity. Here, to keep the response in the limits of the desired value, a certain 
maximum value is benchmarked, and the total of weighted inputs is passed through the activation function.  
The activat ion function refers to the set of transfer functions used to achieve the desired output. There is a 
different kind of the activation function, but primarily either linear or non -linear sets of functions. Some of the 
commonly used sets of activation functio ns are the Binary, linear, and Tan hyperbolic sigmoidal activation 
functions. Let us take a look at each of them in details:  
 
Binary:  
In binary activation function, the output is either a one or a 0. Here, to accomplish this, there is a threshold 
value set  up. If the net weighted input of neurons is more than 1, then the final output of the activation function is 
returned as one or else the output is returned as 0.  
 
Sigmoidal Hyperbolic:  
The Sigmoidal Hyperbola function is generally seen as an " S" shaped cu rve. Here the tan hyperbolic 
function is used to approximate output from the actual net input. The function is defined as:  
F(x) = (1/1 + exp( -????x))  
Where ???? is considered the Steepness parameter.  
 
Types of Artificial Neural Network:  
There are various t ypes of Artificial Neural Networks (ANN) depending upon the human brain neuron and 
network functions, an artificial neural network similarly performs tasks. The majority of the artificial neural 
networks will have some similarities with a more complex biol ogical partner and are very effective at their expected 
tasks. For example, segmentation or classification.  
 
Feedback ANN:  
In this type of ANN, the output returns into the network to accomplish the best -evolved results internally. 
As per the  University of Massachusetts , Lowell Centre for Atmospheric Research. The feedback networks feed 
information back into itself and are well suited to solve optimization issues. The Internal system error corrections 
utilize feedback ANNs.  
 
Feed -Forward ANN:  
A feed -forward network is a basic neural network comprising of an input layer, an output layer, and at least 
one layer of a neuron. Through assessment of its output by reviewing its input, the intensity of the network can be 
noticed based on group behavior of the associa ted neurons, and the output is decided. The primary advantage of this 
network is that it figures out how to evaluate and recognize input patterns.  
 

45 
 Prerequisite  
No specific expertise is needed as a prerequisite before starting this tutorial.  
 
Audience  
Our Artificial Neural Network Tutorial is developed for beginners as well as professionals, to help them 
understand the basic concept of ANNs.  
 
2.3.2. PERCEPTRONS  
One type of ANN system is based on a unit called a perceptron, illustrated in below Figure: A 
perceptron takes a vector of real -valued inputs, calculates a linear combination of these inputs, then 
outputs a 1 if the result is greater than some threshold and -1 otherwise. More precisely, given inputs xl 
through xn the output o(x l, . . . , xn) computed by the perceptron is  
 
 
 
 
 
where each
wi  is a real -valued constant, or weight, that determines the contribution of input 
xi  to 
the perceptron output. Notice the quantity 
) (0w   is a threshold that the weighted combination of 
inputs 
xw xw n n....11  must surpass in order for the perceptron to output a 1.  
 
To simplify notation, we imagine an additional constant input
10x , allowing us to write the above 
inequ ality as
00n
i iixw , or in vector form as 
oxw
. . For brevity, we will sometimes write the 
perceptron function as  
 
  
 
Learning a perceptron involves choosing values for the weights 
w w n,....,0 Therefore, the space H of  
candidate hypotheses considered in perceptron learning is the set of all possible real -valued weight 
vectors.  
 
Representational Power of Perceptrons:  
 
We can view the perceptron as representing a hyperplane  decision surface in the n --dimensional space 
of instances (i.e., points). The perceptron outputs a 1 for instances lying on one side of the hyperplane 

46 
 and outputs a -1 for instances lying on the other side, as illustrated in Figure below The equation for this 
decision hyperplane is 
0.
xw . Of course, some sets of positive and negative examples cannot be 
separated by any hyperplane. Those that can be separated are called linearly separable sets of 
examples.  
 
 
The decision surface represented by a two -input perceptron. (a) A set of training examples and 
the decision surface of a perceptron that classifies them correctly. (b) A set of training examples that is 
not linearly separable (i.e., that cannot be correctly classified by any s traight line). xl and x2 are the 
Perceptron inputs. Positive examples are indicated by "+", negative by "-". The inputs are fed to multiple 
units, and the outputs of these units are then input to a second, final stage. One way is to represent the 
Boolean f unction in disjunctive normal form (i.e., as the disjunction (OR) of a set of conjunctions (ANDs) 
of the inputs and their negations). Note that the input to an AND perceptron can be negated simply by 
changing the sign of the corresponding input weight. Bec ause networks of threshold units can represent 
a rich variety of functions and because single units alone cannot, we will generally be interested in 
learning multilayer networks of threshold units.  
 
The Perceptron Training Rule  
Although we are interested i n learning networks of many interconnected units, let us begin by 
understanding how to learn the weights for a single perceptron. Here the precise learning problem is to 
determine a weight vector that causes the perceptron to produce the correct 
1  output for each of the 
given training examples.  
Several algorithms are known to solve this learning problem. Here we consider two: the 
perceptron rule and the delta rule. These two algorithms are guaranteed to converge to somewhat 
different acceptable hypotheses, under somewhat different conditions. They are important to ANNs 
because they provide the basis for learning networks of many units.  
One way to learn an acceptable weight vector is to begin with random weights, then iteratively 
apply the perceptron to each training example, modifying the perceptron weights whenever it 
misclassifies an example. This process is repeated, iterating through the training examples as many 
times as needed until  
the perceptron classifies all training examples correctly. Weights are modified at each step according to 
the perceptron training rule,  which revises the weight wi associated with input xi according to the rule  
 
 
Here t is the target output for the current training example, o is the output generated by the 

47 
 perceptron, and 
 is a positive constant called the learning rate. The role of the learning rate is to 
moderate the degree to which weights are changed at each step. It is usually set to some small value 
(e.g., 0.1) and is sometimes made to decay as the number of weight -tuning iterations increases.  
Why should this update rule converge toward successful weight values? To get an intuitive feel, 
consider some specific cases. Suppose the training example is cor rectly classified already by the 
perceptron. In this case, (t - o) is zero, making 
wi  zero, so that no weights are updated. Suppose the 
perceptron outputs a -1, when the target output is +1. To make the perceptron output a+1 instead of -1 
in this case, the weights must be altered to increase the value of 

xw.  For example, if xi>0, then 
increasing wi will bring the perceptron closer to correctly classifying this example. Notice the training 
rule will increase w, in this case, because (t - o),
 , and xi are all positive. For example, if xi = .8, 
  = 0.1, 
t = 1, and o = - 1, then the weight update will be 
wi  = 
 (t - o)x i = O.1(1 - (-1))0.8 = 0.16. On the 
other hand, if t = -1 and o = 1, then weights associated with positive xi will be decreased rather than 
increased.  
 
In fact, the above learning procedure can be proven to converge within a finite number of applications 
of the percep tron training rule to a weight vector that correctly classifies all training examples, provided 
the training examples are linearly separable and provided a sufficiently small 
  is used . If the data are 
not linearly separable, convergen ce is not assured.  
 
Gradient Descent and the Delta Rule  
Although the perceptron rule finds a successful weight vector when the training examples are 
linearly separable, it can fail to converge if the examples are not linearly separable. A second training 
rule, called the delta rule, is designed to overcome this difficulty. If the training examples are not 
linearly separable, the  delta rule converges toward a best -fit approximation to the target concept. The 
key idea behind the delta rule is  to use gradient descent to search the hypothesis space of possible 
weight vectors to find the weights that best fit the training examples. This rule is important because 
gradient descent provides the basis for the BACKPROPAGATION algorithm, which can lear n networks 
with many interconnected units. It is also important because gradient descent can serve as the basis for 
learning algorithms that must search through hypothesis spaces containing many different types of 
continuously parameterized hypotheses.  
The delta training rule is best understood by considering the task of training an unthresholded 
perceptron; that is, a linear unit for which the output o is given by  
 
 
 
Thus, a linear unit corresponds to the first stage of a perceptron, without the thresh old. 
In order to derive a weight learning rule for linear units, let us begin by specifying a measure for the 
training error of a hypothesis (weight vector), relative to the training examples. Although there are 
many ways to define this error, one common m easure that will turn out to be especially convenient is  
 
where D is the set of training examples, td is the target output for training example d, and od is the 
output of the linear unit for training example d. By this definition, 

wE  is simply half the squared 

48 
 difference between the target output td and the hear unit output od, summed over all training 
examples. Here we characterize E as a function of 

w , because the linear unit output o depends on this 
weight vecto r. Of course E also depends on the particular set of training examples, but we assume these 
are fixed during training, so we do not bother to write E as an explicit function of these. In particular, 
there we show that under certain conditions the hypothesis that minimizes E is also the most probable 
hypothesis in H given the training data.  
 
2.3.2. Multi -layer Perceptron  
 
Multi -layer Perceptron (MLP)  is a supervised learning algorithm that learns a 
function  f(⋅):Rm→Ro by training on a dataset, where  m is the number of dimensions for input and  o is the 
number of dimensions for output. Given a set of features  X=x1,x2,...,xm and a target  y, it can learn a non -
linear function approximator for either classification or regression. It is different from  logistic 
regression, in that between the input and the output layer, there can be one or more non -linear layers, 
called hidden layers. Figure  shows a one hidden layer MLP with scalar output.  
 
The leftmost layer, known as the input layer, consists of a s et of neurons  {xi|x1,x2,...,x m} representing the 
input features. Each neuron in the hidden layer transforms the values from the previous layer with a 
weighted linear summation  w1x1+w 2x2+...+w mxm, followed by a non -linear activation function  g(⋅):R→R  - 
like the hyperbolic tan function. The output layer receives the values from the last hidden layer and 
transforms them into output values.  
The module contains the public attributes  coefs_  and intercepts_ . coefs_  is a list of weight matrices, 
where weight matrix  at index  i represents the weights between layer  i and layer  i+1. intercepts_  is a list 
of bias vectors, where the vector at index  i represents the bias values added to layer  i+1. 
The advantages of Multi -layer Perceptron are:  
 Capability to learn non -linear  models.  
 Capability to learn models in real -time (on -line learning) using  partial_fit . 
The disadvantages of Multi -layer Perceptron (MLP) include:  
 MLP with hidden layers have a non -convex loss function where there exists more than one local 
minimum. Therefore different random weight initializations can lead to different validation 
accuracy.  
 MLP requires tuning a number of hyperparameters such as the number of hidden neurons, 
layers, and iterations.  
 MLP is sensitive to feature scaling.  
 

49 
 2.4. Support Vector Machines  
 
Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, 
which is used for Classification as well as Regression problems. However, primarily, it is used for 
Classification problems in Machine L earning. The goal of the SVM algorithm is to create the best line or 
decision boundary that can segregate n -dimensional space into classes so that we can easily put the 
new data point in the correct category in the future. This best decision boundary is ca lled a hyperplane.  
SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are 
called as support vectors, and hence algorithm is termed as Support Vector Machine. Consider the 
below diagram in which there are two di fferent categories that are classified using a decision boundary 
or hyperplane:  
 
 
Example:  SVM can be understood with the example that we have used in the KNN classifier. Suppose 
we see a strange cat that also has some features of dogs, so if we want a m odel that can accurately 
identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will 
first train our model with lots of images of cats and dogs so that it can learn about different features of 
cats and dogs, and then we test it with this strange creature. So as support vector creates a decision 
boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see 
the extreme case of cat and dog. On the basis of the support vectors, it will classify it as a cat. Consider 
the below diagram:  
 
 
SVM algorithm can be used for  Face detection, image classification, text categorization,  etc. 
Types of SVM  
SVM can be of two types:  

50 
 o Linear SVM:  Linear SVM is used for linearly separable data, which means if a dataset can be 
classified into two classes by using a single straight line, then such data is termed as linearly 
separable data, and classifier is used called as Linear SVM classifier.  
o Non -linear SVM:  Non -Linear SVM is used for non -linearl y separated data, which means if a 
dataset cannot be classified by using a straight line, then such data is termed as non -linear data 
and classifier used is called as Non -linear SVM classifier.  
Hyperplane and Support Vectors in the SVM algorithm : 
 Hyperpla ne: There can be multiple lines/decision boundaries to segregate the classes in n -
dimensional space, but we need to find out the best decision boundary that helps to classify the data 
points. This best boundary is known as the hyperplane of SVM.  
The dimens ions of the hyperplane depend on the features present in the dataset, which means if there 
are 2 features (as shown in image), then hyperplane will be a straight line. And if there are 3 features, 
then hyperplane will be a 2 -dimension plane.  
We always create a hyperplane that has a maximum margin, which means the maximum distance 
between the data points.  
 
Support Vectors:  
The data points or vectors that are the closest to the hyperplane and which affect the position of the 
hyperplane are terme d as Support Vector. Since these vectors support the hyperplane, hence called a 
Support vector.  How does SVM works?  
 
2.4.1. Linear SVM:  
The working of the SVM algorithm can be understood by using an example. Suppose we have a dataset 
that has two tags (gre en and blue), and the dataset has two features x1 and x2. We want a classifier 
that can classify the pair(x1, x2) of coordinates in either green or blue. Consider the below image:  
 
So as it is 2 -d space so by just using a straight line, we can easily separate these two classes. But there 
can be multiple lines that can separate these classes. Consider the below image:  
 

51 
  
 
Hence, the SVM algorithm helps to find the best line or decision boundary; this best boundary or region 
is called as a  hyperplane . SVM algorithm finds the closest point of the lines from both the classes. These 
points are called support vectors. The distance between the vectors and the hyperplane is called 
as margin . And the goal of SVM is to maximize this margin. The  hyperplane  with ma ximum margin is 
called the  optimal hyperplane .  
 
2.4.2. Non -Linear SVM:  
           If data is linearly arranged, then we can separate it by using a straight line, but for non -linear 
data, we cannot draw a single straight line. Consider the below image:  
 
 
 
 
 
 
 
 
 
 
 
 
 
So to separate these data points, we need to add one more dimension. For linear data, we have used two dimensions 
x and y, so for non -linear data, we will add a third dimension z. It can be calculated as:  
z=x2 +y2 
By adding the third dimension, the sample space will become as below image:  

52 
  
So now, SVM will divide the datasets into classes in the following way. Consider the below image:  
 
 
Since we are in 3 -d Space, hence it is looking like a plane parallel to the x -axis. If we convert it in 2d 
space with z=1, then it will become as:  
 
 
Hence we get a circumference of radius 1 in case of non -linear data.  
 
2.4.3. SVM Kernels  
In practice, SVM algorithm is implemented with kernel that transforms an input data space into 

53 
 the required form. SVM uses a  technique called the kernel trick in which kernel takes a low dimensional 
input space and transforms it into a higher dimensional space. In simple words, kernel converts non -
separable problems into separable problems by adding more dimensions to it. It ma kes SVM more 
powerful, flexible and accurate. The following are some of the types of kernels used by SVM.  
Linear Kernel  
It can be used as a dot product between any two observations. The formul a of linear kernel is as below  
 
K(x,xi)=sum (x∗xi) 
 
From the abo ve formula, we can see that the product between two vectors say 𝑥 & 𝑥𝑖 is the sum of the 
multiplication of each pair of input values.  
 
2.5. Unsupervised Machine Learning:  
2.5.1. Introduction to clustering  
 
As the name suggests, unsupervised learning is a machine learning technique in which models 
are not supervised using training dataset. Instead, models itself find the hidden patterns and insights 
from the given data. It can be compared to learning which takes place in the human brain while learning 
new  things. It can be defined as:  
 
“Unsupervised learning is a type of machine learning in which models are trained using 
unlabeled dataset and are allowed to act on that data without any supervision. ” 
 
Unsupervised learning cannot be directly applied to a re gression or classification problem 
because unlike supervised learning, we have the input data but no corresponding output data. The goal 
of unsupervised learning is to  find the underlying structure of dataset, group that data according to 
similarities, and  represent that dataset in a compressed format  
 
Example:  Suppose the unsupervised learning algorithm is given an input dataset containing images of 
different types of cats and dogs. The algorithm is never trained upon the given dataset, which means it 
does not have any idea about the features of the dataset. Th e task of the unsupervised learning 
algorithm is to identify the image features on their own. Unsupervised learning algorithm will perform 
this task by clustering the image dataset into the groups according to similarities between images.  
 
 
Why use Unsupe rvised Learning?  

54 
 Below are some main reasons which describe the importance of Unsupervised Learning:  
o Unsupervised learning is helpful for finding useful insights from the data.  
o Unsupervised learning is much similar as a human learns to think by their own e xperiences, 
which makes it closer to the real AI.  
o Unsupervised learning works on unlabeled and uncategorized data which make unsupervised 
learning more important.  
o In real -world, we do not always have input data with the corresponding output so to solve suc h 
cases, we need unsupervised learning.  
Working of Unsupervised Learning  
 
Working of unsupervised learning can be understood by the below diagram:  
 
 
Here, we have taken an unlabeled input data, which means it is not categorized and 
corresponding outputs are also not given. Now, this unlabeled input data is fed to the machine learning 
model in order to train it. Firstly, it will interpret the raw data to find the hidden patterns from the data 
and then will apply suitable algorithms such as k -means clusteri ng, Decision tree, etc.  
Once it applies the suitable algorithm, the algorithm divides the data objects into groups according to 
the similarities and difference between the objects.  
Types of Unsupervised Learning Algorithm:  
The unsupervised learning algorit hm can be further categorized into two types of problems:  
 

55 
 o Clustering : Clustering is a method of grouping the objects into clusters such that objects with 
most similarities remains into a group and has less or no similarities with the objects of another 
group. Cluster analysis finds the commonalities between the data objects and categorizes them 
as per the presence and absence of those commonalities.  
o Association : An association rule is an unsupervised learning method which is used for finding 
the relations hips between variables in the large database. It determines the set of items that 
occurs together in the dataset. Association rule makes marketing strategy more effective. Such 
as people who buy X item (suppose a bread) are also tend to purchase Y (Butter/ Jam) item. A 
typical example of Association rule is Market Basket Analysis.  
 
 
Unsupervised Learning algorithms:  
 
Below is the list of some popular unsupervised learning algorithms:  
o K-means clustering  
o KNN (k -nearest neighbors)  
o Hierarchal clustering  
o Anomaly detection  
o Neural Networks  
o Principle Component Analysis  
o Independent Component Analysis  
o Apriori algorithm  
o Singular value decomposition  
Advantages of Unsupervised Learning  
o Unsupervised learning is used for more complex tasks as compared to supervised learning  
because, in unsupervised learning, we don't have labeled input data.  
o Unsupervised learning is preferable as it is easy to get unlabeled data in comparison to labeled 
data.  
Disadvantages of Unsupervised Learning  
o Unsupervised learning is intrinsically more difficult than supervised learning as it does not have 
corresponding output.  
o The result of the unsupervised learning algorithm might be less accurate as input data is not 
labeled, and algorithms do not know the exact output in advance.  
56 
 Supervised Learning  Unsupervised Learning  
Supervised learning algorithms are trained using labeled data.  Unsupervised learning algorithms are trained using unlabeled data.  
Supervised learning model takes direct feedback to check if it is predicting 
correct output or not.  Unsupervised learning model does not take any feedback.  
Supervised learning model predicts the output.  Unsupervised learning model finds the hidden patterns in data.  
In supervised learning, input data is provided to the model along with the 
output.  In unsupervised learning, only input data is provided to the model.  
The goal of supervised learning is to train the model so that it can predict 
the output when it is given new data.  The goal of unsupervised learning is to find the hidden patterns and 
useful  insights from the unknown dataset.  
Supervised learning needs supervision to train the model.  Unsupervised learning does not need any supervision to train the 
model.  
Supervised learning can be categorized 
in Classification  and Regression  problems.  Unsupervised Learning can be classified 
in Clustering  and Associations  problems.  
Supervised learning can be used for those cases where we know the input 
as well as corresponding outputs.  Unsupervised learning can be used for those cases where we have 
only input data and no corresponding output data.  
Supervised learning model produces an accurate result.  Unsupervised learning model may give less accurate result as 
compared to supervised learning.  
Supervised learning is not close to true Artificial intelligence as in this, we 
first train the model for each data, and then only it can predict the correct 
output.  Unsupervised learning is more close to the true Artificial 
Intelligence as it learns similarly as a child learns daily routine 
things by his e xperiences.  
It includes various algorithms such as Linear Regression, Logistic 
Regression, Support Vector Machine, Multi -class Classification, Decision 
tree, Bayesian Logic, etc.  It includes various algorithms such as Clustering, KNN, and Apriori  
algorithm.  
 
2.5.2. K -Mean Clustering  
 
k-means clustering algorithm  
One of the most used clustering algorithm is  k-means . It allows to group the data according to the 
existing similarities among them in  k clusters, given as input to the algorithm. I’ll start with a simple 
example.  
Let’s imagine we have 5 objects (say 5 people) and for each of them we know two features (height and 
weight). We want to group them into  k=2 clusters.  
 
Our dataset will look like this:  
57 
  
First of all, we have to initialize the value of the centroids for our clusters. For instance, let’s choose 
Person 2 and Person 3 as the two centroids  c1 and c2, so that  c1=(120,32)  and c2=(113,33) . 
Now we compute the euclidian distance between each of the two centroids and each point in the dat a. 
If you did all the calculations, you should have come up with the following numbers:  
 
 Distance of object from  c1 Distance of object from  c2 
Person 1  52.3 58.3 
Person 2  0 7.1 
Person 3  7.1 0 
Person 4  70.4 75.4 
Person 5  13.9 9.4 
 
At this point, we will assign each object  to the cluster it is closer to (that is taking the minimum between 
the two computed distances for each object).  
 
We can then arrange the points as follows:  
 
Person 1 → cluster 1  
Person 2 → cluster 1  
Person 3 → cluster 2  
Person  4 → cluster 1  
Person 5→ cluster 2  
 
Let’s iterate, which means  to redefine the centroids by calculating the mean of the members of each of 
the two clusters.  
 
So c’1 = ((167+120+175)/3, (55+32+76)/3) = (154, 54.3) and  c’2 = ((113+108)/2, (33+25)/2) = (110.5, 29)  
 
Then, we calculate the distances again and re -assign the points to the new centroids.  
 
We repeat this process until the centroids don’t move anymore (or the difference between them is 
under a certain small threshold).  
 
In our case, the result we get is gi ven in the figure below.  You can see the two different clusters labelled 
with two different colours and the position of the centroids, given by the crosses.  
 

58 
  
How to apply k -means?  
As you probably already know, I’m using Python libraries to analyze my data. The  k-means  algorithm is 
implemented in the  scikit -learn  package. To use it, you will just need the following line in your script:  
 
What if our data is… non -numerical?  
 
At this point, you will maybe have noticed something. The basic concept of  k-mean s stands on 
mathematical calculations (means, euclidian distances). But what if our data is non -numerical or, in 
other words,  categorical ? Imagine, for instance, to have the ID code and date of birth of the five people 
of the previous example, instead of t heir heights and weights.  
We could think of transforming our categorical values in numerical values and eventually apply  k-
means . But beware:  k-means  uses numerical distances, so it could consider close two really distant 
objects that merely have been assi gned two close numbers.  
 
 
 
 
 
k-modes  is an extension of  k-means . Instead of distances it uses  dissimilarities  (that is, 
quantification of the total mismatches between two objects: the smaller this number, the more similar 
the two objects). And instead of means, it uses  modes.  A mode is a vector of elements that minimizes 
the dissimilarities between the vector itself  and each object of the data. We will have as many modes as 
the number of clusters we required, since they act  as centroids.  
 
 
 
 
 
 
 
 
 
 
 
 

59 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Unit III  
Ensemble and Probabilistic Learning  
 
Ensemble   Learning:  Model Combination Schemes, Voting, Error -Correcting Output Codes, 
Bagging: Random Forest Trees, Boosting: Adaboost, Stacking.  
Probabilistic Learning:  Gaussian mixture models - The Expectation -Maximization (EM) 
Algorithm, Information Criteria, Nearest neighbour methods - Nearest Neighbour Smoothing, 
Efficient Distance Computations: the KD -Tree, Distance Measures.  
 
3. Introduction:      
 
Ensemble Learnin g 
Ensemble learning usually produces more accurate solutions than a single model would. 
60 
 Ensemble Learning is a technique that create multiple models and then combine them them to produce 
improved results. Ensemble learning usually produces more accurate so lutions than a single model 
would.  
 Ensemble learning methods is applied to regression as well as classification.  
o Ensemble learning for regression creates multiple repressors i.e. multiple regression 
models such as linear, polynomial, etc.  
o Ensemble learn ing for classification creates multiple classifiers i.e. multiple classification 
models such as logistic, decision tress, KNN, SVM, etc.  
 
Figure 1: Ensemble learning view  
Which components to combine?  
• different learning algorithms  
• same learning algorithm trained in different ways  
• same learning algorithm trained the same way  
 
There are two steps in ensemble learning:  
Multiples machine learning models were generated using same or different machine learning 
algorithm. These are called “base models”.  The prediction perform on the basis of base models.  
Techniques/Methods in ensemble learning  
Voting, Error -Correcting Output Codes, Bagging: Random Forest Trees, Boosting: Adaboost, Stacking.  
3.1 Model Combination Schemes - Combining Multiple Learners  
We discus sed many different learning algorithms in the previous chapters. Though these are 
generally successful, no one single algorithm is always the most accurate. Now, we are going to discuss 
models composed of multiple learners that complement each other so tha t by combining them, we 
attain higher accuracy.  
 
There are also different ways the multiple base -learners are combined to generate the final 
output:  
 
Figure2: General Idea - Combining Multiple Learners  

61 
  
 
Multiexpert combination   
Multiexpert combination  methods have base -learners that work in parallel . These methods can 
in turn be divided into two:  
 
 In the global approach, also called learner fusion , given an input, all base -learners generate an 
output and all these outputs are used.  
Examples are voting  and stacking . 
 In the local approach, or learner selection , for example, in mixture of experts , there is a gating 
model, which looks at the input and chooses one (or very few) of the learners as responsible for 
generating the output.  
 
Multistage combination  
Multistage combination methods use a serial approach where the next base -learner is trained 
with or tested on only the instances where the previous base -learners are not accurate enough. The 
idea is that the base -learners (or the different representations they use) are sorted in increasing 
complexit y so that a complex base -learner is not used (or its complex representation is not extracted) 
unless the preceding simpler base -learners are not confident.  
An example is cascading . 
 
Let us say that we have L base -learners. We denote by dj(x) the predictio n of base -learner Mj given the 
arbitrary dimensional input x. In the case of multiple representations, each Mj uses a different input 
representation xj . The final prediction is calculated from the predictions of  
the base -learners:  
 
y = f (d 1, d2, . . . , dL |Φ) 
 
where f (·) is the combining function with Φ denoting its parameters.  

62 
  
Figure 1: Base -learners are dj and their outputs are combined using f (·). This is for a single 
output; in the case of classification, each base -learner has K outputs that are separately used to 
calculate yi, and then we choose the maximum.  Note that here all learners observe the same input; it 
may be the case that different learners observe different representations of the same input object or 
event.  
 
When th ere are K outputs, for each learner there are dji(x), i = 1, . . . , K,  
j = 1, . . . , L , and, combining them, we also generate K values, yi, i = 1, . . . , K and then for example in 
classification, we choose the class with  
the maximum yi value:  
                                    
 
3.2 Voting  
The simplest way to combine multiple classifiers is by voting , which corresponds to taking a 
linear combination of the learn  
 
ers, Refer figure 1.  
 
 
This is also known as ensembles and linear opinion pools . In the sim plest case, all learners are 
given equal weight and we have simple voting that corresponds to taking an average. Still, taking a 
(weighted) sum is only one of the possibilities and there are also other combination rules, as shown in 
table 1. I f the outputs are not posterior probabilities, these rules require that outputs be normalized to 
the same scale  
 
Table 1 - Classifier combination rules  
 
 

63 
 An example of the use of these rules is shown in table 2, which demonstrates the effects of 
different  rules. Sum rule is the most intuitive and is the most widely used in practice. Median rule is 
more robust to outliers; minimum and maximum rules are pessimistic and optimistic, respectively. With 
the product rule, each learner has veto power; regardless o f the other ones, if one learner has an 
output of 0, the overall output goes to 0. Note that after the combination rules, yi do not necessarily 
sum up to 1.  
 
Table 2: Example of combination rules on three learners and three classes  
 
In weighted sum, dji is the vote of learner j for class Ci and wj is the weight of its vote. Simple voting is a 
special case where all voters have equal weight, namely, wj = 1/L. In classification, this is called plurality 
voting where the class having the maximum number of vo tes is the winner.  
 
When there are two classes, this is majority voting where the winning class gets more than half of the 
votes. If the voters can also supply the additional information of how much they vote for each class 
(e.g., by the posterior probabil ity), then after normalization, these can be used as weights in a weighted 
voting scheme. Equivalently, if dji are the class posterior probabilities, P(Ci | x,Mj ), then we can just sum 
them up ( wj = 1/L) and choose the class with maximum yi . 
 
In the case  of regression, simple or weighted averaging or median can be used to fuse the outputs of 
base -regressors. Median is more robust to noise than the average.  
 
Another possible way to find wj is to assess the accuracies of the learners (regressor or classifie r) on a 
separate validation set and use that information to compute the weights, so that we give more weights 
to more accurate learners.  
 
Voting schemes can be seen as approximations under a Bayesian framework with weights 
approximating prior model probabi lities, and model decisions approximating model -conditional 
likelihoods.   
 
 
 
Simple voting corresponds to a uniform prior. If we have a prior distribution preferring simpler models, 
this would give larger weights to them. We cannot integrate over all mode ls; we only choose a subset 
for which we believe P(Mj ) is high, or we can have another Bayesian step and calculate P(Ci | x,Mj ), the 
probability of a model given the sample, and sample high probable models from this density.  
 
Let us assume that dj are ii d with expected value E[d j] and variance Var (dj ), then when we take a simple 

64 
 average with wj = 1/L, the expected value and variance of the output are  
 
 
We see that the expected value does not change, so the bias does not change. But variance, and 
therefore mean square error, decreases as the number of independent voters, L, increases. In the 
general case,  
 
 
 
which implies that if learners are positively correlated, variance (and error) increase. We can thus view 
using different algor ithms and input features as efforts to decrease, if not completely eliminate, the 
positive correlation.  
 
3.3 Error -Correcting Output Codes  
The Error -Correcting  Output  Codes  method is a technique that allows a multi -class classification 
problem to be reframed as multiple binary classification problems, allowing the use of native binary 
classification models to be used directly.  
 
Unlike  one-vs-rest and one-vs-one methods  that offer a similar solution by dividing a multi -class 
classification problem into a fixed number of binary classification problems, the error -correcting output 
codes technique allows each class to be encoded as an arbitrary numbe r of binary classification 
problems. When an overdetermined representation is used, it allows the extra models to act as “error -
correction” predictions that can result in better predictive performance.  
 
In error -correcting output codes (ECOC), the main cla ssification task is defined in terms of a 
number of subtasks that are implemented by the base -learners. The idea is that the original task of 
separating one class from all other classes may be a difficult problem. Instead, we want to define a set 
of simple r classification problems, each specializing in one aspect of the task, and combining these 
simpler classifiers, we get the final classifier.  
 
Base -learners are binary classifiers having output −1/ + 1, and there is a code matrix W of K × L whose K 
rows are the binary codes of classes in terms of the L base -learners dj. For example, if the second row of 
W is [−1,+1,+1,−1], this means that for us to say an instance belongs to C2, the instance should be on the 
negative side of d1 and d4, and on the pos itive side of d2 and d3. Similarly, the columns of the code 
matrix defines the task of the base -learners. For example, if the third column is [−1,+1,+1]T , we 
understand that the task of the third base -learner, d3, is to separate the instances of C1 from t he 
instances of C2 and C3 combined. This is how we form the training set of the base -learners. For example 
in this case, all instances labeled with C2 and C3 form X+
3 and instances labeled with C1 form X−
3, and d3 is 
trained so that xt ∈ X+
3 give output +1 and xt ∈ X−
3 give output −1. 
 
The code matrix thus allows us to define a polychotomy ( K > 2 classification problem) in terms of 
dichotomies ( K = 2 classification problem), and it is a method that is applicable using any learning 

65 
 algorithm to implement the dichotomizer base -learners —for example, linear or multilayer perceptrons 
(with a single output), decision trees, or SVMs whose original definit ion is for two -class problems.  
The typical one discriminant per class setting corresponds to the diagonal code matrix where L = K. For 
example, for K = 4, 
 
 
we have  
                     
The problem here is that if there is an error with one of the baselea rners, there may be a 
misclassification because the class code words are so similar. So the approach in error -correcting codes 
is to have L > K and increase the Hamming distance between the code words. One possibility is pairwise 
separation of classes wher e there is a separate baselearner to separate C i from C j, for i < j. In this case, L 
= K(K − 1)/2 and with K = 4, the code matrix is  
 
 
where a 0 entry denotes “don’t care.” That is, d1 is trained to separate C 1 from C 2 and does not use the 
training instances belonging to the other classes. Similarly, we say that an instance belongs to C 2 if d1 = 
−1 and d4 = d5 = +1, and we do not consider the values of d2, d3, and d6. The problem here is that L is 
O(K2), and for large K pairwise separation may not be feasible.  
 
If we can have L high, we can just randomly generate the code matrix with −1 / + 1 and this will work 
fine, but if we want to keep L low, we need to optimize W. The approach is to set L beforehand and 
then find W such that the distances between rows, and at the same time the distances between 
columns, are as large as possible, in terms of Hamming distance. With K classes, there are 2(K-1) − 1 
possible columns, namely, two -class problems. This is because K bits can b e written in 2 K different ways 
and complements (e.g., “0101” and “1010,” from our point of view, define the same discriminant) 
dividing the possible combinations by 2 and then subtracting 1 because a column of all 0s (or 1s) is 
useless. For example, when K = 4, we have  
 
 
 
When K is large, for a given value of L, we look for L columns out of the 2(K-1)−1. We would like these 
columns of W to be as different as possible so that the tasks to be learned by the base -learners are as 
different from each other as possible. At the same time, we would like the rows of W to be as different 
as possible so that we can have maximum error correction in c ase one or more base -learners fail.  
 
ECOC can be written as a voting scheme where the entries of W, wij , are considered as vote weights:  

66 
  
and then we choose the class with the highest yi . Taking a weighted sum and then choosing the 
maximum instead of ch ecking for an exact match allows dj to no longer need to be binary but to take a 
value between −1 and +1, carrying soft certainties instead of hard decisions. Note that a value pj 
between 0 and 1, for example, a posterior probability, can be converted to a  value dj between −1 and +1 
simply as  
 
One problem with ECOC is that because the code matrix W is set a priori, there is no guarantee that the 
subtasks as defined by the columns of W will be simple.  
 
3.4 Bagging  
Bootstrap aggregating, often abbreviated as  bagging, involves having each model in the 
ensemble vote with equal weight. In order to promote model variance, bagging trains each model in the 
ensemble using a randomly drawn subset of the training set. As an e xample, the  random 
forest  algorithm combines random decision trees with bagging to achieve very high classification 
accuracy.  
The simplest method of combining classifiers is known  as bagging, which stands for bootstrap 
aggregating, the statistical description of the method. This is fine if you know what a bootstrap is, but 
fairly useless if you don’t. A bootstrap sample is a sample taken from the original dataset with 
replacement, so that we may get some data several times and others not at all. The bootstrap sample is 
the same size as the original, and lots and lots of these samples are taken: B of them, where B is at least 
50, and could even be in the thousands. The name bootstrap  is more popular in computer science than 
anywhere else, since there is also a bootstrap loader, which is the first program to run when a computer 
is turned on. It comes from the nonsensical idea of ‘picking yourself up by your bootstraps,’ which 
means lif ting yourself up by your shoelaces, and is meant to imply starting from nothing.  
Bootstrap sampling seems like a very strange thing to do. We’ve taken a perfectly good dataset, 
mucked it up by sampling from it, which might be good if we had made a smaller dataset (since it would 
be faster), but we still ended up with a dataset the same size. Worse, we’ve done it lots of times. Surely 
this is just a way to burn up computer time without gaining anything. The benefit of it is that we will get 
lots of learners that perform slightly differently, which is exactly what we want for an ensemble 
method. Another benefit is that estimates of the accuracy of the classification function can be made 
without complicated analytic work, by throwing computer resources at the p roblem (technically, 
bagging is a variance reducing algorithm; the meaning of this will become clearer when we talk about 
bias and variance). Having taken a set of bootstrap samples, the bagging method simply requires that 
we fit a model to each dataset, a nd then combine them by taking the output to be the majority vote of 
all the classifiers. A NumPy implementation is shown next, and then we will look at a simple example.  
 
# Compute bootstrap samples  
samplePoints = np.random .randint( 0,nPoints ,(nPoints ,nSamples )) 
classifiers = [] 
 
for i in range( nSamples ): 
sample = [] 
sampleTarget = [] 
for j in range( nPoints ): 
sample .append( data [samplePoints [j,i]]) 

67 
 sampleTarget .append( targets [samplePoints [j,i]]) 
# Train classifiers  
classifiers .append( self.tree.make_tree( sample ,sampleTarget ,features )) 
 
The example consists of taking the party data that was used to demonstrate the decision tree, and 
restricting the trees to stumps, so that they can make a classification based on just one variable  
 
When we want to construct the decision tree to decide what to do in the evening, we start by listing 
everything that we’ve done for the past few days to get a suitable dataset (here, the last ten days):  
 
 
The output of a decision tree that uses the whole dataset for this is not su rprising: it takes the two 
largest classes, and separates them. However, using just stumps of trees and 20 samples, bagging can 
separate the data perfectly, as this output shows:  
 
 
 
3.4.1 RANDOM FORESTS  
A random forest is an ensemble learning method where  multiple decision trees are constructed 
and then they are merged to get a more accurate prediction.  
If there is one method in machine learning that has grown in popularity over the last few years, 
then it is the idea of random forests. The concept has bee n around for longer than that, with several 
different people inventing variations, but the name that is most strongly attached to it is that of 
Breiman, who also described the CART algorithm in unit 2.  
 
Figure 3: Example of random forest with majority voting  

68 
  
 
The idea is largely that if one tree is good, then many trees (a forest) should be better, provided 
that there is enough variety between them. The most interesting thing about a random forest is the 
ways that it creates randomness from a standard  dataset. The first of the methods that it uses is the 
one that we have just seen: bagging. If we wish to create a forest then we can make the trees different 
by training them on slightly different data, so we take bootstrap samples from the dataset for ea ch tree. 
However, this isn’t enough randomness yet. The other obvious place where it is possible to add 
randomness is to limit the choices that the decision tree can make. At each node, a random subset of 
the features is given to the tree, and it can only pick from that subset rather than from the whole set.  
As well as increasing the randomness in the training of each tree, it also speeds up the training, 
since there are fewer features to search over at each stage. Of course, it does introduce a new 
paramet er (how many features to consider), but the random forest does not seem to be very sensitive 
to this parameter; in practice, a subset size that is the square root of the number of features seems to 
be common. The effect of these two forms of randomness is to reduce the variance without effecting 
the bias. Another benefit of this is that there is no need to prune the trees. There is another parameter 
that we don’t know how to choose yet, which is the number of trees to put into the forest. However, 
this is f airly easy to pick if we want optimal results: we can keep on building trees until the error stops 
decreasing.  
Once the set of trees are trained, the output of the forest is the majority vote for classification, 
as with the other committee methods that we  have seen, or the mean response for regression. And 
those are pretty much the main features needed for creating a random forest. The algorithm is given 
next before we see some results of using the random forest.  
 
Algorithm  
Here is an outline of the random  forest algorithm.  
1. The random forests algorithm generates many classification trees. Each tree is generated as 
follows:  
a) If the number of examples in the training set is N, take a sample of N examples at 
random - but with replacement, from the original data . This sample will be the training 
set for generating the tree.  
b) If there are M input variables, a number m is specified such that at each node, m 
variables are selected at random out of the M and the best split on these m is used to 

69 
 split the node. The val ue of m is held constant during the generation of the various 
trees in the forest.  
c) Each tree is grown to the largest extent possible.  
2. To classify a new object from an input vector, put the input vector down each of the trees in the 
forest. Each tree gives a classification, and we say the tree “votes” for that class. The forest 
chooses the classification  
 
The implementation of this is very easy: we modify the decision to take an extra parameter, which is m, 
the number of features that should be used in the selection set at each stage. We will look at an 
example of using it shortly as a comparison to boosting.  
 
Looking at the algorithm you might be able to see that it is a very unusual machine learning 
method because it is embarrassingly parallel: since the t rees do not depend upon each other, you can 
both create and get decisions from different trees on different individual processors if you have them. 
This means that the random forest can run on as many processors as you have available with nearly 
linear spe edup.  
 
There is one more nice thing to mention about random forests, which is that with a little bit of 
programming effort they come with built -in test data: the bootstrap sample will miss out about 35% of 
the data on average, the so -called out -of-bootstra p examples. If we keep track of these datapoints then 
they can be used as novel samples for that particular tree, giving an estimated test error that we get 
without having to use any extra datapoints.  
 
This avoids the need for cross -validation.  
 
As a brief  example of using the random forest, we start by demonstrating that the random 
forest gets the correct results on the Party example that has been used in both this and the previous 
chapters, based on 10 trees, each trained on 7 samples, and with just two l evels allowed in each tree:  
 
 
 
As a rather more involved example, the car evaluation dataset in the UCI Repository contains 1,728 
examples aiming to classify whether or not a car is a good purchase based on six attributes. The 
following results compare a single decision tree, bagging, and a random forest with 50 trees, each based 
on 100 samples, and with a maximum depth of five for each tree. It can be seen that the random forest 
is the most accurate of the three methods.  
 

70 
  
 
 
Strengths and weaknesses  
 
Strengths  
The following are some of the important strengths of random forests.  
• It runs efficiently on large data bases.  
• It can handle thousands of input variables without variable deletion.  
• It gives estimates of what variables are important in the classifi cation.  
• It has an effective method for estimating missing data and maintains accuracy when a large 
proportion of the data are missing.  
• Generated forests can be saved for future use on other data.  
• Prototypes are computed that give information about the rela tion between the variables and the 
classification.  
• The capabilities of the above can be extended to unlabeled data, leading to unsupervised 
clustering, data views and outlier detection.  
• It offers an experimental method for detecting variable interactions.  
• Random forest run times are quite fast, and they are able to deal with unbalanced and missing 
data.  
• They can handle binary features, categorical features, numerical features without any need for 
scaling.  
Weaknesses  
• A weakness of random forest algorithms is  that when used for regression they cannot predict 
beyond the range in the training data, and that they may over -fit data sets that are particularly 
noisy.  

71 
 • The sizes of the models created by random forests may be very large. It may take hundreds of 
megabyt es of memory and may be slow to evaluate.  
• Random forest models are black boxes that are very hard to interpret.  
3.5 Boosting  
 Boosting: train next learner on mistakes made by previous learner(s)  
 
In bagging, generating complementary base -learners is left to  chance and to the unstability of the 
learning method. In boosting, we actively try to generate complementary base -learners by training the 
next learner on the mistakes of the previous learners. The original boosting algorithm combines three 
weak learners to generate a strong learner. A weak learner has error probability less than 1/2, which 
makes it better than random guessing on a two -class problem, and a strong learner has arbitrarily small 
error probability.  
 
Original Boosting Concept  
Given a large trai ning set, we randomly divide it into three. We use X1 and train d1. We then take X2 
and feed it to d1. We take all instances misclassified by d1 and also as many instances on which d1 is 
correct  
from X2, and these together form the training set of d2. We then take X3 and feed it to d1 and d2. The 
instances on which d1 and d2 disagree form the training set of d3. During testing, given an instance, we 
give it to d1 and d2; if they agree, that is the response, otherwise the response of d3 is taken as the 
output.  
 
1. Split data X into {X 1, X2, X3} 
2. Train d1 on X 1 
 Test d 1 on X 2 
3. Train d 2 on d 1’s mistakes on X 2 (plus some right)  
 Test d 1 and d 2 on X 3 
4. Train d 3 on disagreements between d 1 and d 2 
 Testing: apply d 1 and d 2; if disagree, use d 3 
 Drawback: need large X  
 
overall system has reduced error rate, and the error rate can arbitrarily be reduced by using such 
systems recursively, that is, a boosting system of three models used as dj in a higher system.  
 
Though it is quite successful, the disadvantage of the origin al boosting method is that it requires a very 
large training sample. The sample should be divided into three and furthermore, the second and third 
classifiers are only trained on a subset on which the previous ones err. So unless one has a quite large 
training set, d2 and d3 will not have training  
sets of reasonable size.  
 
3.5.1AdaBoost  
         Freund and Schapire (1996) proposed a variant, named AdaBoost , short for adaptive boosting, 
that uses the same training set over and over and thus need not be large , but the classifiers should be 
simple so that they do not overfit. AdaBoost can also combine an arbitrary number of baselearners, not 
three.  
 
AdaBoost algorithm  
72 
  
 
The idea is to modify the probabilities of drawing the instances as a function of the error . Let us say pt
j 
denotes the probability that the instance pair (xt, rt) is drawn to train the jth base -learner.  
Initially, all pt
1 = 1/N. Then we add new base -learners as follows, starting from j = 1:  Є j denotes the 
error rate of dj .  
AdaBoost  requires that learners are weak, that is,  Є j < 1/2,∀j;  if  not, we stop adding new base -
learners. Note that this error rate is not on the original problem but on the dataset used at step j. We 
define  βj = Є j /(1 −Є j) < 1, and we set pt
j+1 = βj pt
j if dj correctly classifies xt ; otherwise, pt
j +1 = pt
j.   
Because pt
j+1  should be probabilities, there is a normalization where we divide pt
j+1 by  𝑡 pt
j+1 , so that 
they sum up to 1. This has the effect that the probability of a correctly classified ins tance is decreased, 
and the probability of a misclassified instance increases. Then a new sample of the same size is drawn 
from the original sample according to these modified probabilities, pt
j+1 with replacement, and is used to 
train d j+1. 
  
This has the  effect that dj+1 focuses more on instances misclassified by dj ; that is why the base -learners 
are chosen to be simple and not accurate, since otherwise the next training sample would contain only 
a few outlier and noisy instances repeated many times over. For example, with decision trees, decision 
stumps , which are t rees grown only one or two levels, are used. So it is clear that these would have bias 
but the decrease in variance is larger and the overall error decreases. An algorithm like the linear 
discriminant has low variance, and we cannot gain by AdaBoosting lin ear discriminants.  
. 
3.5.2 Stacking  - Stacked Generalization  
Stacked generalization is a technique proposed by Wolpert (1992) that extends voting in that 
the way the output of the base -learners is combined need not be linear but is learned through a 
combin er system, f (·|Φ), which is another learner, whose parameters Φ are also trained. (see the 
below given figure)  
 

73 
  
Figure: In stacked generalization, the combiner is another learner and is not restricted to being a linear 
combination as in voting.  
 
y = f (d1, d2, . . . , d L |Φ) 
 
The combiner learns what the correct output is when the base -learners give a certain output 
combination. We cannot train the combiner function on the training data because the base -learners 
may be memorizing the training set; the combiner system should act ually learn how the baselearners 
make errors. Stacking is a means of estimating and correcting for the biases of the base -learners. 
Therefore, the combiner should be trained on data unused in training the base -learners.  
 
If f (·|w1, . . . , wL) is a linear  model with constraints, wi ≥ 0,  𝑗Wj = 1, the optimal weights can be found 
by constrained regression, but of course we do not need to enforce this; in stacking, there is no 
restriction on the combiner function and unlike voting, f (·) can be nonlinear. Fo r example, it may be 
implemented as a multilayer perceptron with Φ its connection weights.  
 
The outputs of the base -learners dj define a new L-dimensional space in which the output 
discriminant/regression function is learned by the combiner function.  
 
In stacked generalization, we would like the base -learners to be as different as possible so that they will 
complement each other, and, for this, it is best if they are based on different learning algorithms. If we 
are combining classifiers that can generate continuous outputs, for example, posterior probabilities, it is 
better that they be the combined rather than hard decisions.  
When we compare a trained combiner as we have in stacking, with a fixed rule such as in 
voting, we see that both have their advanta ges: A trained rule is more flexible and may have less bias, 
but adds extra parameters, risks introducing variance, and needs extra time and data for training. Note 
also that there is no need to normalize classifier outputs before stacking.  
 
3.6 Probabilis tic Learning  
In machine learning , a probabilistic classifier  is a classifier  that is able to predict, given an 
observation of an input, a  probability distribution  over a  set of classes, rather than only outputting the 
most likely class that the observation should belong to. Probabilistic classifiers provide classification 
that can be useful in its own right  or when combining classifiers into  ensembles . 
 
One criticism that is often made of neural networks —especially the MLP —is that it is not clear exactly 
what it is doing: while we can go and have a look at th e activations of the neurons and the weights, they 
don’t tell us much.  

74 
 In this topic ( probabilistic classifier  ) we are going to look at methods that are based on statistics, and 
that are therefore more transparent, in that we can always extract and look a t the probabilities and see 
what they are, rather than having to worry about weights that have no obvious meaning.  
 
 
3.7 GAUSSIAN MIXTURE MODELS  
However, suppose that we have the same data, but without target labels. This requires 
unsupervised learning,  Suppose that the different classes each come from their own Gaussian 
distribution. This is known as multi -modal data, since there is one distribution (mode) for each different 
class. We can’t fit one Gaussian to the data, because it doesn’t look Gaussian o verall.  
 
There is, however, something we can do. If we know how many classes there are in the data, 
then we can try to estimate the parameters for that many Gaussians, all at once. If we don’t know, then 
we can try different numbers and see which one works  best. We will talk about this issue more for a 
different method (the k-means algorithm) in Unit 2. It is perfectly possible to use any other probability 
distribution instead of a Gaussian, but Gaussians are by far the most common choice. Then the output 
for any particular datapoint that is input to the algorithm will be the sum of the values expected by all 
of the M Gaussians:  
 
 
where _(x ; μm,  𝑚) is a Gaussian function with mean μm and covariance matrix  𝑚, and the  αm are 
weights with the constraint that  𝛼𝑚 𝑀
𝑚=1  =1. 
 
The given figures 4 shows two examples, where the data (shown by the histograms) comes from two 
different Gaussians, and the model is computed as a sum or mixture of the two Gaussians together.  
 
 
FIGU RE 4: Histograms of training data from a mixture of two Gaussians and two fitted models, shown as 
the line plot. The model shown on the left fits well, but the one on the right produces two Gaussians 
right on top of each other that do not fit the data well . 
 
 
The figure also gives you some idea of how to use the mixture model once it has been created. The 
probability that input xi belongs to class m can be written as (where a hat on a variable (ˆ·) means that 
we are estimating the value of that variable):  
 
The problem is how to choose the weights αm. The common approach is to aim for the maximum 

75 
 likelihood solution (the likelihood is the conditional probability of the data given the model, and the 
maximum likelihood solution varies the model to maximise thi s conditional probability). In fact, it is 
common to compute the log likelihood and then to maximise that; it is guaranteed to be negative, since 
probabilities are all less than 1, and the logarithm spreads out the values, making the optimisation more 
effective. The algorithm that is used is an example of a very general one known as the expectation -
maximisation (or more compactly, EM) algorithm.  
 
3.8 The Expectation -Maximisation (EM) Algorithm  
The basic idea of the EM algorithm is that sometimes it is easie r to add extra variables that are 
not actually known (called hidden or latent variables) and then to maximise the function over those 
variables. This might seem to be making a problem much more complicated than it needs to be, but it 
turns out for many pro blems that it makes finding the solution significantly easier.  
In order to see how it works, we will consider the simplest interesting case of the Gaussian 
mixture model: a combination of just two Gaussian mixtures. The assumption now is that sample from 
that Gaussian. If the probability of picking Gaussian one is p, then the entire model looks like this 
(where N( μ, σ2) specifies a Gaussian distribution with mean μ and standard deviation σ): 
 
 
 
If the probability distribution of p is written as π, then the  probability density is:  
 
 
 
Finding the maximum likelihood solution (actually the maximum log likelihood) to this problem 
is then a case of computing the sum of the logarithm of Equation  over all of the training data, and 
differentiating it, which would be rather difficult. Fortunately, there is a way around it. The key insight 
that we need is that if we knew which of the two Gaussian components the datapoint came from, then 
the computation would be easy. The mean and standard deviation for each component  could be 
computed from the datapoints that belong to that component, and there would not be a problem. 
Although we don’t know which component each datapoint came from, we can pretend we do, by 
introducing a new variable f. If f = 0 then the data came from  Gaussian one, if f = 1 then it came from  
Gaussian two.  
 
This is the typical initial step of an EM algorithm: adding latent variables. Now we just need to 
work out how to optimise over them. This is the time when the reason for the algorithm being called 
expectation -maximisation becomes clear.We don’t know much about variable f (hardly surprising, since 
we invented it), but we can compute its expectation (that is, the value that we ‘expect’ to see, which is 
the mean average) from the data:  
 
 
 
where D deno tes the data. Note that since we have set f = 1 this means that we are choosing Gaussian 
two.  

76 
  
Computing the value of this expectation is known as the E -step. Then this estimate of the 
expectation is maximised over the model parameters (the parameters of t he two Gaussians and the 
mixing parameter π), the M -step. This requires differentiating the expectation with respect to each of 
the model parameters. These two steps are simply iterated until the algorithm converges. Note that the 
estimate never gets any s maller, and it turns out that EM algorithms are guaranteed to reach a local 
maxima. To see how this looks for the two -component Gaussian mixture, we’ll take a closer look at the 
algorithm:  
 
 
The trick with applying EM algorithms to problems is in identify ing the correct latent variables 
to include, and then simply working through the steps. They are very powerful methods for a wide 
variety of statistical learning problems. We are now going to turn our attention to something much 
simpler, which is how we ca n use information about nearby datapoints to decide on classification 
output. For this we don’t use a model of the data at all, but directly use the data that is available.  
 
3.9 Information Criteria  
we introduced the idea of a validation set, or using cross -validation if there was not enough 
data. However, this replaces data with computation time, as many models are trained on different 
datasets.  
 
An alternative idea is to identify some measure that tel ls us about how well we can expect this 
trained model to perform. Probabilistic model selection (or “ information criteria ”) provides an 
analytical technique for scoring and choosing among candidate models. Models are scored both on 
their performance on the  training dataset and based on the complexity of the model.There are two 
such information criteria that are commonly used:  
 
 
 
In these equations, k is the number of parameters in the model, N is the number of training 
examples, and L is the best (largest)  likelihood of the model. In both cases, based on the way that they 
are written here, the model with the largest value is taken.  
 
3.10 Nearest neighbour methods  

77 
 Suppose that you are in a nightclub and decide to dance. It is unlikely that you will know the 
dance moves for the particular song that is playing, so you will probably try to work out what to do by 
looking at what the people close to you are doing. The first thing you could do would be just to pick the 
person closest to you and copy them. However, since most of the people who are in the nightclub are 
also unlikely to know all the moves, you might decide to look at a few more people and do what most of 
them are doing. This is pretty much exactly the idea behind nearest neighbour methods: if we don’t 
have a model that describes the data, then the best thing to do is to look at similar data and choose to 
be in the same class as them.  
 
We have the datapoints  positioned within input space, so we just need to work out which of the 
training data are close to it. This requires computing the distance to each datapoint in the training set, 
which is relatively expensive: if we are in normal Euclidean space, then we have to compute d 
subtractions and d squarings (we can ignore the square root since we only want to know which points 
are the closest, not the actual distance) and this has to be done O( N2) times. We can then identify the k 
nearest neighbours  to the test point, and then set the class of the test point to be the most common 
one out of those for the nearest neighbours. The choice of k is not trivial. Make it too small and nearest 
neighbour methods are sensitive to noise, too large and the accura cy reduces as points that are too far 
away are considered. Some possible effects of changing the size of k on the decision boundary are 
shown in below Figure 5.  
 
 
FIGURE 5:  The nearest neighbours decision boundary with left: one neighbour and right:  
two n eighbours . 
 
This method suffers from the curse of dimensionality. First, as shown above, the computational 
costs get higher as the number of dimensions grows. This is not as bad as it might appear at first: there 
are sets of methods such as KD -Trees (will discuss in u pcoming topics) that compute this in O( N log N) 
time. However, more importantly, as the number of dimensions increases, so the distance to other 
datapoints tends to increase. In addition, they can be far away in a variety of different directions —there 
migh t be points that are relatively close in some dimensions, but a long way in others. There are 
methods for dealing with these problems, known as adaptive nearest neighbour methods, and there is a 
reference to them in the Further Reading section at the end o f the chapter.  
 
The only part of this that requires any care during the implementation is what to do when there 
is more than one class found in the closest points, but even with that the implementation is nice and 
simple:  
 

78 
  
 
We are going to look next at h ow we can use these methods for regression, before we turn to the 
question of how to perform the distance calculations as efficiently as possible, something that is done 
simply but inefficiently in the code above. We will then consider briefly whether or n ot the Euclidean 
distance is always the most useful way to calculate distances, and what alternatives there are.  
 
For the k-nearest neighbours algorithm the bias -variance decomposition can be computed as:  
 
 
The way to interpret this is that when k is smal l, so that there are few neighbours considered, the model 
has flexibility and can represent the underlying model well, but that it makes mistakes (has high 
variance) because there is relatively little data. As k increases, the variance decreases, but at th e cost of 
less flexibility and so more bias.  
 
3.11 Nearest Neighbour Smoothing  
Nearest neighbour methods can also be used for regression by returning the average value of the 
neighbours to a point, or a spline or similar fit as the new value. The most comm on methods are known 
as kernel smoothers, and they use a kernel (a weighting function between pairs of points) that decides 
how much emphasis (weight) to put onto the contribution from each datapoint according to its distance 
from the input.  
 
Here we shall simply use two kernels that are used for smoothing. Both of these kernels are 
designed to give more weight to points that are closer to the current input, with the weights decreasing 
smoothly to zero as they pass out of the range of the current input , with the range specified by a 
parameter λ.  
 
They are the Epanechnikov quadratic kernel:  
 

79 
 and the tricube kernel:  
  
The results of using these kernels are shown in below Figure 6 on a dataset that consists of the time 
between eruptions (technically known as the repose) and the duration of the eruptions of Mount 
Ruapehu, the large volcano in the centre of New Zealand’s n orth island. Values of λ  of 2 and 4 were 
used here. Picking λ  requires experimentation. Large values average over more datapoints, and 
therefore produce lower variance, but at the cost of higher bias.  
 
 

80 
  
FIGURE 6:  Output of the nearest neighbour method and two kernel smoothers on the data of duration 
and repose of eruptions of Mount Ruapehu 1860 –2006.  
 
3.12 Efficient Distance Computations: the KD -Tree  
 
As was mentioned above, computing the distances between all pairs of points is very 
computationally exp ensive. Fortunately, as with many problems in computer science, designing an 
efficient data structure can reduce the computational overhead a lot. For the problem of finding 
nearest neighbours the data structure of choice is the KD -Tree. It has been around  since the late 1970s, 
when it was devised by Friedman and Bentley, and it reduces the cost of finding a nearest neighbour to 
O(log N) for O( N) storage. The construction of the tree is O( N log2 N), with much of the computational 
cost being in the computation of the median, which with a naïve algorithm requires a sort and is 
therefore O( N log N), or can be computed with a randomised algorithm in O( N) time.  
 
The idea behind the KD -tree is very simple. You create a binary tree by choosing one dimensio n 
at a time to split into two, and placing the line through the median of the point coordinates of that 
dimension. The points themselves end up as leaves of the tree. Making the tree follows pretty much the 
same steps as usual for constructing a binary tre e: we identify a place to split into two choices, left and 
right, and then carry on down the tree. This makes it natural to write the algorithm recursively. The 
choice of what to split and where is what makes the KD -tree special. Just one dimension is spli t in each 
step, and the position of the split is found by computing the median of the points that are to be split in 
that one dimension, and putting the line there. In general, the choice of which dimension to split 

81 
 alternates through the different choices , or it can be made randomly. The algorithm below cycles 
through the possible dimensions based on the depth of the tree so far, so that in two dimensions it 
alternates horizontal and vertical splits.  
The centre of the construction method is simply a recur sive function that picks the axis to split 
on, finds the median value on that axis, and separates the points according to that value, which in 
Python can be written as:  
 
 
 
Suppose that we had seven two -dimensional points to make a tree from: (5 , 4), (1, 6), (6, 1), (7, 5), (2, 
7), (2, 2), (5, 8) (as plotted in Figure 7).  
 
FIGURE 7:  The initial set of 2D data.  
 
The algorithm will pick the first coordinate to split on initially, and the median point here is 5, so the 
split is through x = 5. Of those on th e left of the line, the median y coordinate is 6, and for those on the 
right it is 5. At this point we have separated all the points, and so the algorithm terminates with the split 
shown in Figure 8 and the tree shown in Figure 9.  

82 
  
FIGURE 8:  The splits and leaf points found by  the KD -tree. 
 
 
FIGURE 9:  The KD -tree that made the splits.  
 
Searching the tree is the same as any other binary tree; we are more interested in finding the nearest 
neighbours of a test point. This is fairly easy: starti ng at the root of the tree you recurse down through 
the tree comparing just one dimension at a time until you find a leaf node that is in the region 
containing the test point. Using the tree shown in Figure 9 we introduce the test point (3 , 5), which 
finds  (2, 2) as the leaf for the box that (3 , 5) is in. However, looking at Figure 10 we see that this is not 
the closest point at all, so we need to do some more work.  
 
 
FIGURE 10 Two test points for the example KD -tree. 
 
 

83 
 3.13 Distance Measures  
 
We have computed the distance between points as the Euclidean distance, which is something 
that you learnt about in high school. However, it is not the only option, nor is it necessarily the most 
useful. In this section we will look at the underlying idea behind d istance calculations and possible 
alternatives.  
 
If I were to ask you to find the distance between my house and the nearest shop, then your first 
guess might involve taking a map of my town, locating my house and the shop, and using a ruler to 
measure the distance between them. By careful application of the map scale you can now tell me how 
far it is. However, when I set out to buy some milk I’m liable to find that I have to walk rather further 
than you’ve told me, since the direct line that you measured wo uld involve walking through (or over) 
several houses, and some serious fence -scaling. Your ‘as the crow flies’ distance is the shortest possible 
path, and it is the straight -line, or Euclidean, distance. You can measure it on the map by just using a 
ruler,  but it essentially consists of measuring the distance in one direction (we’ll call it  
north -south) and then the distance in another direction that is perpendicular to the first (let’s call it 
east -west) and then squaring them, adding them together, and th en taking the square root of that. 
Writing that out, the Euclidean distance that we are all used to is:  
 
 
where ( x1, y1) is the location of my house in some coordinate system (say by using a GPS tracker) and 
(x2, y2) is the location of the shop.  
 
If I told you that my town was laid out on a grid block system, as is common in towns that were 
built in the interval between the invention of the motor car and the invention of innovative town 
planners, then you would probably use a different measure. You woul d measure the distance between 
my house and the shop in the ‘north -south’ direction and the distance in the ‘east -west’ direction, and 
then add the two distances together. This would correspond to the distance I actually had to walk. It is 
often known as t he city -block or Manhattan distance and looks like:  
 
The point of this discussion is to show that there is more than one way to measure a distance, 
and that they can provide radically different answers. These two different distances can be seen in 
Figure 11. Mathematically, these distance measures are known as metrics. A metric function or norm 
takes two inputs and gives a scalar (the distance) back, which is positive, and 0 if and only if the two 
points are the same, symmetric (so that the distance  
to the  shop is the same as the distance back), and obeys the triangle inequality, which says that the 
distance from a to b plus the distance from b to c should not be less than the direct distance from a to c. 
 

84 
  
FIGURE 10:  The Euclidean and city -block distances  between two points.  
 
Most of the data that we are going to have to analyse lives in rather more than two dimensions. 
Fortunately, the Euclidean distance that we know about generalises very well to higher dimensions (and 
so does the city -block metric). In fact, these two measures are both instances of a class of metrics that 
work in any number of dimensions. The general measure is the Minkowski metric and it is written as:  
 
 
If we put k = 1 then we get the city -block distance (Equation (7.12)), and k = 2 gives the 
Euclidean distance (Equation (7.11)). Thus, you might possibly see the Euclidean metric written as the L2 
norm and the city -block distance as the L1 norm. These norms have another interesting feature. 
Remember that we can define different ave rages of a set of numbers. If we define the average as the 
point that minimises the sum of the distance to every datapoint, then it turns out that the mean 
minimises the Euclidean distance (the sum -of-squares distance), and the median minimises the L1 
metr ic. 
 
There are plenty of other possible metrics to choose, depending upon the dataspace. We 
generally assume that the space is flat (if it isn’t, then none of these techniques work, and we don’t 
want to worry about that). However, it can still be beneficia l to look at other metrics. Suppose that we 
want our classifier to be able to recognise images, for example of faces. We take a set of digital photos 
of faces and use the pixel values as features. Then we use the nearest neighbour algorithm that we’ve 
just seen to identify each face. Even if we ensure that all of the photos are taken fully face -on, there are 
still a few things that will get in the way of this method. One is that slight variations in the angle of the 
head (or the camera) could make a differe nce; another is that different distances between the face and 
the camera (scaling) will change the results; and another is that different lighting conditions will make a 
difference. We can try to fix all of these things in preprocessing, but there is also another alternative: 
use a different metric that is invariant to these changes, i.e., it does not vary as they do. The idea of 
invariant metrics is to find measures that ignore changes that you don’t want. So if you want to be able 
to rotate shapes around and still recognize them, you need a metric that is invariant to rotation.  
 
A common invariant metric in use for images is the tangent distance, which is an approximation 
to the Taylor expansion in first derivatives, and works very well for small rotations  and scalings; for 
example, it was used to halve the final error rate on nearest neighbor classification of a set of 
handwritten letters. Invariant metrics are an interesting topic for further study, and there is a reference 
for them in the Further Reading  section if you are interested.  
 

85 
 Unit IV  
Reinforcement Learning  and Evaluating Hypotheses  
 
Introduction, Learning Task, Q Learning, Non deterministic Rewards and actions, temporal -
difference learning, Relationship to Dynamic Programming, Active reinforcement learning, 
Generalization in reinforcement learning.  
Motivation, Basics of Sampling Theory: Error Estimation and Estimating Binomial Proportions, 
The Binomial Distribution, Estimators, Bias, and Variance    
 
Reinforcement learning addresses the question of how an autonomous agent that senses and acts in its 
environment can learn to choose optimal actions to achieve its goals.  
 
4.1. Introduction  
 Consider building a learning robot . The robot, or agent , has a set  of sensors to observe the state of 
its environment, and a set of actions it can perform to alter this state.  
 Its task is to learn a control strategy, or policy , for choosing actions that achieve its goals.  
 The goals of the agent can be defined by a rewa rd function that assigns a numerical value to each 
distinct action the agent may take from each distinct state.  
 This reward function may be built into the robot, or known only to an external teacher who 
provides the reward value for each action performed by the robot.  
 The task of the robot is to perform sequences of actions, observe their consequences, and learn a 
control policy.  
 The control policy is one that, from any initial state, chooses actions that maximize the reward 
accumulated over time by the agent.  
Example:  
 A mobile robot may have sensors such as a camera and sonars, and actions such as "move 
forward" and "turn."  
 The robot may have a goal of docking onto its battery charger whenever its battery level is low.  
 The goal of docking to the batt ery charger can be captured by assigning a positive reward (Eg., 
+100) to state -action transitions that immediately result in a connection to the charger and a 
reward of zero to every other state -action transition.  
 
Reinforcement Learning Problem  
 An agent interacting with its environment. The agent exists in an environment described by some 
set of possible states S.  
 Agent perform any of a set of possible actions A. Each time it performs an action a, in some state st 
the agent receives a real -value d reward r, that indicates the immediate value of this state -action 
transition. This produces a sequence of states si, actions ai, and immediate rewards ri as shown in 
the figure.  
 The agent's task is to learn a control policy, 𝝅: S → A , that  maximizes the expected sum of these 
rewards, with future rewards discounted exponentially by their delay.   
86 
  
 
 
Reinforcement learning problem characteristics  
 
1. Delayed reward : The task of the agent is to learn a target function 𝜋 that maps from the cur rent state 
s to the optimal action a = 𝜋 (s). In reinforcement learning, training information is not available in (s, 𝜋 
(s)). Instead, the trainer provides only a sequence of immediate reward values as the agent executes its 
sequence of actions. The agen t, therefore, faces the problem of temporal credit assignment : 
determining which of the actions in its sequence are to be credited with producing the eventual 
rewards.  
 
2. Exploration: In reinforcement learning, the agent influences the distribution of training examples by 
the action sequence it chooses. This raises the question of which experimentation strategy produces 
most effective learning. The learner faces a trade -off in choosing whether to favor exploration of 
unknown states and actions, or exploitation of states and actions that it has already learned will yield 
high reward.  
 
3. Partially observable states: The agent's sensors can perceive the entire state of the environment at 
each time step, in many practical situations sensors provide only partial information. In such cases, the 
agent needs to consider its previous observations together with its current sensor data when choosing 
actions, and the best policy may be one that cho oses actions specifically to improve the observability of 
the environment.  
 
4. Life -long learning: Robot requires to learn several related tasks within the same environment, using 
the same sensors. For example, a mobile robot may need to learn how to dock  on its battery charger, 
how to navigate through narrow corridors, and how to pick up output from laser printers. This setting 
raises the possibility of using previously obtained experience or knowledge to reduce sample complexity 
when learning new tasks.  
 
4.2. Learning Task  
 Consider Markov decision process (MDP) where the agent can perceive a set S of distinct states of 
its environment and has a set A of actions that it can perform.  
 At each discrete time step t, the agent senses the current state st , chooses a current action at, and 
performs it.  

87 
  The environment responds by giving the agent a reward rt = r(st, at) and by producing the 
succeeding state st+l = δ(st, at). Here the functions δ(st, at) and r(st, at) depend only on the current 
state and ac tion, and not on earlier states or actions.  
 
The task of the agent is to learn a policy, 𝝅: S → A , for selecting its next action a, based on the current 
observed state st; that is, (st) = at .  
 
How shall we specify precisely which policy π we would like the agent to learn?  
 
1. One approach is to require the policy that produces the greatest possible cumulative reward for the 
robot over time.  
 To state this requirement more precisely, define the cumulative value Vπ (st ) achieved by following 
an arbitrary policy π from an arbitrary initial state st as follows:  
 
 
 Where, the sequence of rewards rt+i is generated by beginning at state st and by repeatedly using 
the policy π to select actions.  
 Here 0 ≤ γ ≤ 1 is a constant that determines the relative value of delayed versus immediate 
rewards. if we set γ = 0, only the immediate reward is considered. As we set γ closer to 1, future 
rewards are given greater emphasis relative to the immediate rewa rd.  
 The quantity Vπ (st) is called the discounted cumulative reward achieved by policy π from initial 
state s. It is reasonable to discount future rewards relative to immediate rewards because, in many 
cases, we prefer to obtain the reward sooner rather t han later.  
 
2. Other definitions of total reward is finite horizon reward,  
 
 
Considers the undiscounted sum of rewards over a finite number h of steps  
 
3. Another approach is average reward  
 
 
Considers the average reward per time step over the entire  lifetime of the agent.  
 
We require that the agent learn a policy π that maximizes Vπ (st) for all states s. such a policy is called 
an optimal policy and denote it by π*  

88 
  
 
Refer the value function Vπ *(s) an optimal policy as V*(s). V*(s) gives the maximum discounted 
cumulative reward that the agent can obtain starting from state s. 
 
Example:  
A simple grid -world environment is depicted in the diagram  
 
 The six grid squares in this diagram represent six possible states, or locations, for the agent.  
 Each arrow in the diagram represents a possible action the agent can take to move from one state 
to another.  
 The number associated with each arrow represents  the immediate reward r(s, a) the agent receives 
if it executes the corresponding state -action transition  
 The immediate reward in this environment is defined to be zero for all state -action transitions 
except for those leading into the state labelled G. T he state G as the goal state, and the agent can 
receive reward by entering this state.  
 
Once the states, actions, and immediate rewards are defined, choose a value for the discount factor γ, 
determine the optimal policy π * and its value function V*(s).  
 
Let’s choose γ = 0.9. The diagram at the bottom of the figure shows one optimal policy for this setting.  
 

89 
  
 
Values of V*(s) and Q(s, a) follow from r(s, a), and the discount factor γ = 0.9. An optimal policy, 
corresponding to actions with maximal Q values, is also shown.  
 
The discounted future reward from the bottom centre state is  
0+ γ 100+ γ2 0+ γ3 0+... = 9 0 
4.1.3. Q LEARNING  
How can an agent learn an optimal policy π * for an arbitrary environment?  
The training information available to the learner is the sequence of immediate rewards r(si,ai) 
for i  = 0, 1,2, . . . . Given this kind of training information it is easier to learn a numerical evaluation 
function defined over states and actions, then implement the optimal policy in terms of this evaluation 
function.  
 
What evaluation function should the agent attempt to learn?  
One obvious choice is V*. The agent should prefer state sl over state s2 whenever V*(sl) > V*(s2), 
because the cumulative future reward will be greater from sl  
The optimal action in state s is the action a that maximizes the sum o f the immediate reward r(s, a) plus 
the value V* of the immediate successor state, discounted by γ.  
 
 
 
4.1.3.1. The Q Function  
The value of Evaluation function Q(s, a) is the reward received immediately upon executing 
action a from state s, plus the value (discounted by γ ) of following the optimal policy thereafter  
 
 

90 
  
Rewrite Equation (3) in terms of Q(s, a) as  
 
 
Equation (5) makes clear, it need only consider each available action a in its current state s and choose 
the action that maximizes Q(s, a) .  
 
4.1.3.2. An Algorithm for Learning Q  
 Learning the Q function corresponds to learning the optimal policy .  
 The key problem is finding a reliable way to estimate training values for Q, given only a sequence of 
immediate rewards r spread out over time. This can be accomplished through iterative 
approximation  
 
Rewriting Equation  
 
 
 
 
 
 
 
 
Q learning algorithm:  
 
 
 Q learning algorithm assuming deterministic rewards and actions. The discount factor γ may be any 
constant such that 0 ≤ γ < 1  

91 
  𝑄̂ to refer to the learner's estimate, or hypothesis, of the actual Q function  
 
4.1.3.2. An Illustrative Example  
 
 To illustrate the operation of the Q learning algorithm, consider a single action taken by an agent, 
and the corresponding refinement to 𝑄̂ shown in below figure  
 
 The agent moves one cell to the right in its grid world and receives an immediate reward of zero for 
this transition.  
 Apply the training rule of Equation  
 
to refine its estimate Q for the state -action transition it just executed.  
 
 According to the trai ning rule, the new 𝑄̂ estimate for this transition is the sum of the received 
reward (zero) and the highest 𝑄̂ value associated with the resulting state (100), discounted by γ (.9).   
 
 
4.1.3.3. Convergence  
Will the Q Learning Algorithm converge toward a Q equal to the true Q function?  
Yes, under certain conditions.  
1. Assume the system is a deterministic MDP.  
2. Assume the immediate reward values are bounded; that is, there exists some positive constant c such 
that for all states s and actions a, | r(s, a)| < c  
3. Assume the agent selects actions in such a fashion that it visits every possible state -action pair 
infinitely often  
 

92 
  
 
 
 
 
 
 
 

93 
  
 
 
4.1.3.4. Experimentation Strategies  
The Q learning algorithm does not specify how actions are chosen by the agent.  
 One obvious strategy would be for the agent in state s to select the action a that maximizes 𝑄̂(s, a), 
thereby exploiting its current approximation 𝑄̂.  
 However, with this strategy the agent runs the risk that it will overcommit to actions that are 
found during early training to have high Q values, while failing to explore other actions that have 
even higher values.  
 For this reason, Q learning uses a prob abilistic approach to selecting actions. Actions with higher 𝑄̂ 
values are assigned higher probabilities, but every action is assigned a nonzero probability.  
 One way to assign such probabilities is   
 
 
 Where, P(ai |s) is the probability of selecting action ai, given that the agent is in state s, and k > 0 is 
a constant that determines how strongly the selection favors actions with high 𝑄̂ values  
4.2. Evaluating Hypotheses  
4.2.1. Motivation  
It is important to evaluate the performance of learned hypotheses as precisely as possible.  
 One reason is simply to understand whether to use the hypothesis.  
 A second reason is that evaluating hypotheses is an integral component of many learning 
methods.  
 
Two key difficulties aris e while learning a hypothesis and estimating its future accuracy given only a 
limited set of data:  
 
1. Bias in the estimate . The observed accuracy of the learned hypothesis over the training examples is 
often a poor estimator of its accuracy over future e xamples. Because the learned hypothesis was 
derived from these examples, they will typically provide an optimistically biased estimate of hypothesis 
accuracy over future examples. This is especially likely when the learner considers a very rich hypothesis 
space, enabling it to overfit the training examples. To obtain an unbiased estimate of future accuracy, 
test the hypothesis on some set of test examples chosen independently of the training examples and 

94 
 the hypothesis.  
 
2. Variance in the estimate. Even i f the hypothesis accuracy is measured over an unbiased set of test 
examples independent of the training examples, the measured accuracy can still vary from the true 
accuracy, depending on the makeup of the particular set of test examples. The smaller the s et of test 
examples, the greater the expected variance.  
 
4.2.2. Estimating Hypothesis Accuracy  
 
Sample Error –  
The sample error of a hypothesis with respect to some sample S of instances drawn from X is the 
fraction of S that it misclassifies.  
 
Definition: The sample error ( errors(h) ) of hypothesis h with respect to target function f and data 
sample S is  
 
 
 
Where n is the number of examples in S, and the quantity δ(f(x), h(x)) is 1 if f (x) ≠ h(x), and 0 
otherwise.  
True Error –  
The true error of a hypothesis is the probability that it will misclassify a single randomly drawn 
instance from the distribution D.  
Definition: The true error (error D (h)) of hypothesis h with respect to target function f and 
distribution D, is the proba bility that h will misclassify an instance drawn at random according to D. 
 
 
Confidence Intervals for Discrete -Valued Hypotheses  
Suppose we wish to estimate the true error for some discrete valued hypothesis h, based on its 
observed sample error over a s ample S, where  
 The sample S contains n examples drawn independent of one another, and independent of h, 
according to the probability distribution D  
 n ≥ 30  
 Hypothesis h commits r errors over these n examples (i.e., errors (h) = r/n).  
 
Under these condit ions, statistical theory allows to make the following assertions:  
1. Given no other information, the most probable value of error D (h) is errors(h)  
2. With approximately 95% probability , the true error error D (h) lies in the interval  
 

95 
  
Example:  
Suppose the data sample S contains n = 40 examples and that hypothesis h commits r = 12 errors over 
this data.  
 The sample error is errors(h) = r/n = 12/40 = 0.30  
 Given no other information, true error is error D (h) = errors(h), i.e., error D (h) = 0.30  
 With the 95% confidence interval estimate for error D (h).  
 
 
 
= 0.30 ± (1.96 * 0.07)  
= 0.30 ± 0.14  
3. A different constant, ZN, is used to calculate the N% confidence interval . The general expression for 
approximate N% confidence intervals for error D (h) is  
 
 
 
Where,  
 
 
The above equation describes how to calculate the confidence intervals, or error bars, for estimates of 
error D (h) that are based on errors(h)  
 
Example:  
Suppose the data sample S contains n = 40 examples and that hypothesis h commits r = 12 errors over 
this data.  
 The sample error is errors(h) = r/n = 12/40 = 0.30  
 With the 68% confidence interval estimate for error D (h).  
 

96 
  
= 0.30 ± (1.00 * 0.07)  
= 0.30  ± 0.07  
 
4.2.3. Basics of Sampling Theory  
4.2.3.1. Error Estimation and Estimating Binomial Proportions  
 Collect a random sample S of n independently drawn instances from the distribution D, and then 
measure the sample error errors( h). Repeat this experiment many times, each time drawing a 
different random sample Si of size n, we would expect to observe different values for the various 
errorsi(h), depending on random differences in the makeup of the various Si. We say that errorsi(h) , 
the outcome of the ith such experiment, is a random variable .  
 Imagine that we were to run k random experiments, measuring the random variables errors1(h), 
errors2(h) . . . errorssk(h) and plotted a histogram displaying the frequency with which each 
poss ible error value is observed.  
 As k grows, the histogram would approach a particular probability distribution called the Binomial 
distribution which is shown in below figure.  
 
A Binomial distribution is defined by the probability function  
 
 
 
If the random variable X follows a Binomial distribution, then:  
 The probability Pr(X = r) that X will take on the value r is given by P(r)  
 

97 
  
 
4.2.3.2. The Binomial Distribution  
Consider the following problem for better understanding of Binomial Distribution  
 Given a worn and bent coin and estimate the probability that the coin will turn up heads when 
tossed.  
 Unknown probability of heads p. Toss the coin n times and record the nu mber of times r that it 
turns up heads.  
Estimate of p = r / n  
 If the experiment were rerun , generating a new set of n coin tosses, we might expect the 
number of heads r to vary somewhat from the value measured in the first experiment, yielding 
a somewhat different estimate for p.  
 The Binomial distribution describes for each possible value of r (i.e., from 0 to n), the probability 
of observing exactly r heads given a sample of n independent tosses of a coin whose true 
probability of heads is p.  
 
The general setting to which the Binomial distribution applies is:  
1. There is a base experiment (e.g., toss of the coin) whose outcome can be described by a random 
variable ‘Y’. The random variable Y can take on two possible values (e.g., Y = 1 if heads,  Y = 0 if tails).  
2. The probability that Y = 1 on any single trial of the base experiment is given by some constant p, 
independent of the outcome of any other experiment. The probability that Y = 0 is therefore (1 - p). 
Typically, p is not known in advan ce, and the problem is to estimate it.  
3. A series of n independent trials of the underlying experiment is performed (e.g., n independent coin 
tosses), producing the sequence of independent, identically distributed random variables Y1, Y2, . . . , 
Yn. Let  R denote the number of trials for which Yi = 1 in this series of n experiments  
 
4. The probability that the random variable R will take on a specific value r (e.g., the probability of 
observing exactly r heads) is given by the Binomial distribution  
 

98 
  
 
Mean, Variance and Standard Deviation  
The Mean (expected value) is the average of the values taken on by repeatedly sampling the random 
variable  
 
Definition: Consider a random variable Y that takes on the possible values y1, . . . yn. The expected 
value (Mean) of Y, E[Y], is  
 
The Variance captures how far the random variable is expected to vary from its mean value.  
Definition: The variance of a random variable Y, Var[Y], is  
 
The variance describes the expected squared error in using a single observat ion of Y to estimate its 
mean E[Y].  
 
The square root of the variance is called the standard deviation of Y, denoted σy  
 
Definition: The standard deviation of a random variable Y, σy, is  
 
In case the random variable Y is governed by a Binomial distribution , then the Mean, Variance and standard 
deviation are given by  
 
 
4.2.3.3. Estimators, Bias, and Variance  
Let us describe errors(h) and errorD(h) using the terms in Equation (1) defining the Binomial 
distribution. We then have  
 
Where,  
 n is the number of instances in the sample S,  

99 
  r is the number of instances from S misclassified by h  
 p is the probability of misclassifying a single instance drawn from D  
 
 Estimator:  
 
errors(h) an estimator for the true error errorD(h): An estimator is any random variable used to 
estimate some parameter of the underlying population from which the sample is drawn  
 Estimation bias: is the difference between the expected value of the estimator and the true value of 
the parameter.  
 
Definition: The estimation bias of an estimator Y for an arbitrary parameter p is  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIT V  
Genetic Algorithms  
 
Motivation, Genetic Algorithms: Representing Hypotheses, Genetic Operator, Fitness Function 
and Selection, An Illustrative Example, Hypothesis Space Search, Genetic Programming, Models 

100 
 of Evolution and Learning: Lamarkian Evolution, Baldwin Effect, Paralle lizing Genetic 
Algorithms.  
 
5.1. Motivation  
Genetic algorithms (GAS) provide a learning method motivated by an analogy to biological 
evolution. Rather than search from general -to-specific hypotheses, or from simple -to-complex, GAS 
generate successor hypotheses by repeatedly mutating and recombining parts of the best currently 
known hypotheses. At each step, a collection of hypotheses called the current population is updated by 
replacing some fraction of the population by offspring of the most fit current hypotheses. The process 
forms a generate -and-test beam -search of hypotheses, in which variants of the best current hypotheses 
are most likely to be considered next. The popularity of GAS is motivated by a number of factors 
including:  
 Evolution is known to be a successful, robust method for adaptation within biological systems.  
 GAS can search spaces of hypotheses containing complex interacting parts, where the impact of 
each part on overall hypothesis fitness may be difficult to model.  
 Genetic algorithms are easily parallelized and can take advantage of  the decreasing costs of 
powerful computer hardware.  
 
9.2 Genetic Algorithms  
The problem addressed by GAS is to search a space of candidate hypotheses to identify the best 
hypothesis. In GAS the "best hypothesis" is defined as the one that optimizes a pred efined numerical 
measure for the problem at hand, called b the hypothesis fitness . For example, if the learning task is the 
problem of approximating an unknown function given training examples of its input and output, then 
fitness could be defined as the a ccuracy of the hypothesis over this training data. If the task is to learn a 
strategy for playing chess, fitness could be defined as the number of games won by the individual when 
playing against other individuals in the current population.  
Although differ ent implementations of genetic algorithms vary in their details, they typically share the 
following structure: The algorithm operates by iteratively updating a pool of hypotheses, called the 
population. On each iteration, all members of the population are evaluated according to the fitness 
function. A new population is then generated by probabilistically selecting the fit individuals from the 
current population. Some of these selected individuals are carried forward into the next generation 
population intac t. Others are used as the basis for creating new offspring individuals by applying genetic 
operations such as crossover and mutation.  
101 
  
 
 
The inputs to this algorithm include the fitness function for ranking candidate hypotheses, a 
threshold defining an acceptable level of fitness for terminating the algorithm, the size of the 
population to be maintained, and parameters that determine how su ccessor populations are to be 
generated: the fraction of the population to be replaced at each generation and the mutation rate. 
Notice in this algorithm each iteration through the main loop produces a new generation of hypotheses 
based on the current popu lation. First, a certain number of hypotheses from the current population are 
selected for inclusion in the next generation. These are selected probabilistically, where the probability 
of selecting hypothesis hi is given by  
 
Thus, the probability that a hypothesis will be selected is proportional to its own fitness and is 
inversely proportional to the fitness of the other competing hypotheses in the current population.  
Once these members of the current generation have been selected for inclusion in the ne xt generation 
population, additional members are generated using a crossover operation. Crossover, defined in detail 
in the next section, takes two parent hypotheses from the current generation and creates two offspring 
hypotheses  
by recombining portions o f both parents. The parent hypotheses are chosen probabilistically from the 
current population, again using the probability function given by Equation (9.1). After new members 
have been created by this crossover operation, the new generation population now  contains the 

102 
 desired number of members. At this point, a certain fraction m of these members are chosen at 
random, and  random mutations all performed to alter these members.  
 
 
This GA algorithm thus performs a randomized, parallel beam search for hypothes es that perform well 
according to the fitness function. In the following subsections, we describe in more detail the 
representation of hypotheses and genetic operators used in this algorithm.  
 
Representing Hypotheses  
Hypotheses in GAS are often represented  by bit strings, so that they can be easily manipulated 
by genetic operators such as mutation and crossover. The hypotheses represented by these bit strings 
can be quite complex. For example, sets of if -then rules can easily be represented in this way, by 
choosing an encoding of rules that allocates specific substrings for each rule precondition and 
postcondition.  
To see how if -then rules can be encoded by bit strings, .first consider how we might use a bit 
string to describe a constraint on the value of a single attribute. To pick an example, consider the 
attribute Outlook, which can take on any of the three values Sunny, Overcast, or Rain. One obvious way 
to represent a constraint on Outlook is to use a bit string of length three, in which each bit position 
corresponds to one of its three possible values. Placing a 1 in some position indicates that the attribute 
is allowed to take on the corresponding value. For example, the string 010 represe nts the constraint 
that Outlook must take on the second of these values, or Outlook = Overcast. Similarly, the string 011 
represents the more general constraint that allows two possible values, or (Outlook = Overcast v Rain). 
Note 11 1 represents the most general possible constraint, indicating that we don't care which of its 
possible values the attribute takes on.  
Given this method for representing constraints on a single attribute, conjunctions of constraints 
on multiple attributes can easily be represent ed by concatenating the corresponding bit strings. For 
example, consider a second attribute, Wind, that can take on the value Strong or Weak. A rule 
precondition such as  
 
(Outlook = Overcast ^Rain) A (Wind = Strong)  
 
can then be represented by the followi ng bit string of length five:  
 
    Outlook    Wind  
01 1       10 
 
Rule postconditions (such as PlayTennis = yes) can be represented in a similar fashion. Thus, an entire 
rule can be described by concatenating the bit strings describing the rule precondition s, together with 
the bit string describing the rule postcondition. For example, the rule  
 
IF Wind = Strong THEN PlayTennis = yes 
 
would be represented by the string  
 
    Outlook     Wind     PlayTennis  
111      10     10 
 
where the first three bits describe the "don't care" constraint on Outlook, the next two bits describe the 
constraint on Wind, and the final two bits describe the rule postcondition (here we assume PlayTennis 
103 
 can take on the values Yes or No). Note the bit  string representing the rule contains a substring for each 
attribute in the hypothesis space, even if that attribute is not constrained by the rule preconditions. This 
yields a fixed length bit -string representation for rules, in which substrings at speci fic locations describe 
constraints on specific attributes. Given this representation for single rules, we can represent sets of 
rules by similarly concatenating the bit string representations of the individual rules.  
 
 
In designing a bit string encoding fo r some hypothesis space, it is useful to arrange for every 
syntactically legal bit string to represent a well -defined hypothesis. To illustrate, note in the rule 
encoding in the above paragraph the bit string 11 1 10 11 represents a rule whose postconditio n does 
not constrain the target attribute PlayTennis. If we wish to avoid considering this hypothesis, we may 
employ a different encoding (e.g., allocate just one bit to the PlayTennis postcondition to indicate 
whether the value is Yes or No), alter the ge netic operators so that they explicitly avoid constructing 
such bit strings, or simply assign a very  low fitness to such bit strings.  
 
In some GAS, hypotheses are represented by symbolic descriptions rather than bit strings.  
 
Genetic Operators  
The generat ion of successors in a GA is determined by a set of operators that recombine and 
mutate selected members of the current population. These operators correspond to idealized versions 
of the genetic operations found in biological evolution. The two most commo n operators are crossover 
and mutation.  
The crossover operator produces two new offspring from two parent strings, by copying 
selected bits from each parent. The bit at position i in each offspring is copied from the bit at position i 
in one of the two par ents. The choice of which parent contributes the bit for position i is determined by 
an additional string called the crossover mask. To illustrate, consider the single -point crossover 
operator at the top of Table Consider the topmost of the two offspring i n this case. This offspring takes 
its first five bits from the first parent and its remaining six bits from the second parent, because the 
crossover mask 11 11 1000000 specifies these choices for each of the bit positions. The second offspring 
uses the sam e crossover mask, but switches the roles of the two parents. Therefore, it contains the bits 
that were not used by the first offspring. In single -point crossover, the crossover mask is always 
constructed so that it begins with a string containing n contigu ous Is, followed by the necessary number 
of 0s to complete the string. This results in offspring in which the first n bits are contributed by one 
parent and the remaining bits by the second parent. Each time the single -point crossover operator is 
applied t he crossover point n is chosen at random, and the crossover mask is then created and applied.  
  
104 
  
 
 
 
 
 
 
In two-point crossover, offspring are created by substituting intermediate segments of one parent into 
the middle of the second parent string. Put another way, the crossover mask is a string beginning with 
no zeros, followed by a contiguous string of nl ones, followed by the nece ssary number of zeros to 
complete the string. Each time the two -point crossover operator is applied, a mask is generated by 
randomly choosing the integers no and nl.  
 
Fitness Function and Selection  
The fitness function defines the criterion for ranking po tential hypotheses and for 
probabilistically selecting them for inclusion in the next generation population. If the task is to learn 
classification rules, then the fitness function typically has a component that scores the classification 
accuracy of the ru le over a set of provided training examples. Often other criteria may be included as 
well, such as the complexity or generality of the rule. More generally, when the bit -string hypothesis is 
interpreted as a complex procedure (e.g., when the bit string rep resents a collection of if -then rules that 
will be chained together to control a robotic device), the fitness function may measure the overall 
performance of the resulting procedure rather than performance of individual rules.  
In our prototypical GA shown in above Table , the probability that a hypothesis will be selected is given 
by the ratio of its fitness to the fitness of other members of the current population as seen in Equation 
above . This method is sometimes called fitness proportionate selection, or roulette wheel selection. 
Other methods for using fitness to select hypotheses have also been proposed. For example, in 
tournament selection, two hypotheses are first chosen at random from the current population. With 
some predefined probability p the m ore fit of these two is then selected, and with probability (1 - p) the 
less fit hypothesis is selected. Tournament selection often yields a more diverse population than fitness 
proportionate selection. In another method called rank selection, the hypothes es in the current 
population are first sorted by fitness. The probability that a hypothesis will be selected is then 
proportional to its rank in this sorted list, rather than its fitness.  

105 
  
5.3. An Illustrative Example  
A genetic algorithm can be viewed as a  general optimization method that searches a large space 
of candidate objects seeking one that performs best according to the fitness function. Although not 
guaranteed to find an optimal object, GAS often succeed in finding an object with high fitness. GAS  have 
been applied to a number of optimization problems outside machine learning, including problems such 
as circuit layout and job -shop scheduling. Within machine learning, they have been applied both to 
function -approximation problems and to tasks such a s choosing the network topology for artificial 
neural network learning systems.  
To illustrate the use of GAS for concept learning, we briefly summarize the GABIL system 
described by DeJong et al. (1993). GABIL uses a GA to learn boolean concepts represente d by a 
disjunctive set of propositional rules. In experiments over several concept learning problems, GABIL was 
found to be roughly comparable in generalization accuracy to other learning algorithms such as the 
decision tree learning algorithm C4.5 and the  rule learning system AQ14. The learning tasks in this study 
included both artificial learning tasks designed to explore the systems' generalization accuracy and the 
real world problem of breast cancer diagnosis.  
The specific instantiation of the GA algori thm in GABIL can be summarized as follows:  
Representation. Each hypothesis in GABIL corresponds to a disjunctive set of propositional rules, 
encoded as described in Section 9.2.1. In particular, the hypothesis space of rule preconditions consists 
of a conj unction of constraints on a fixed set of attributes, as described in that earlier section. To 
represent a set of rules, the bit -string representations of individual rules are concatenated. To illustrate, 
consider a hypothesis space in which rule preconditi ons are conjunctions of constraints over two 
Boolean attributes, a1 and a2.The rule postcondition is described by a single bit that indicates the 
predicted  value of the target attribute c. Thus, the hypothesis consisting of the two rules  
 
IF a l=T^a 2=F THEN  c=T; IF a 2=T THEN c=F  
 
 
would be represented by the string  
 
a1   a2   c   a1   a2   c 
10  01  1  11  01 0  
 
Note the length of the bit string grows with the number of rules in the hypothesis. This variable bit -
string length requires a slight modification to  the crossover operator, as described below.  
 
Genetic operators. GABIL uses the standard mutation operator of  above  Table in which a single 
bit is chosen at random and replaced by its complement. The crossover operator that it uses is a fairly 
standard extension to the two -point crossover operator described in Table 9.2. In particular, to 
accommodate the variable -lengt h bit strings that encode rule sets, and to constrain the system so that 
crossover occurs only between like sections of the bit strings that encode rules, the following approach 
is taken. To perform a crossover operation on two parents, two crossover point s are first chosen at 
random in the first parent string. Let dl (dz) denote the distance from the leftmost (rightmost) of these 
two crossover points to the rule boundary immediately to its left. The crossover points in the second 
parent are now randomly ch osen, subject to the constraint that they must have the same dl and d2 
value. For example, if the two parent strings are  
106 
  
 
and the crossover points chosen for the first parent are the points following bit positions 1 and 8,  
 
 
 
where "[" and "1" indicate crossover points, then dl = 1 and dz = 3. Hence the allowed pairs of crossover 
points for the second parent include the pairs of bit positions (1,3), (1,8), and (6,8). If the pair (1,3) 
happens to  be chosen,  
 
 
 
then the two resulting offspring will be  
 
 
 
As this example illustrates, this crossover operation enables offspring to contain a different number of 
rules than their parents, while assuring that all bit strings generated in this fashion represent well -
defined rule sets.  
Fitness function. The fitne ss of each hypothesized rule set is based on its classification accuracy over 
the training data. In particular, the function used to measure fitness is  
 
 
where correct (h) is the percent of all training examples correctly classified by hypothesis h. 
 
In experiments comparing the behavior of GABIL to decision tree learning algorithms such as 
C4.5 and ID5R, and to the rule learning algorithm AQ14report roughly comparable performance among 
these systems, tested on a variety of learning problems. For example,  over a set of 12 synthetic 
problems, GABIL achieved an average generalization accuracy of 92.1 %, whereas the performance of 
the other systems ranged from 91.2 % to 96.6 %.  
 
Extensions  

107 
 In one set of experiments they explored the addition of two new geneti c operators that were 
motivated by the generalization operators common in many symbolic learning methods. The first of 
these operators,  AddAlternative, generalizes the constraint on a specific attribute by changing a 0 to a 
1 in the substring correspondin g to the attribute. For example, if the constraint on an attribute is 
represented by the string 10010, this operator might change it to 101 10. This operator was applied 
with probability .O1 to selected members of the population on each generation. The sec ond operator, 
Dropcondition performs a more drastic generalization step, by replacing all bits for a particular attribute 
by a 1. This operator corresponds to generalizing the rule by completely dropping the constraint on the 
attribute, and was applied on each generation with probability .60. The authors report this revised 
system achieved an average performance of 95.2% over the above set of synthetic learning tasks, 
compared to 92.1% for the basic GA algorithm.  
 
In the above experiment, the two new operat ors were applied with the same probability to each 
hypothesis in the population on each generation. In a second experiment, the bit -string representation 
for hypotheses was extended to include two bits that determine which of these operators may be 
applied  to the hypothesis. In this extended representation, the bit string for a typical rule set hypothesis 
would be  
 
 
 
where the final two bits indicate in this case that the AddAlternative operator may be applied to this bit 
string, but that the Dropcondition operator may not. These two new bits define part of the search 
strategy used by the GA and are themselves altered and evolved using the same crossover and mutation 
operators that operate on other bits in the string. While the authors report m ixed results with this 
approach (i.e., improved performance on some problems, decreased performance on others), it 
provides an interesting illustration of how GAS might in principle be used to evolve their own 
hypothesis search methods.  
 
5.4 Hypothesis Spa ce Search  
As illustrated above, GAS employ a randomized beam search method to seek a maximally fit hypothesis. 
This search is quite different from that of other learning methods we have considered in this book. To 
contrast the hypothesis space search of GA S with that of neural network BACKPROPAGATION, for 
example, the radiant descent search in BACKPROPAGATION moves smoothly from one hypothesis to a 
new hypothesis that is very similar. In contrast, the GA search can move much more abruptly, replacing 
a paren t hypothesis by an offspring that may be radically different from the parent. Note the GA search 
is therefore less likely to fall into the same kind of local minima that can plague gradient descent 
methods.  
One practical difficulty in some GA applications  is the problem of crowding. Crowding is a phenomenon 
in which some individual that is more highly fit than others in the population quickly reproduces, so that 
copies of this individual and very similar individuals take over a large fraction of the popula tion. The 
negative impact of crowding is that it reduces the diversity of the population, thereby slowing further 
progress by the GA. Several strategies have been explored for reducing crowding. One approach is to 
alter the selection function, using criter ia such as tournament selection or rank selection in place of 
fitness proportionate roulette wheel selection. A related strategy is "fitness sharing," in which the 
measured fitness of an individual is reduced by the presence of other, similar individuals i n the 
population. A third approach is to restrict the kinds of individuals allowed to recombine to form 
offspring. For example, by allowing only the most similar individuals to recombine, we can encourage 

108 
 the formation of clusters of similar individuals, o r multiple "subspecies" within the population. A related 
approach is to spatially distribute individuals and allow only nearby individuals to recombine. Many of 
these techniques are inspired by the analogy to biological evolution.  
 
Population Evolution and  the Schema Theorem  
It is interesting to ask whether one can mathematically characterize the evolution over time of the 
population within a GA. The schema theorem provides one such characterization. It is based on the 
concept of schemas, or patterns that d escribe sets of bit strings. To be precise, a schema is any string 
composed of 0s, 1s, and *'s. Each schema represents the set of bit strings containing the indicated 0s 
and 1s, with each “*” interpreted as a "don't care." For example, the schema 0*10 repr esents the set of 
bit strings that includes exactly 0010 and 01 10.  
An individual bit string can be viewed as a representative of each of the different schemas that it 
matches. For example, the bit string 0010 can be thought of as a representative of 24 distinct schemas 
including 00**, 0* 10, ****, etc. Similarly, a population of bit strings can be viewed in terms of the set 
of schemas that it represents and the number of individuals associated with each of these schema.  
The schema theorem characterizes th e evolution of the population within a GA in terms of the number 
of instances representing each schema. Let m(s, t) denote the number of instances of schema s in the 
population at time t (i.e., during the tth generation). The schema theorem describes the e xpected value 
of m(s,t+1 ) in terms of m(s, t) and other properties of the schema, population, and GA algorithm 
parameters.  
The evolution of the population in the GA depends on the selection step, the recombination step, and 
the mutation step. Let us start by considering just the effect of the selection step. Let f (h) denote the 
fitness of the individual bit string h and 
tf denote the average fitness of all individuals in the 
population at time t. Let n be the total number of individuals in the population. Let 
ptsh , indicate 
that the individual h is both a representative of schema s and a me mber of the population at time t. 
Finally, let 
tsu, denote the average fitness of instances of schema s in the population at time t.  
We are interested in calculating the expected value of m(s,t+1 ), which we denote E[m(s,t+1 )]. We can 
calculate E[m(s,t+1 )] using the probability distribution for selection given in Equation, which can be 
restated using our current terminology as follows  
 
Now if we select one member for the new population according to this probability distributio n, then the 
probability that we will select a representative of schema s is 
  

109 
  
The second step above follows from the fact that by definition,  
 
 
 
Equation  gives the probability that a single hypothesis selected by the GA will be an instance of schema 
s. Therefore, the expected number of instances of s resulting from the n independent selection steps 
that create the entire new generation is just n times this probability.  
 
 
 
Equation states that the expected number of instances of schema s at generation t+ 1 is proportional to 
the average fitness 
tsu,  of instances of this schema at time t , and inversely proportional to the 
average fitness 
tf  of all members of the population at time t. Thus, we can expect schemas with 
above average fitness to be represented with increasing frequency on successive generations. If we 
view the GA as performing a virtual parallel search through the space of possibl e schemas at the same 
time it performs its explicit parallel search through the space of individuals, then Equation indicates that 
more fit schemas will grow in influence over time.  
While the above analysis considered only the selection step of the GA, the crossover and mutation 
steps must be considered as well. The schema theorem considers only the possible negative influence 
of these genetic operators (e.g., random mutation may decrease the number of representatives of s, 
independent of 
tsu,  and considers only the case of single -point crossover. The full schema theorem 
thus provides a lower bound on the expected frequency of schema s, as follows:  
 
 
Here, pc is the probability that the single -point crossover operator will be applied  
to an arbitrary individual, and pm, is the probability that an arbitrary bit of an arbitrary individual will be 
mutated by the mutation operator. o(s) is the number of defined bits  in schema s, where 0 and 1 are 
defined bits, but * is not. d(s) is the distan ce between the leftmost and rightmost defined bits in s. 
Finally, l is the length of the individual bit strings in the population. Notice the leftmost term in Equation 
is identical to the term from Equation and describes the effect of the selection step. T he middle term 
describes the effect of the single -point crossover operator -in particular, it describes the probability that 
an arbitrary individual representing s will still represent s following application of this crossover 
operator. The rightmost term d escribes the probability that an arbitrary individual representing schema 
s will still represent schema s following application of the mutation operator. Note that the effects of 
single -point crossover and mutation increase with the number of defined bits o(s) in the schema and 
with the distance d(s) between the defined bits. Thus, the schema theorem can be roughly interpreted 
as stating that more fit schemas will tend to grow in influence, especially schemas containing a small 
number of defined bits (i.e.,  containing a large number of *'s), and especially when these defined bits 

110 
 are near one another within the bit string. The schema theorem is perhaps the most widely cited 
characterization of population evolution within a GA. One way in which it is incomple te is that it fails to 
consider the (presumably) positive effects of crossover and mutation. Numerous more recent 
theoretical analyses have been proposed, including analyses based on Markov chain models and on 
statistical mechanics models.  
 
5.5 GENETIC PRO GRAMMING  
Genetic programming (GP) is a form of evolutionary computation in which the individuals in the 
evolving population are computer programs rather than bit strings. The basic genetic programming 
approach and presents a broad range of simple programs that can be successfully learned by GP.  
 
Representing Programs  
Programs manipulated by a GP are typically represented by trees corresponding to the parse tree of the 
program. Each function call is represented by a node in the tree, and the arguments to the functio n are 
given by its descendant nodes. For example, below Figure illustrates this tree representation for the 
function sin(x) +
yx2 . To apply genetic programming to a particular domain, the user must define 
the primitive functions to be con sidered (e.g., sin, cos, 
 , +, -, exponential~), as well as the terminals 
(e.g., x, y, constants such as 2). The genetic programming algorithm then uses an evolutionary search to 
explore the vast space of programs that can be described  using these primitives. As in a genetic 
algorithm, the prototypical genetic programming algorithm maintains a population of individuals (in this 
case, program trees). On each iteration, it produces a new generation of individuals using selection, 
crossove r, and mutation. The fitness of a given individual program in the population is typically 
determined by executing the program on a set of training data. Crossover operations are performed by 
replacing a randomly chosen subtree of one parent program by a su btree from the other parent 
program.  
 

111 
  
 
Above Figure  illustrates  a typical crossover operation. It describes a set of experiments applying 
a GP to a number of applications. In his experiments, 10% of the current population, selected 
probabilistically according to fitness, is retained unchanged in the next generation. T he remainder of 
the new generation is created by applying crossover to pairs of programs from the current generation, 
again selected probabilistically according to their fitness. The mutation operator was not used in this 
particular set of experiments.  
 
Illustrative Example  
One illustrative example presented by Koza (1992) involves learning an algorithm for stacking the blocks 
shown in below Figure The task is to develop a general algorithm for stacking the blocks into a single 
stack that spells the word "universal," independent of the initial configuration of blocks in the world. 
The actions available for manipulating blocks allow moving only a single block at a time. In particular, 
the top block on the stack can be moved to the table surface, or a block on the table surface can be 
moved to the top of the stack.  

112 
  
 
As in most GP applications, the choice of problem representation has a significant impact on the 
ease of solving the problem. In Koza's formulation, the primitive functions used to compose prog rams 
for this task include the following three terminal arguments:  
 CS (current stack), which refers to the name of the top block on the stack, or F if there is no 
current stack.  
 TB (top correct block), which refers to the name of the topmost block on the s tack, such that it 
and those blocks beneath it are in the correct order.  
 NN (next necessary), which refers to the name of the next block needed above TB in the stack, 
in order to spell the word "universal," or F if no more blocks are needed.  
 
As can be se en, this particular choice of terminal arguments provides a natural representation for 
describing programs for manipulating blocks for this task. Imagine, in contrast, the relative difficulty of 
the task if we were to instead define the terminal arguments to be the x and y coordinates of each 
block.  
In addition to these terminal arguments, the program language in this application included the 
following primitive functions:  
 (MS x) (move to stack), if block x is on the table, this operator moves x to the top  of the stack 
and returns the value T. Otherwise, it does nothing and returns the value F.  
 (MT x) (move to table), if block x is somewhere in the stack, this moves the block at the top of 
the stack to the table and returns the value T. Otherwise, it return s the value F.  
 (EQ x y) (equal), which returns T if x equals y, and returns F otherwise.  
 (NOT x), which returns T if x = F, and returns F if x = T.  
 (DU x y) (do until), which executes the expression x repeatedly until expressiony returns the 
value T.  
 
To a llow the system to evaluate the fitness of any given program, Koza provided a set of 166 training 
example problems representing a broad variety of initial block configurations, including problems of 
differing degrees of difficulty. The fitness of any given  program was taken to be the number of these 
examples solved by the algorithm. The population was initialized to a set of 300 random programs. 
After 10 generations, the system discovered the following program, which solves all 166 problems.  
 
(EQ (DU (MT CS )(NOT CS)) (DU (MS NN)(NOT NN)) )  
 
Notice this program contains a sequence of two DU, or "Do Until" statements. The first repeatedly 

113 
 moves the current top of the stack onto the table, until the stack becomes empty. The second "Do 
Until" statement then repe atedly moves the next necessary block from the table onto the stack. The 
role played by the top level EQ expression here is to provide a syntactically legal way to sequence these 
two "Do Until" loops.  
Somewhat surprisingly, after only a few generations, th is GP was able to discover a program that solves 
all 166 training problems. Of course the ability of the system to accomplish this depends strongly on the 
primitive arguments and functions provided, and on the set of training example cases used to evaluate  
fitness.  
 
Remarks on Genetic Programming  
As illustrated in the above example, genetic programming extends genetic algorithms to the 
evolution of complete computer programs. Despite the huge size of the hypothesis space it must 
search, genetic programming has been demonstrated to produce intriguin g results in a number of 
applications. A comparison of GP to other methods for searching through the space of computer 
programs, such as hillclimbing and simulated annealing, is given by O'Reilly and Oppacher (1994).  
While the above example of GP search is  fairly simple, Koza et al. (1996) summarize the use of a GP in 
several more complex tasks such as designing electronic filter circuits and classifying segments of 
protein molecules. The filter circuit design problem provides an example of a considerably m ore 
complex problem. Here, programs are evolved that transform a simple fixed seed circuit into a final 
circuit design. The primitive functions used by the GP to construct its programs are functions that edit 
the seed circuit by inserting or deleting circu it components and wiring connections. The fitness of each 
program is calculated by simulating the circuit it outputs (using the SPICE circuit simulator) to determine 
how closely this circuit meets the design specifications for the desired filter. More prec isely, the fitness 
score is the sum of the magnitudes of errors between the desired and actual circuit output at 101 
different input frequencies. In this case, a population of size 640,000 was maintained, with selection 
producing 10% of the successor popul ation, crossover producing 89%, and mutation producing 1%. The 
system was executed on a 64 -node parallel processor. Within the first randomly generated population, 
the circuits produced were so unreasonable that the SPICE simulator could not even simulate the 
behavior of 98% of the circuits. The percentage of unsimulatable circuits dropped to 84.9% following 
the first generation, to 75.0% following the second generation, and to an average of 9.6% over 
succeeding generations. The fitness score of the best ci rcuit in the initial population was 159, compared 
to a score of 39 after 20 generations and a score of 0.8 after 137 generations. The best circuit, produced 
after 137 generations, exhibited performance very similar to the desired behavior.  
In most cases, t he performance of genetic programming depends crucially on the choice of 
representation and on the choice of fitness function. For this reason, an active area of current research 
is aimed at the automatic discovery and incorporation of subroutines that imp rove on the original set of 
primitive functions, thereby allowing the system to dynamically alter the primitives from which it 
constructs individuals. See, for example, Koza (1994).  
 
5.6 Models of Evolution and Learning  
In many natural systems, individual organisms learn to adapt significantly during their lifetime. 
At the same time, biological and social processes allow their species to adapt over a time frame of many 
generations. One interesting question regarding evolutionary systems is "What is the rela tionship 
between learning during the lifetime of a single individual, and the longer time frame species -level 
learning afforded by evolution?'  
 
Lamarckian Evolution  
Larnarck was a scientist who, in the late nineteenth century, proposed that evolution over many 
generations was directly influenced by the experiences of individual organisms during their lifetime. In 
114 
 particular, he proposed that experiences of a single organism directly affected the genetic makeup of 
their offspring: If an individual learned du ring its lifetime to avoid some toxic food, it could pass this 
trait on genetically to its offspring, which therefore would not need to learn the trait. This is an 
attractive conjecture, because it would presumably allow for more efficient evolutionary pro gress than 
a generate -and-test process (like that of GAS and GPs) that ignores the experience gained during an 
individual's lifetime. Despite the attractiveness of this theory, current scientific evidence 
overwhelmingly contradicts Lamarck's model. The cur rently accepted view is that the genetic makeup 
of an individual is, in fact, unaffected by the lifetime experience of one's biological parents. Despite this 
apparent biological fact, recent computer studies have shown that Lamarckian processes can 
sometim es improve the effectiveness of computerized genetic algorithms (see Grefenstette 1991; 
Ackley and Littman 1994; and Hart and Belew 1995).  
 
Baldwin Effect  
Although Lamarckian evolution is not an accepted model of biological evolution, other mechanisms 
have  been suggested by which individual learning can alter the course of evolution. One such 
mechanism is called the Baldwin effect, after J. M. Baldwin (1896), who first suggested the idea. The 
Baldwin effect is based on the following observations:  
 If a species is evolving in a changing environment, there will be evolutionary pressure to favor 
individuals with the capability to learn during their lifetime. For example, if a new predator 
appears in the environment, then individuals capable of learning  to avoid the predator will be 
more successful than individuals who cannot learn. In effect, the ability to learn allows an 
individual to perform a small local search during its lifetime to maximize its fitness. In contrast, 
nonlearning individuals whose f itness is fully determined by their genetic makeup will operate 
at a relative disadvantage.  
 Those individuals who are able to learn many traits will rely less strongly on their genetic code 
to "hard -wire" traits. As a result, these individuals can support a more diverse gene pool, relying 
on individual learning to overcome the "missing" or "not quite optimized" traits in the genetic 
code. This more diverse gene pool can, in turn, support more rapid evolutionary adaptation. 
Thus, the ability of individuals t o learn can have an indirect accelerating effect on the rate of 
evolutionary adaptation for the entire population.  
 
To illustrate, imagine some new change in the environment of some species, such as a new 
predator. Such a change will selectively favor indi viduals capable of learning to avoid the predator. As 
the proportion of such self -improving individuals in the population grows, the population will be able to 
support a more diverse gene pool, allowing evolutionary processes (even non -Lamarckian generate -
and-test processes) to adapt more rapidly. This accelerated adaptation may in turn enable standard 
evolutionary processes to more quickly evolve a genetic (nonlearned) trait to avoid the predator (e.g., 
an instinctive fear of this animal). Thus, the Baldwi n effect provides an indirect mechanism for 
individual learning to positively impact the rate of evolutionary progress. By increasing survivability and 
genetic diversity of the species, individual learning supports more rapid evolutionary progress, thereby  
increasing the chance that the species will evolve genetic, nonlearned traits that better fit the new 
environment.  
There have been several attempts to develop computational models to study the Baldwin 
effect. For example, Hinton and Nowlan (1987) experim ented with evolving a population of simple 
neural networks, in which some network weights were fixed during the individual network "lifetime," 
while others were trainable. The genetic makeup of the individual determined which weights were 
115 
 trainable and whi ch were fixed. In their experiments, when no individual learning was allowed, the 
population failed to improve its fitness over time. However, when individual learning was allowed, the 
population quickly improved its fitness. During early generations of ev olution the population contained 
a greater proportion of individuals with many trainable weights. However, as evolution proceeded, the 
number of fixed, correct network weights tended to increase, as the population evolved toward 
genetically given weight va lues and toward less dependence on individual learning of weights. 
Additional computational studies of the Baldwin effect have been reported by Belew (1990), Harvey 
(1993), and French and Messinger (1994). An excellent overview of this topic can be found i n Mitchell 
(1996). A special issue of the journal Evolutionary Computation on this topic (Turney et al. 1997) 
contains several articles on the Baldwin effect.  
 
5.7 Parallelizing Genetic Algorithms  
GAS are naturally suited to parallel implementation, and a number of approaches to 
parallelization have been explored. Coarse grain approaches to parallelization subdivide the population 
into somewhat distinct groups of individuals, called demes. Each deme is assigned to a different 
computational node, and a stand ard GA search is performed at each node. Communication and cross -
fertilization between demes occurs on a less frequent basis than within demes. Transfer between 
demes occurs by a migration process, in which individuals from one deme are copied or transferr ed to 
other demes. This process is modeled after the kind of cross -fertilization that might occur between 
physically separated subpopulations of biological species. One benefit of such approaches is that it 
reduces the crowding problem often encountered in  nonparallel GAS, in which the system falls into a 
local optimum due to the early appearance of a genotype that comes to dominate the entire 
population. Examples of coarse -grained parallel GAS are described by Tanese (1989) and by Cohoon et 
al. (1987).  
In contrast to coarse -grained parallel implementations of GAS, fine -grained implementations 
typically assign one processor per individual in the population. Recombination then takes place among 
neighboring individuals. Several different types of neighborhoods  have been proposed, ranging from 
planar grid to torus. Examples of such systems are described by Spiessens and Manderick (1991). An 
edited collection of papers on parallel GAS is available in Stender (1993).  
 
 


                                 DEPARTMENT OF  
          COMPUTER SCIENCE AND ENGINEERING  
                              DIGITAL NOTES  
ON                                                
DEEP LEARNING  
(R20A6610)  
  
  
                                      Prepared by  
                                      K.Chandusha  
 
 
    MALLA REDDY COLLEGE OF                             
ENGINEERING&TECHNOLOGY  (AutonomousInstitution –UGC,Govt.of India) 
Recognizedunder2(f)and12(B)ofUGCACT 1956  
(AffiliatedtoJNTUH,Hyderabad,Approved byAICTE -AccreditedbyNBA&NAAC –‘A’Grade -ISO9001:2015 Certified ) 
Maisammaguda,Dhulapally(PostVia. Hakimpet),Secunderabad –500100,TelanganaState, India  
SYLLABUS  
IV YearB. TechCSE                         L/T/P/C  3/-/-/-3 
(R20A6610)DEEP LEARNING  
COURSEOBJECTIVES:  
1. To understand the basic concepts and techniques of Deep Learning and the need of 
Deep Learningtechniques in real -world problems  
2. TounderstandCNNalgorithmsandthewaytoevaluateperformanceofthe CNN 
architectures.  
3. ToapplyRNNandLSTMtolearn,predictandclassifythereal -worldproblems in 
theparadigmsofDeepLearning.  
4. Tounderstand,learnanddesignGANsfortheselected problems.  
5. TounderstandtheconceptofA uto-encodersandenhancingGANsusingauto -encoders.  
 
UNIT -I: 
INTRODUCTIONTODEEPLEARNING:HistoricalTrendsinDeepLearning, Why  
DL is Growing, Artificial Neural Network, Non -linear classification example using 
Neural Networks: XOR/XNOR, Single/Multiple Layer Perceptron, Feed Forward 
Network, Deep Feed - forward networks, Stochastic Gradient –Based learning, Hidden 
Units, Architecture Design, Back - Propagation.  
UNIT -II: 
CONVOLUTION NEURAL NETWORK (CNN): Introduction to CNNs and their 
applic ations in computer vision, CNN basic architecture, Activation functions -sigmoid, 
tanh, ReLU, Softmax layer, Types of pooling layers, Training of CNN in TensorFlow, 
various popular CNN architectures: VGG, Google Net, ResNet etc, Dropout, 
Normalization, Data  augmentation  
UNIT -III 
RECURRENT NEURAL NETWORK (RNN): Introduction to RNNs and their 
applications in sequential data analysis, Back propagation through time (BPTT), 
Vanishing Gradient Problem, gradient clipping Long Short Term Memory (LSTM) 
Networks, Gate d Recurrent Units, Bidirectional LSTMs, Bidirectional RNNs.  
UNIT -IV 
GENERATIVE ADVERSARIAL NETWORKS (GANS): Generative models, Concept 
and principles of GANs, Architecture of GANs (generator and discriminator networks), 
Comparison between discriminative an d generative models, Generative Adversarial 
Networks (GANs), Applications of GANs.  
UNIT -V 
AUTO -ENCODERS: Auto -encoders, Architecture and components of auto -encoders 
(encoder and decoder), Training an auto -encoder for data compression and 
reconstruction, Re lationship between Autoencoders and GANs, Hybrid Models: 
Encoder -Decoder GANs.  
TEXTBOOKS:  
1. DeepLearning:AnMITPressBookbyIanGoodfellowandYoshuaBengioAaron 
Courville.  
2. MichaelNielson,NeuralNetworksandDeepLearning,Determination Press,2015.  
3. SatishKumar,Neuralnetworks:AclassroomApproach,TataMcGraw -HillEducation, 
2004.  
REFERENCES:  
1. DeepLearningwithPython,FrancoisChollet,Manningpublications, 2018  
2. Advanced Deep  Learning  with  Keras, Rowel  Atienza, PACKT  Publications, 
2018  
COURSE OUTCOMES:  
 
CO1:UnderstandthebasicconceptsandtechniquesofDeepLearningand the 
needofDeepLearningtechniquesinreal -worldproblems.  
CO2:Understand CNN algorithmsand theway toevaluateperformance of  
theCNN architectures.  
CO3:ApplyRNNandLSTMtolearn,predictandclassifythereal -world  
problemsintheparadigmsofDeep Learning.  
CO4:Understand,learnanddesignGANsfortheselected problems.  
CO5:UnderstandtheconceptofAuto -encodersandenhancingGANsusing auto- 
encoders.  
B.Tech –CSE R-20 
Deep Learning   
 UNIT -I: 
INTRODUCTIONTODEEPLEARNING: HistoricalTrends in 
Deep Learning, Why DL is Growing, Artificial Neural Network,Non -
linear classification example using Neural Networks: XOR/XNOR, 
Single/Multiple Layer Perceptron, Feed Forward Network, Deep 
Feed - forward networks, Stochastic Gradient –Based learning, 
Hidden Units, Architecture Design, Back - Propagation, Deep learning 
frameworks and libraries (e.g., TensorFlow/Keras, PyTorch).  
INTRODUCTIONTODEEP LEARNING:  
Deep learning is  a branch of machine learning which is based on artificial neural 
networks. It is capable of learning complex patterns and relationships within data. In deep 
learning, we don’t need to explicitly program everything. It has become increasinglypopular 
in rec ent years due to the advances in processing power and the availability oflarge 
datasets. Because it is based on artificial neural networks (ANNs) also known as deep neural 
networks (DNNs). These neural networks are inspired by the structure and 
functionoft hehumanbrain’s biologicalneurons,andtheyaredesignedtolearnfromlarge 
amounts of data.  
1. Deep Learning is a subfield of Machine Learning that involves the use of neural 
networks to model and solve complex problems. Neural networks are modeled 
after the structu re and function of the human brain and consist of layers of 
interconnected nodes that process and transform data.  
2. The key characteristic of Deep Learning is the use of deep neural networks,which 
have multiple layers of interconnected nodes. These networks can learn complex 
representations of data by discovering hierarchical patterns andfeatures in the 
data. Deep Learning algorithms can automatically learn and improve from data 
without the need for manual feature engineering.  
3. Deep Learning has achieved significant success in various fields, including image 
recognition, natural language processing, speech recognition, and 
recommendation systems. Some of the popular Deep Learning architectures 
include Convolutional Neural Networks (CNNs), Recurren t Neural Networks 
(RNNs), and Deep Belief Networks (DBNs).  
4. Training deep neural networks typically requires a large amount of data and 
computational resources. However, the availability of cloud computing and the 
developmentofspecializedhardware,suchasGrap hicsProcessingUnits (GPUs), has 
made it easier to train deep neural networks.  
 
In summary, Deep Learning is a subfield of Machine Learning that involves the useof 
deep neural networks to model and solve complex problems. Deep Learning 
hasachievedsignifican tsuccessinvariousfields,anditsuseisexpectedtocontinuetogrow as more 
data becomes available, and more powerful computing resources become available.  
B.Tech –CSE R-20 
Deep Learning   
 
WhatisDeep Learning?  
Deep learning is the branch of “ Machine Learning  ”which is based on artificial neural 
network architecture. An artificial neural network or ANN uses layers of interconnected 
nodes called n eurons that work together to process and learn from theinput data.  
In a fully connected Deep neural network, there is an input layer and one or more 
hiddenlayersconnectedoneaftertheother.Eachneuronreceivesinputfromthe previous layer 
neurons or the input la yer. The output of one neuron becomes the input to other neurons in 
the next layer of the network, and this process continues until the final layer produces the 
output of the network. The layers of the neural network transform the input data through a 
series of nonlinear transformations, allowing the network to learn complex representations 
of the input data.  
 
 
 
B.Tech –CSE R-20 
Deep Learning   
 Today, Deep learning has become one of the most popular and visible areas of 
machine learning, due to its success in a variety  of applications, such as computer vision, 
natural language processing, and Reinforcement learning.  
Deep learning can be used for supervised, unsupervised as well as reinforcement 
machine learning. it uses a variety of ways to process these.  
• Supervised Mac hine Learning: Supervised machine learning  is the 
machine learning  technique in which the neural network learns to make 
predictions or classify data based on the labeled datas ets. Here we input both 
input features along with the target variables. the neural network learns to make 
predictions based on the cost or error that comes from the difference between 
thepredictedandtheactualtarget,thisprocessisknownasbackpropagation. Deep  
learning algorithms like Convolutional neural networks, Recurrent neural 
networks are used for many supervised tasks like image classifications and 
recognition, sentiment analysis, language translations, etc.  
• UnsupervisedMachineLearning : Unsupervisedmachinelearning  is the machine 
learning  technique in which the neural network learns to discover the patterns o r 
to cluster the dataset based on unlabeled datasets. Here thereare no target 
variables. while the machine has to self -determined the hidden patterns or 
relationships within the datasets. Deep learning algorithms like autoencoders 
and generative models are  used for unsupervised tasks like clustering, 
dimensionality reduction, and anomaly detection.  
• ReinforcementMachineLearning : ReinforcementMachineLearning  is the 
machinelearning  techniqueinwhichanagentlearnstomakedecisionsin an 
environm ent to maximize a reward signal. The agent interacts with the 
environment by taking action and observing the resulting rewards. Deeplearning 
can be used to learn policies, or a set of actions, that maximizes the 
cumulativerewardovertime.Deepreinforcementle arningalgorithmslike Deep Q 
networks and Deep Deterministic Policy Gradient (DDPG) are used to reinforce 
tasks like robotics and game playing etc.  
Artificialneural networks:  
“Artificialneuralnetworks ” arebuiltontheprinciplesofthestructureand 
operationofhumanneurons. Itis alsoknown as neural networks or neural nets. An artificial 
neural network’s input layer, which is the first layer, receives input from external  sources 
and passes it on to the hidden layer, which is the second layer. Each neuron in the hidden 
layer gets information from the neurons in the previous layer, computes the 
weightedtotal,andthentransfersit tothe neuronsinthe nextlayer.Theseconnections a re 
weighted, which means that the impacts of the inputs from the preceding layer aremore or 
less optimized by giving each input a distinct weight. These weights are then adjusted during 
the training process to enhance the performance of the model.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
FullyConnectedArtificialNeural Network  
 
Artificial neurons, also known as units, are found in artificial neural networks. The 
wholeArtificialNeuralNetwork iscomposed oftheseartificialneurons,whichare arranged in a 
series of layers. The  complexities of neural networks will depend on the complexities of the 
underlying patterns in the dataset whether a layer has a dozen units or millions of 
units.Commonly, Artificial Neural Network has an inputlayer,anoutputlayer as well as 
hidden layers. The input layer receives data from the outside world which the neural 
network needs to analyze or learn about.  
Ina fullyconnectedartificialneural network,thereis aninputlayerandone or more 
hidden layers connected one after the other. Each neuron receives i nput from the previous 
layer neurons or the input layer. The output of one neuron becomes the input to other 
neurons in the next layer of the network, and this process continues until the final layer 
produces the output of the network. Then, after passing through one or more hidden layers, 
this data is transformed into valuable data for the output layer. Finally, the output layer 
provides an output in the form of an artificial neural network’s response to the data that 
comes in.  
Units are linked to one anot her from one layer to another in the bulk of neural 
networks. Each of these links has weights that control how much one -unit influences 
another. The neural network learns more and more about the data as it moves from oneunit 
to another, ultimately producin g an output from the output layer.  
DifferencebetweenMachineLearningandDeep Learning:  
Machine learning and deep learning both are subsets of artificial intelligence but 
there are many similarities and differences between them.  

B.Tech –CSE R-20 
Deep Learning   
  
Machine  Learning  Deep  Learning  
Apply statistical algorithms to learn the 
hidden patterns and relationships in the 
dataset.  Uses artificial neural networkarchitecture 
to learn the hidden patterns and 
relationships in the dataset.  
 
Canworkonthesmalleramountof  dataset  Requiresthelargervolumeofdataset 
compared to machine learning  
 
Betterforthelow -label task.  Better for complex task like image 
processing, natural language processing, 
etc. 
Takeslesstimetotrainthe model.  Takesmoretimetotrainthe model.  
A model is created by relevant features 
which are manually extracted from images 
to detect an object in the image.  Relevant features are automatically 
extracted from images. It is an end -to- 
end learning process.  
 
Lesscomplexandeasytointerpretthe result.   
Morecomplex,itworksliketheblackbox 
interpretationsoftheresultarenot easy.  
It can work on the CPU or requires less 
computingpowerascomparedtodeep 
learning.   
Itrequiresahigh -performancecomputer 
with GPU.  
 
Typesofneural networks:  
 
DeepLearningmodelsareabletoautomaticallylearnfeaturesfromthedata, 
whichmakesthemwell -suitedfortaskssuchasimagerecognition,speechrecognition, 
andnaturallanguageprocessing.Themostwidelyusedarchitecturesindeeplearningare  
B.Tech –CSE R-20 
Deep Learning   
 
feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural 
networks (RNNs).  
Feedforward neural networks (FNNs) are the simplest type of ANN, with a linear flow of 
information through the network. FNNs have been widely used for tasks such as image 
classification, speech recognition, and natu ral language processing.  
Convolutional Neural Networks (CNNs) are specifically for image and video recognitiontasks. 
CNNs are able to automatically learn features from  the images, which makes them well -
suitedfortaskssuchasimageclassification,objectdetection,andimage segmentation.  
Recurrent Neural Networks (RNNs) are a type of neural network that is able to process 
sequential data, such as time series and natural language. RNNs are able to maintain an 
internalstatethatcapturesinformationaboutthepreviousinputs,whichmakesthem well -
suitedfortaskssuchasspeechrecognition,naturallanguageprocessing,and language 
translation.  
ApplicationsofDeep Learning:  
The main applications of deep learning can be divided in to computer vision, natural 
language processing (NLP), and reinforcement learning.  
 
Computer vision  
In computer vision , Deep learning models can enable machines to identify and 
understand visual data. Some of the main applications of deep learning in computer vision 
include:  
• Object detection and recognition: Deep learning model can be used to identify 
and locate objects within images and videos, making it possible for machines to 
perform tasks such as self -driving cars, surveillance, and robotics.  
• Image classification: Deep learn ing models can be used to classify images into 
categories such as animals, plants, and buildings. This is used in applicationssuch 
as medical imaging, quality control, and image retrieval.  
B.Tech –CSE R-20 
Deep Learning   
 • Imagesegmentation: Deeplearningmodelscanbeusedfor image segmentation into 
different regions, making it possible to identify specific features within images.  
Naturallanguageprocessing (NLP) : 
In NLP, the Deep learning model can enable machines to understand and generate 
human language. Some of the main applications of deep learning in NLPinclude:  
• Automatic Text Generation – Deep learning model can learn the corpus of text 
andnewtextlikesummaries,essayscanbeautomaticallygeneratedusing these 
trained models.  
• Language translation: Deep learning models can translate text from one 
language to another, making it possible to communicate with people from 
different linguistic backgro unds.  
• Sentiment analysis: Deep learning models can analyze the sentiment of a piece 
oftext,makingitpossibletodeterminewhetherthetextispositive,negative, or 
neutral. This is used in applications such as customer service, social media 
monitoring, and politic al analysis.  
• Speech recognition: Deep learning models can recognize and transcribe spoken 
words, making it possible to perform tasks such as speech -to-text conversion, 
voice search, and voice -controlled devices.  
Reinforcement learning:  
In reinforcement learning , deep learning works as training agents to take action inan 
environment to maximize a rew ard. Some of the main applications of deep learning in 
reinforcement learning include:  
• Game playing: Deep reinforcement learning models have been able to beat 
human experts at games such as Go, Chess, and Atari.  
• Robotics: Deep reinforcement learning models  can be used to train robots to 
perform complex tasks such as grasping objects, navigation, and manipulation.  
• Control systems: Deep reinforcement learning models can be used to control 
complex systems such as power grids, traffic management, and supply cha in 
optimization.  
Popularspecificapplicationsof DL: 
  
 
 
 
 
 
 
ChallengesinDeep Learning:  
Deep learning has made significant advancements in various fields, but there are still 
some challenges that need to be addressed. Here are some of the main challenges in 
deep learning:  
1.Data availability:  It requires large amounts of data to learn from. For using deep 
learning it’s a big concern to gather as much data for training.  

B.Tech –CSE R-20 
Deep Learning   
 2. Computational Resources:  For training the deep learning model, it is 
computationally expensive because it requires specialized hardware like GPUs 
and TPUs.  
3. Time -consuming:  While working on sequential data depending on the 
computational resource it can take very large even in days or months.  
4. Interpretability:Deeplearningmodelsarecomplex,itworkslikeablack box.it is very 
difficult to interpret the result.  
5. Overfitting:  when the  model is trained again and again, it becomes too 
specializedforthetrainingdata,leadingtooverfittingandpoorperformance on new 
data.  
 
AdvantagesofDeep Learning:  
 
1. High accuracy:  Deep Learning algorithms can achieve state -of-the-art 
performance in various tasks, such as image recognition and natural language 
processing.  
2. Automated feature engineering:  Deep Learning algorithms can automatically 
discover and learn relevant features f rom data without the need for manual 
feature engineering.  
3. Scalability:  Deep Learning models can scale to handle large and complexdatasets, 
and can learn from massive amounts of data.  
4. Flexibility : DeepLearning models can be appliedto a wide range of tasks a ndcan 
handle various types of data, such as images, text, and speech.  
5. Continual improvement:  Deep Learning models can continually improve their 
performance as more data becomes available.  
 
DisadvantagesofDeep Learning:  
 
1. Highcomputationalrequirements: DeepLea rningmodelsrequirelarge amounts of 
data and computational resources to train and optimize.  
2. Requires large amounts of labeled data : Deep Learning models often require a 
large amount of labeled data for training, which can be expensive and time - 
consuming to  acquire.  
3. Interpretability: DeepLearningmodelscanbechallengingtointerpret,making 
itdifficulttounderstandhowtheymakedecisions. Overfitting: Deep Learning 
models can sometimes overfit to the training data, resulting in poor performance 
on new and unseen data.  
4. Black -box nature:  Deep Learning models are often treated as black boxes,making 
it difficult to understand how they work and how they arrived at their 
predictions.  
In summary, while Deep Learning offers many advantages,including 
high accuracy and scal ability, it also has some disadvantages, such as high 
computational requirements, the need for large amounts of labeled data, 
andinterpretabilitychallenges.Theselimitationsneedtobecarefully considered 
when deciding whether to use Deep Learning for a specif ic task.  
B.Tech –CSE R-20 
Deep Learning   
 HistoricalTrendsinDeep Learning:  
Deep learning has experienced significant historical trends since its 
inception. Here are some key milestones and trends that have 
shaped the field:  
1. Early Developments: Deep learning traces its roots back to 
the 1960s with the development of Artificial Neural Networks 
(ANNs).  
• The idea of using interconnected nodes inspired by the human 
brain's structure laid the foundation for later deep learning 
advancements.  
2. WinterofAI: Inthe1970sand1980s,deeple arningfacedaperiodofstagnation known 
as the "AI winter."  
• Limited computational power, insufficient data, and theoretical 
challenges hindered progress in the field, leading to decreased 
interest and funding.  
 
3. Backpropagation: In the 1980s, the backpropagation algorithm, 
which efficiently trains deep neural networks, was rediscovered and 
popularized.  
• This breakthrough allowed for more efficient training of 
multi -layer neural networks, addressing some of the 
limitations faced during the AI winter.  
 
4. Rise of Convolutional Neural Networks (CNNs): In the late 1990s and 
early 2000s, CNNs gained prominence in the fieldof computer vision.  
• TheLeNet -5architecturedevelopedbyYannLeCunrevolutionized 
image recognition tasks and demonstrated the potential of dee p 
learning in visual perception.  
 
5. BigDataandGPUs :Theearly2010smarkedaturningpointfor 
deeplearningwiththeadventofbigdataandtheavailabilityofpowerful 
Graphics Processing Units (GPUs).  
• Theabundanceoflabeleddata,combinedwithGPUacceleration, 
enabled the training of large -scale deep neuralnetworks and 
significantly improved performance.  
6. ImageNetandDeepLearningRenaissance: TheImageNetLargeScale 
VisualRecognitionChallengein2012,wonbyadeepneura lnetworkknown as 
AlexNet, brought deep learning into the spotlight.  
• This event sparked a renaissance in the field, encouraging 
researcherstoexploredeeplearningarchitecturesandtechniques 
across various domains.  
 
7. DeepLearninginNaturalLanguageProcessing(NLP): Deep learning  
B.Tech –CSE R-20 
Deep Learning   
 techniques, particularly recurrent neural networks(RNNs) and later transformer 
models, have made substantial advancements in NLP tasks.  
• Models like LSTM (Long Short -Term Memory) and BERT 
(Bidirectional Encoder Representations from Transformers) have 
achieved state -of-the-art results in tasks like machine translation, 
sentiment analysis, and question answering.  
 
8. Generative Models: The introduction of generative models like 
Variational Autoencoders (VAEs) and Generative Adversarial 
Networks (GANs) opened up possibilities for generating realistic 
images, videos, and audio.  
• GANs,inparticular,havedemonstratedimpressivecapabilitiesin 
generating synthetic data.  
 
9. TransferLearning andPretraining: Transferlearninghas b ecome a 
prevalent technique in deep learning, enabling models to leverage 
knowledge from pretraining on large datasets and then fine -tune on 
specific tasks.  
• Thisapproachhasledtosignificantperformanceimprovementsand 
reduced training time, especially in scen arioswith limited labeled 
data. 
 
10. ExplainabilityandInterpretability: Asdeeplearningmodels 
have become increasingly complex, researchers havefocused on 
improving their explainability and interpretability.  
• Techniques like attention mechanisms, saliency maps, a nd 
model -agnosticinterpretabilitymethodsaimtoshedlightonthe 
decision -making processes of deep learning models.  
Why DLis Growing:  
• ProcessingpowerneededforDeeplearningisreadilybecomingavailable using 
GPUs, Distributed Computing and powerful CPUs.  
• Moreover,asthedataamountgrows,DeepLearningmodelsseemto outperform 
Machine Learning models.  
• Focusoncustomizationandrealtime  decision.  
• Uncover patterns that is hard to detect using traditional techniques. Find latent 
features (super variables) without signif icant manual feature engineering.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
Processin  ML/DL:  
 
ArtificialNeural Networks:  
 
Artificial Neural Networks contain artificial neurons which are called units . These 
units are arranged in a series of layers that together constitute the whole Artificial Neural 
Network in a system.  
 
A layer can have only a dozen units or millions of units as this depends on how the 
complex neural networks will be required to learn the hidden patterns in the dataset. 
Commonly, Artificial Neur al Network has an input layer, an output layer as well as hidden 
layers.  
 
The input layer receives data from the outside world which the neural network needs 
to analyze or learn about. Then this data passes through one or multiple hidden layers that 
transf orm the input into data that is valuable for the output layer. Finally, the output layer 
provides an output in the form of a response of the Artificial Neural Networks to input data 
provided.  
 
In the majority of neural networks, units are interconnected fr om one layer toanother. 
Each of these connections has weights that determine the influence of one unit on another 
unit. As the data transfers from one unit to another, the neural network learns more and more 
about the data which eventually results in an ou tput from the output layer.  

B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
 
Thestructuresandoperationsofhumanneuronsserveasthebasisforartificial neural 
networks. It is also known as neural networks or neural nets. The input layer of an artificial 
neural network is the first layer , and it receives input from external sources and releases it to 
the hidden layer, which is the second layer. In the hidden layer, each neuron receives input 
from the previous layer neurons, computes the weighted sum, and sends it to the neurons in 
the nex t layer.  
These connections are weighted means effects of the inputs from the previous layer 
are optimized more or less by assigning different -different weights to each input and it is 
adjusted during the training process by optimizing these weights for improved model 
performance.  
ArtificialneuronsvsBiological neurons  
The concept of artificial neural networks comes from biological neurons found in 
animal brains So they share a lot of similarities in structure and function wise.  
• Structure : The structure of artificial neural networks is inspired by biological 
neurons. A biological neuron has a cell body or soma to process the impulses, 
dendrites to receive them, and an axon that transfers them to other neurons.The 
input nodes of artificial neural  networks receive input signals, the hidden layer 
nodes compute these input signals, and the output layer nodes compute the final 
output by processing the hidden layer’s results using activation functions.  
Biological Neuron  Artificial Neuron  
Dendrite  Inputs  

B.Tech –CSE R-20 
Deep Learning   
  
Biological Neuron  Artificial Neuron  
Cellnucleusor Soma  Nodes  
Synapses  Weights  
Axon  Output  
• Synapses : Synapses are the links between biological neurons that enable the 
transmission of impulses from dendrites to the cell body. Synapses are 
theweightsthatjointheone -layernodestothenext -layernodesinartificial neurons. The 
strength of the links is determined by the weight value.  
• Learning : In biological neurons, learning happens in the cell body nucleus or 
soma,whichhasanu cleusthathelpstoprocesstheimpulses.Anaction potential is 
produced and travels through the axons if the impulses are powerful enough to 
reach the threshold. This becomes possible by synaptic plasticity,which represents 
the ability of synapses to become stro nger or weaker over timein reaction to 
changes in their activity. In artificial neural networks, backpropagation is a 
technique used for learning, which adjusts the weights 
betweennodesaccordingtotheerror ordifferences betweenpredictedand actual 
outcomes.  
Biological Neuron  Artificial Neuron  
Synaptic plasticity  Backpropagations  
• Activation :Inbiologicalneurons,activationisthe firing rate oftheneuron which 
happens when the impulses are strong enough to reach the threshold. In artificial 
neural networks, A mathematical function known as an activation function maps 
the input to the output, and executes activations.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
 
HowdoArtificialNeuralNetworks learn?  
 
Artificial neural networks are trained using a training set. For example, suppose you  
want to teach an ANN to recognize a cat. Then it is shown thousands of different images of 
catssothatthenetworkcanlearntoidentifyacat.Oncetheneuralnetworkhasbeen trained enough 
using images of cats, then you need to check if it can identify cat images cor rectly. This is 
done by making the ANN classify the images it is provided by deciding whether they are cat 
images or not.The output obtained by the ANN is corroborated by a human -provided 
description of whether the image is a cat image or not.  
If the ANN identifies incorrectly then back -propagation  is used to adjust whatever it 
has learned during training. Backpropagation is done by fine -tuning the weights of the 
connections in ANN units based on the error rate obtained.  This process continues until the 
artificial neural network can correctly recognize a cat in an image with minimal possibleerror 
rates.  
 
WhatarethetypesofArtificialNeural Networks?  
 
• Feedforward Neural Network : The feedforward neural network is one of the 
most basic artificial neural networks. In this ANN, the data or the input provided 
travels in a single direction. It enters into the ANN through the input layer  and 
exits through the output layer while hidden layers may or may not exist. So, the 
feedforward neural network has a front -propagated wave only and usually doesnot 
have backpropagation.  
• Convolutional Neural Network : A Convolutional neural network has some 
similarities to the feed -forward neural network, where the connections between 
units have weights that determine the influence of one unit on another unit. But a 
CNN has one or more than one convolutional layer that uses a convolution 
operationontheinputandthenpassestheresultobtainedintheformofoutput to the next 
layer. CNN has applications in speech and image processing which is particularly 
useful in computer vision . 
• Modular Neural Network: A Modular Neural Network contains a collection of 
different neural networks that work independently towards obtaining the output 
withnointeractionbetweenthem.Eachofthedifferentneuralnetworks  

B.Tech –CSE R-20 
Deep Learning   
 performs a different s ub-task by obtaining unique inputs compared to other 
networks. The advantage of this modular neural network is that it breaks down a 
large and complex computational process into smaller components, thus 
decreasing its complexity while still obtaining the r equired output.  
• Radial basis function Neural Network: Radial basis functions are those 
functions that consider the distance of a point concerning the center. RBF 
functions have two layers.In the first layer, the input is mapped into all theRadial 
basis func tions in the hidden layer and then the output layer computes the output 
in the next step. Radial basis function nets are normally used to model the data 
that represents any underlying trend or function.  
• Recurrent Neural Network: The Recurrent Neural Network saves the output 
ofalayerandfeedsthisoutputbacktotheinputtobetterpredicttheoutcomeof the layer. 
The first layer in the RNN is quite similar to the feed -forward neural network and 
the recurrentneuralnetwork starts once the outputof the 
firstlayeriscomputed.Afterthislayer,eachunitwillremembersomeinformationfrom 
thepreviousstepsothatitcanactasamemorycellinperforming computations.  
 
ApplicationsofArtificialNeural Networks  
 
1. Social Media: Artificial Neural Networks are used heavily in Social Media. For 
example,let’stakethe ‘Peopleyoumayknow’ featureonFacebookthat 
suggestspeoplethatyoumightknowinreallifesothatyoucansendthem friend requests. 
Well, this magical effect is achieved by using Artificial Neural Networks that 
analyze your profile, your interests, your current friends, and also their friends and 
various othe r factors to calculate the people you mightpotentially know. Another 
common application of Machine Learning in social media is facial recognition . 
This is done by finding around 100 referenc e points on the person’s face and then 
matching them with those already available in the database using convolutional 
neural networks.  
2. Marketing and Sales: When you log onto E -commerce sites like Amazon and 
Flipkart, they will recommend your products to bu y based on your previous 
browsing history. Similarly, suppose you love Pasta, then Zomato, Swiggy, etc. 
will show you restaurant recommendations based on your tastes and previous 
orderhistory.Thisistrueacrossallnew -agemarketingsegmentslikeBook 
sites,Movies ervices,Hospitalitysites,etc.anditisdonebyimplementing personalized 
marketing . This uses Artificial Neural Networks to identify the customer likes, 
dislikes, previous shopping history, etc., and thentailor the marketing campaigns 
accordingly.  
3. Healthcare : Artificial Neural Networks are used in Oncology to train algorithms 
thatcanidentify canceroustissueatthemicroscopiclevelatthesameaccuracy as trained 
physicians. Various rare diseases may manifest in physical characteristics and can 
be identified in their pr emature stages by using Facial Analysis on the patient 
photos. So the full -scale implementation of Artificial Neural Networks in the 
healthcare environment can only enhance the diagnostic abilities of medical 
experts and ultimately lead to the overall impr ovement in the quality of medical 
care all over the world.  
4. Personal Assistants: Applications like Siri, Alexa, Cortana, etc., and also heard 
thembasedonthephonesyouhave!!!Thesearepersonalassistantsandan  
B.Tech –CSE R-20 
Deep Learning   
 exampleofspeechrecognitionthatuses NaturalLanguageProcessing to interact 
with the users and formulate a response accordingly. Natural Language Processing 
uses artificial neural networks that are made to handle many tasks of these 
personal assistants such as managing the language syntax, sem antics,correct 
speech, the conversation that is going on, etc.  
 
NeuralNetwork,Non -linearclassificationexampleusingNeural 
Networks: XOR/XNOR:  
XORproblemwithneural networks:  
Among various logical gates, the XOR or also known as the 
“exclusive or” problem is o ne of the logical operations when 
performed on binary inputs that yield output for different 
combinations of input, and for the same combination of input no 
output is produced. The outputs generated by the XOR logic are 
notlinearly separable in the hyperplane. So, in this article  let us 
see what is the XOR logic and how to integrate the XOR logic 
using neural  networks.  
From the below truth table,  it can be inferred that XOR 
produces an output for different states of inputs and for thesame 
inputs the XOR logic does not produce any output. The Output of 
XOR logic is yielded by the equation as shown below.  
 
X Y Output  
0 0 0 
0 1 1 
1 0 1 
1 1 0 
Output= X.Y’+X’.Y  
 
The XOR gate can be usually termed as a combination of NOT and 
AND gates and this type of logic finds its vast application in cryptography 
and fault tolerance. The logical diagram of an XOR gate is shown below.  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Thelinearseparabilityof points 
Linearseparabilityofpoints is the ability to classify the datapoints in 
thehyperplane  by avoiding the overlapping of the classes in the planes. 
Each of the classes should fall above or below the separating line and then 
they are termed as linearly separable data points. With respect to logical 
gates operations like AND or OR the outputs generated by this logic are 
linearly separable  in the hyperplane. The linear separable data points 
appear to be as shown below.  
 
 
 
So here we can see that the pink dots and red triangle points in the 
plot do not overlap each other and the linear line is easily separating the 
two classes where the upp er boundary of the plot can be considered as one 
classification and the below region can be considered as the other region of 
classification.  

B.Tech –CSE R-20 
Deep Learning   
 
Needforlinearseparabilityinneuralnetworks  
Linear separability is required in neural networks is required asbasic 
operations of neural networks would be in N -dimensional space and the 
data points of the neural networks have to be linearly separable to 
eradicate the issues with wrong weight updation and wrong classifications 
Linear separabi lity of data is also considered as one of the prerequisites 
which help in the easy interpretation of input spaces into points whether 
the network is positive and negative and linearly separate the data pointsin 
the hyperplane.  
 
HowtosolvetheXORproblemwithn euralnetworks:  
 
The XORproblemwith neural networkscan be solved byusing Multi - 
Layer Perceptrons or a neural network architecture with an input layer, 
hidden layer, and output layer. So during the forward propagation through 
the neural networks, the weights get updated to the corresponding layers 
and the XOR logic gets executed. The Neural network architecture to solve 
the XOR problem will be as shown below.  
 
 
 
 
 
So with th is overall architecture and certain weight parameters 
between each layer, the XOR logic output can be yielded through forward 
propagation. The overall neural network architecture uses the ReLu 
activation function to ensure the weights updated in each of th e processes  
B.Tech –CSE R-20 
Deep Learning   
 to be 1 or 0 accordingly where for the positive set of weights the output at 
the particular neuron will be 1 and for a negative weight updation at the 
particular neuron will be 0 respectively. So let us understand one outputfo r 
the first input state  
 
Example :ForX1=0andX2=0weshouldgetaninputof0.Letussolve it. 
 
Solution:   
ConsideringX1=0andX2=0 
H1=RELU(0.1+0.1+0)= 0 
H2=RELU(0.1+0.1+0)= 0 
So now we have obtained the weights that were propagated from the 
input layertothehidden layer. Now,letus propagate fromthehiddenlayer to 
the output layer.  
 
Y=RELU(0.1+0.( -2))= 0 
 
This is how multi -layer neural networks or also known as Multi - 
Layer perceptrons (MLP) are used to solve the XOR prob lem and for all 
other input sets the architecture provided above can be verified and the 
right outcome for XOR logic can be yielded.  
 
So, amongthevariouslogicaloperations,XORlogical operationisone 
such problem wherein linear separability of data points is not possible 
using single neurons or perceptrons. So, for solving the XOR problem for 
neural networks it is necessary to use multiple neurons in the neural 
network architecture with certain weights and appropriate activation 
functions to solve the XOR prob lem with neural networks.  
 
A perceptron is a neural network unit that does a precise 
computationtodetectfeaturesintheinputdata.Perceptronismainly used  
toclassifythedataintotwoparts.Therefore,itisalsoknownas Linear 
BinaryClassifier . 
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Perceptron uses the step function that returns +1 if the weightedsum of 
itsinput 0 and -1. 
The activation function is used to map the input between the required 
valuelike (0, 1) or ( -1, 1).  
Aregularneuralnetworklookslike this: 
 
 

B.Tech –CSE R-20 
Deep Learning   
 
Theperceptronconsistsof4 parts.  
o InputvalueorOneinputlayer: Theinputlayeroftheperceptronismadeof artificial 
input neurons and takes the initial data int o the system for further 
processing.  
o Weightsand Bias:  
Weight: It represents the dimension or strength of the connection between 
units.Iftheweighttonode1tonode2hasahigherquantity,thenneuron1 has a 
more considerable influence on the neuron.  
Bias: It is the same as the intercept added in a linear equation. It is an 
additionalparameterwhichtaskistomodifytheoutputalongwiththe weighted 
sum of the input to the other neuron.  
o Netsum: Itcalculatesthetotal sum. 
o ActivationFunction: Aneuroncanbeactivatedornot,isdeterminedbyan 
activation function. The activation function calculates a weighted sum and 
further adding bias with it to give the result.  
 
 
 
Astandardneuralnetworklookslikethebelow diagram.  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
How doesit work?  
Theperceptronworksonthesesimplestepswhicharegiven  below:  
a. Inthefirststep,alltheinputsxaremultipliedwiththeirweights w. 
 

B.Tech –CSE R-20 
Deep Learning   
 
b. Inthisstep,addalltheincreasedvaluesandcallthemthe Weighted sum. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
c. Inthelaststep, applytheweightedsumtoacorrect ActivationFunction . For 
Example:  
AUnitStepActivation Function,  
 
 
Therearetwotypesofarchitecture.Thesetypesfocusonthefunctionalityof artificial 
neural networks as follows - 
 
o SingleLayer Perceptron  
B.Tech –CSE R-20 
Deep Learning   
 
o Multi -Layer Perceptron  
SingleLayer Perceptron  
The single -layer perceptron was the first neural network model, proposed in 
1958 by Frank Rosenbluth. It is one of the earliest models for learning. Our goal is to 
find a linear decision function measured by the weight vector w and the bias 
parameter b.  
To understand the perceptron layer, it is necessary to comprehend artificial 
neural networks (ANNs). The artificial neural network (ANN) is an information 
processing system, whose mechanism is inspired by the funct ionality of biological 
neural circuits. An artificial neural network consists of several processing units thatare 
interconnected.  
This is the first proposal when the neural model is built. The content of the 
neuron's local memory contains a vector of weigh t. The single vector perceptron is 
calculatedbycalculatingthesumoftheinputvectormultipliedbythecorresponding 
element of the vector, with each increasing the amount of the corresponding 
component of the vector by weight. The value that is displayed in the o utput is the 
input of an activation function.  
Let us focus on the implementation of a single -layer perceptron for an image 
classification problem using TensorFlow. The best example of drawing a single -layer 
perceptron is through the representation of " logistic regression ." 
 
Now,wehavetodothefollowingnecessary stepsoftraininglogistic regression - 
 
o The weights are initialized with the random values at the origination of 
each training.  
B.Tech –CSE R-20 
Deep Learning   
 
o For each element of the training set, the error is calculated with the difference 
between the desired output and the actual output. The calculated error isused 
to adjust the weight.  
o The process is repeated until the fault made on the entire training set is less 
than the specified limit until the maximum nu mber of iterations has been 
reached.  
we will understand the concept of a multi -layer perceptron and its 
implementation in Python using the TensorFlow library.  
Multi -layer Perceptron:  
Multi -layerperceptionis  alsoknownas MLP.Itis fully connecteddense layers, 
which transform any input dimension to the desired dimension. A multi -layer 
perception is a neural network that has multiple layers. To create a neural network, 
we combine neurons together so that the outp uts of some neurons are inputs of 
other neurons.  
Agentleintroductionto neuralnetworks&TensorFlow canbefound here:  
• Neural Networks  
• Introductionto TensorFlow  
 
A multi -layer perceptron has one input layer and for eac h input, there is one 
neuron (or node), it has one output layer with a single node for each output andit 
can have any number of hidden layers and each hidden layer can have any 
numberofnodes.AschematicdiagramofaMulti -LayerPerceptron(MLP)isdepicted below.  
 
In the multi -layer perceptron diagram above, we can see that there are three inputsand 
thus three input nodes and the hidden layer has three nodes. The output layer gives two 
outputs, therefore there are two output nodes. The nodes in the input layer take  input and 
forward it for further process, in the diagram above the nodes in the input layer forwardstheir 
output to each of the three nodes in the hidden layer, and in the same way, the hidden layer 
processes the information and passes it to the output la yer. 
B.Tech –CSE R-20 
Deep Learning   
 Every node in the multi -layer perception uses a sigmoid activation function. The 
sigmoidactivationfunctiontakesrealvaluesasinputandconvertsthemtonumbers between 0 and 1 
using the sigmoid formula.  
FeedForward Network:  
Whyareneuralnetworks used?  
 
Neuronal networks can theoretically estimate any function, regardless of its 
complexity. Supervised learning is a method of determining the correct Y for a fresh X by 
learning a function that translates a given X into a specifie d Y. But what are the differences 
between neural networks and other methods of machine learning? The answer is based on the 
Inductive Bias phenomenon, a psychological phenomenon.  
 
Machine learning models are built on assumptions such as the one where X and  Y are 
related. An Inductive Bias of linear regression is the linear relationship between X and Y. In 
this way, a line or hyperplane gets fitted to the data.  
 
When X and Y have a complex relationship, it can get difficult for a LinearRegression 
method to predict Y. For this situation, the curve must be multi -dimensional or approximate 
to the relationship.  
 
A manual adjustment is needed sometimes based on the complexity of the function 
and thenumberoflayers within thenetwork. In m ost cases, trialand error methods combined 
with experience get used to accomplishingthis. Hence, this is the reason these parameters are 
called hyperparameters . 
 
Whatisa feedforwardneural network?  
 
Feed forward neural networks are artificial neural networks in which nodes do not 
form loops. This type of neural network is also known as a multi -layer neural network as all 
information is only passed forward.  
 
During data flow, input nodes receive data, which travel through hidden layers, and 
exit output nodes. Nolinks exist in the network that could get used to bysending information 
back from the output node.  
 
Afeed forwardneuralnetworkapproximatesfun ctionsinthefollowing way:  
 
• Analgorithm calculatesclassifiers byusingthe formulay=f* (x). 
• Inputxisthereforeassignedtocategory y. 
• According to the feed forward model, y = f (x; θ). This value determines the 
closestapproximation of the function.  
 
Feed forward neural networks serve as the basis for object detection in photos, as 
shown in the Google Photos app.  
B.Tech –CSE R-20 
Deep Learning   
 
Whatistheworkingprincipleofafeedforwardneural network?  
 
When the feed forward neural network gets simplified, it can appear as a  single layer 
perceptron.  
 
This model multiplies inputs with weights as they enter the layer. Afterward, the 
weighted input values get added together to get the sum. As long as the sum of the values 
rises aboveacertain threshold, set at zero,theoutput valu eis usually1, whileifit falls below the 
threshold, it is usually -1. 
 
As a feed forward neural network model, the single -layer perceptron often gets used 
for classification. Machine learning can also get integrated into single -layer perceptrons. 
Through tr aining, neural networks can adjust their weights based on a property called the 
delta rule, which helps them compare their outputs with the intended values.  
 
As a result of training and learning, gradient descent occurs. Similarly, multi -layered 
perceptron s update their weights. But, this process gets known as back -propagation. If this is 
the case, the network's hidden layers will get adjusted according to the output valuesproduced 
by the final layer.  
 
Layersof feedforwardneural network  
 
B.Tech –CSE R-20 
Deep Learning   
 • Input layer:  
 
The neurons of this layer receive input and pass it on to the other layers of the 
network. Feature or attribute numbers in the dataset must match the number of 
neurons in the input layer.  
 
• Output layer:  
 
According to the type of model getting b uilt, this layer represents the forecasted 
feature.  
 
• Hidden layer:  
 
Input and output layers get separated by hidden layers. Depending on the type of 
model, there may be several hidden layers.  
 
There are several neurons in hidden layers that transform the input beforeactually 
transferring it to the next layer. This network gets constantly updated with weights in 
order to make it easier to predict.  
 
• Neuron weights:  
 
Neurons get connected by a weight, which measures their strength or magnitude. 
Similartolinearregression coefficients,inputweightscanalsogetcompared. Weight is 
normally between 0 and 1, with a value between 0 and 1.  
 
• Neurons:  
 
Artificial neurons get used in feed forward networks, which later get adapted from 
biological neurons. A neur al network consists of artificial neurons. Neurons functionin 
two ways: first, they create weighted input sums, and second, they activate the sums 
to make them normal.  
 
Activation functions can either be linear or nonlinear. Neurons have weights 
based on t heir inputs. During the learning phase, the network studies these weights.  
 
 
 
• Activation Function:  
 
Neurons are responsible for making decisions in this area. According to the 
activation function, the neurons determine whether to make a linear or nonlinear 
decision. Since it passes through so many layers, it prevents the cascading effect 
from increasing neuron outputs.  
 
An activation function can be classified into three major categories: sigmoid, 
Tanh, and Rectified Linear Unit (ReLu).  
 
a) Sigmoid:  
B.Tech –CSE R-20 
Deep Learning   
 
Input values between0and1getmappedtotheoutput values.  
 
b) Tanh:  
 
A valuebetween -1and 1getsmappedto theinput values.  
 
c) RectifiedLinear Unit:  
 
Onlypositivevaluesareallowedtoflowthroughthisfunction.Negative 
values get mapped to 0.  
 
Functioninfeedforwardneural network:  
 
 
Cost function  
 
In a feed forward neural network, the cost function plays an important role.The 
categorized data points are little  affected by minor adjustments to weights and 
biases. Thus, a smooth cost function can get used to determine a method ofadjusting 
weights and biases to improve performance.  
 
Followingisa definitionofthemeansquareerrorcost function:  
 
 
 
Where,  
 
w=theweightsg atheredinthenetwork b = 
biases  
n= numberofinputsfor training  
B.Tech –CSE R-20 
Deep Learning   
 
a=outputvectors x 
= input  
‖v‖=vectorv'snormal length  
 
Loss function  
 
The loss function of a neural network gets used to determine if an adjustment 
needs to be made in the learning process.  
 
Neurons in the output layer are equal to the number of classes. Showing the 
differences between predicted and actual probability distributions. Following is the 
cross -entropy loss for binary classification.  
 
 
 
 
 
Asa resultofmulticlassca tegorization,across -entropyloss  occurs:  
 
 
 
Gradientlearning  algorithm  
 
In the gradientdescentalgorithm, the next point gets calculatedbyscaling the 
gradient at the current position by a learning rate. Then subtracted from the current 
position by the achieved value.  
 
To decrease the function, it subtracts the value (to increase, it would add). As 
an example, here is how to write this procedure:  
 
 
The gradient gets adjusted by the parameter η, which also determines thestep 
size. Performance is signific antly affected by the learning rate in machine learning.  
B.Tech –CSE R-20 
Deep Learning   
 
Output  units  
 
In the output layer, output units are those units that provide the desired output 
or prediction, thereby fulfilling the task that the neural network needs to complete.  
 
There is a close relationship between the choice of output units and the cost 
function. Any unit that can serve as a hidden unit can also serve as an output unit ina 
neural network.  
 
AdvantagesoffeedforwardNeural Networks  
• Machinelearningcanbeboostedwithfeedforwardneuralnetworks'simplified 
architecture.  
• Multi -networkinthefeedforwardnetworksoperateindependently,witha 
moderated intermediary.  
• Complextasksneedseveralneuronsinthe network.  
• Neural networks can handle and process non linear data easily comparedto 
perceptrons and sigmoid neurons, which are otherwise complex.  
• A neural network deals with the complicated problem of decision 
boundaries.  
• Depending on the data, the neural network architecture can vary. For 
example, convolutio nal neural networks (CNNs) perform exceptionally 
well in image processing, whereas Recurrent Neural Networks (RNNs) 
perform well in text and voice processing.  
• Neural networks ne ed Graphics Processing Units (GPUs) to handle large 
datasets for massive computational and hardware performance. Several 
GPUs get used widely in the market, including Kaggle Notebooks and 
Google Collab Notebooks.  
 
Applicationsoffeedforwardneural networks:  
 
 
Therearemanyapplicationsfortheseneuralnetworks.Thefollowingareafewof them.  
B.Tech –CSE R-20 
Deep Learning   
 
A) Physiologicalfeedforward system  
 
Itispossibletoidentifyfeedforwardmanagementinthissituationbecausethecentral involuntary 
regulates the heartbeat before exercise.  
 
B) Generegulationandfeed forward  
 
Detectingnon -temporarychangestotheatmosphereisafunctionofthismotifasafeed forward 
system. You can find the majority of this pattern in the illustrious networks.  
 
C) Automationandmachine management  
 
Automationcontrolusin gfeedforwardisoneofthedisciplinesin automation.  
 
D) Parallelfeedforwardcompensationwith derivative  
 
An open -loop transfer converts non -minimum part systems into minimum part systems using 
this technique.  
 
Understandingthemathbehindneural networks  
 
Typical deep learning algorithms are neural networks (NNs). As a result of their 
unique structure, their popularity results from their 'deep' understanding of data.  
 
Furthermore, NNs are flexible in terms of comple xity and structure. Despite all the 
advanced stuff, they can't work without the basic elements: they may work better with the 
advanced stuff, but the underlying structure remains the same.  
 
 
 
 
 
DeepFeed -forward networks:  
 
NNsget constructed similarlyto ourb iologicalneurons, and theyresemble the 
following:  
 
B.Tech –CSE R-20 
Deep Learning   
 
Neurons are hexagons in this image. In neural networks, neurons getarranged 
into layers: input is the first layer, and output is the last with the hiddenlayer in the 
middle.  
 
NN consists of two main elements that compute mathematical operations. 
Neurons calculate weighted sumsusinginput dataandsynaptic weights sinceneural 
networks are just mathematical computations based on synaptic links.  
 
Thefollowingisasimplified visualization:  
 
 
Ina matrixformat,itlooks as follows:  
 
 
Inthe third step,avectorofonesgetsmultipliedbytheoutput of our hidden  
layer:  
 
 
 
 
 
 
 
Using the output value, we can calculate the result. Understanding these 
fundamental concepts will make building NN much easier, and you will be amazed at 
how quickly you can do it. Every layer's output becomes the following layer's input.  
B.Tech –CSE R-20 
Deep Learning   
 
Thearchitectureofthe network:  
 
In a network, the architecture refers to the number of hidden layers and unitsin 
each layer that make up the network.A feed forward network based on the Universal 
Approximation Theorem must have a "squashing" activation function at least on one 
hidden layer.  
 
The network can approximate any Borel  measurable function within a finite - 
dimensional space with at least some amount of non -zero error when there are 
enough hidden units. It simply states that we can always represent any functionusing 
the multi -layer perceptron (MLP), regardless of what fun ction we try to learn.  
 
Thus, we now know there will always be an MLP to solve our problem, but 
there is no specific method for finding it. It is impossible to say whether it will be 
possible to solve the given problem if we use N layers with M hidden unit s. 
 
Research is still ongoing, and for now, the only way to determine this 
configuration is by experimenting with it. While it is challenging to find theappropriate 
architecture, we need to try many configurations before finding the one that can 
represent the target function.  
 
There are two possible explanationsfor this.Firstly, the optimization algorithm 
may not find the correct parameters, and secondly, the training algorithms may use 
the wrong function because of overfitting.  
 
Whatisbackpropagationinfeedforwardneural network?  
 
Backpropagation is a technique based on gradient descent. Each stage of a 
gradient descent process involves iteratively moving a function in the opposite 
direction of its gradient (the slope).  
 
The goal is to reduce the cost function given the training data while learning a 
neural network. Network weights and biases of all neurons in each layer determine 
the cost function. Backpropagation gets used to calculate the gradient of the cost 
function iteratively. And then update weights and biases in the opposite direction to 
reduce the gradient.  
 
We must define the error of the backpropagation formula to specify ith neuron 
in the ith layer of a network for the j -th training. Example as follows (in which 
represents the weighted input to the neuron, and L represents the loss.)  
 
 
In backpropagationformulas,theerrorisdefinedas above:  

B.Tech –CSE R-20 
Deep Learning   
 
Below is the full derivation of the formulas. For each formula below, L stands 
for the output layer, g for the activation function, ∇ the gradient, W[l]T layer l weights 
transposed.  
 
A proportional activation of neuron i at layer l based on b li bias from l ayer i to 
layer i, w[k] weight from layer l to layer l -1, and ak−1activation of neuron k at layer l -1 
for training example j.  
 
 
 
The first equation shows how to calculate the error at the output layer for 
sample j. Following that, we can use the second equation to calculate the error in the 
layer just before the output layer.  
 
Based on the error values for the next layer, the second equation cancalculate 
the error in any layer. Because this algorithm calculate s errors backward, it is known 
as backpropagation. For sample j, we calculate the gradient of the loss function by 
taking the third and fourth equations and dividing them by the biases and weights.  
 
Wecan update biasesand weights by averaging gradients of the lossfunction 
relative to biases and weights for all samples using the average gradients. The 
process is known as batch gradient descent. We will have to wait a long time if we 
have too many samples.  
 
If each sample has a gradient, it is possible to upd ate the biases/weights 
accordingly. The process is known as stochastic gradient descent. Even though this 
algorithm is faster than batch gradient descent, it does not yield a good estimate of 
the gradient calculated using a single sample.  
 
It is possible to update biases and weights based on the average gradients of 
batches. It gets referred to as mini -batch gradient descent and gets preferred overthe 
other two.  
 
StochasticGradientDescent (SGD):  
Gradient Descent is an iterative optimization process  that searches for an objective 
function’soptimumvalue(Minimum/Maximum).Itisoneofthemostusedmethodsfor  
B.Tech –CSE R-20 
Deep Learning   
 changing a model’s parameters in order to reduce a cost function in machine learning projects.  
Theprimarygoalofgradientdescentistoidentifythemodelparametersthat provide the 
maximum accuracy on both training and test datasets. In gradient descent, the gradient is a 
vector pointing in the general direction of the function’s steepest rise at a 
particul arpoint.Thealgorithmmightgraduallydroptowardslowervaluesofthefunction by moving 
in the opposite direction of the gradient, until reaching the minimum of the function.  
TypesofGradient Descent:  
Typically,therearethreetypesofGradient Descent:  
1. BatchGradient Descent  
2. StochasticGradient Descent  
3. Mini -batch Gradient Descent  
 
1. StochasticGradientDescent (SGD):  
Stochastic Gradient Descent(SGD) isa variant of the GradientDescent algorithm that is 
used for optimizing machine learning models. It addresses the computational inefficiency of 
traditional Gradient Descent methods when dealing with large datasets in machine learning 
projects.  
In SGD, instead of using the entire dataset for each iteration, only a single random 
trainingexample(orasmallbatch)isselectedtocalculatethegradientandupdatethe model 
parameters. This ra ndom selection introduces randomness into the optimization process, 
hence the term “stochastic” in stochastic Gradient Descent.  
TheadvantageofusingSGDisitscomputationalefficiency,especiallywhen dealing with 
large datasets. By using a single example or a sm all batch, the computational cost per 
iteration is significantly reduced compared to traditional Gradient Descent methods that 
require processing the entire dataset.  
StochasticGradientDescent Algorithm:  
• Initialization :Randomlyinitializetheparametersofthe model. 
• SetParameters :Determinethenumberofiterationsandthelearningrate (alpha) for 
updating the parameters.  
• Stochastic Gradient Descent Loop : Repeat the following steps until the model 
converges or reaches the maximum number of iterations:  
a. Shufflethetrainingda tasettointroduce randomness.  
b. Iterateovereachtrainingexample(orasmallbatch)intheshuffled order.  
c. Computethegradientofthecostfunctionwithrespecttothemodel parameters 
using the current training example (or batch).  
d. Update the model parameters by taking a step in the direction of the 
negativegradient, scaled by the learning rate.  
e. Evaluate the convergence criteria, such as the difference in the cost function 
between iterations of the gradient.  
• ReturnOptimizedParameters :Oncetheconvergencecriteriaare met 
orthemaximu mnumberofiterationsisreached,returntheoptimizedmodel parameters.  
B.Tech –CSE R-20 
Deep Learning   
 In SGD, since only one sample from the dataset is chosen at random for eachiteration, 
the path taken by the algorithm to reach the minima is usually noisier than your typica l 
Gradient Descent algorithm. But that doesn’t matter all that much because the path taken by 
the algorithm does not matter, as long as we reach the minimum and with a significantly 
shorter training time.  
Hidden Units:  
 
Inneural networks,  a hidden layer is located between the input and output of the 
algorithm,inwhichthefunctionappliesweightstotheinputsanddirectsthemthrough an activation 
function  as the output. In short, the hidden layers perform nonlinear transformations of the 
inputs entered into the network. Hidden layers vary depending on the fu nction of the neural 
network, and similarly, the layers may vary depending on their associated weights.  
 
HowdoesaHiddenLayer work?  
 
Hidden layers, simply put, are layers of mathematical functions each designed to 
produce an output specific to an intended re sult. For example, some forms of hidden layers 
are known as squashing functions. These functions are particularly useful when the intended 
output of the algorithm is a probability because they take an input and produce an output value 
between 0 and 1, the range for defining probability.  
 
Hidden layers allow for the function of a neural network to be broken down into 
specific transformations of the data. Each hidden layer fun ction is specialized to produce a 
defined output. For example, a hidden layer functions that are used to identify human eyes 
and ears may be used in conjunction by subsequent layers to identify faces in images. While 
the functions to identify eyes alone ar e not enough to independently recognize objects, they 
can function jointly within a neural network.  
HiddenLayersandMachine  Learning:  
Hidden layers are very common in neural networks, however their use andarchitecture 
often vary from case to case. As refere nced above, hidden layers can beseparated by their 
functional characteristics. For example, in a CNN used for object recognition, a hidden layer 
that is used to identify wheels cannot solely identify a car, however when placed in 
conjunction with additiona l layers used to identify windows, a large metallic body, and 
headlights, the neural network can then make predictions and identify possible cars within 
visual data.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
 
ChoosingHidden  Layers  
1. Wellifthedataislinearlyseparablethen youdon'tneedanyhidden layers 
at all.  
 
2. If data is less complex and is having fewer dimensions or 
featuresthen neural networks with 1 to 2 hidden layers would work.  
 
3. Ifdataishavinglargedimensionsorfeaturesthentogetan optimum 
solution, 3 to 5 hidden layers can be used.  
It should be kept in mind that increasing hidden layers would also 
increase the complexity of the model and choosing hidden layers such as 8, 9, 
or in two digits may sometimes lead to overfitting.  
Choosin gNodesinHidden  Layers  
Once hidden layers have been decided the next task is to choose the 
number of nodes in each hidden layer.  
 
1. The number of hidden neurons should be between the size of 
theinput layer and the output layer.  
 
2. Themostappropriatenumberofhiddenneurons is 

B.Tech –CSE R-20 
Deep Learning   
 
Sqrt(inputlayernodes*outputlayer nodes)  
 
3. The number of hidden neurons should keep on decreasing in 
subsequent layers to get more and more close to pattern and 
feature extraction and to identify the  target class.  
The above algorithms are only a general use case and they can be 
moulded according to use case.Sometimes the number of nodes in hidden 
layers can increase also in subsequent layers and the number of hidden layers 
can also be more than the id eal case.  
This whole depends upon the use case and problem statement that we 
are dealing with.  
Architecture Design:  
Typesofneuralnetworksmodelsarelisted below:  
 
• Perceptron  
• FeedForwardNeural  Network  
• Multilayer Perceptron  
• ConvolutionalNeural Network  
• RadialBasisFunctionalNeural Network  
• RecurrentNeural Network  
• LSTM – LongShort -Term  Memory  
• SequencetoSequence Models  
• ModularNeural  Network  
 
B.Tech –CSE R-20 
Deep Learning   
 
AnIntroductiontoArtificialNeural Network  
 
Neuralnetworksrepresent deeplearning using artificialintelligence .Certain application 
scenarios are too heavy or out of scope for traditional machine learningalgorithms to handle. 
As they are commonly known, Neural Network  pitches in such scenarios and fills the gap. 
Also, enroll in the neural networks and deep learning course and enhance your skills today.  
 
Artificial neural networks are inspire d by the biological neurons within the human 
body which activate under certain circumstances resulting in a related action performed bythe 
body in response. Artificial neural nets consist of various layers of interconnected artificial 
neurons powered by ac tivation functions that help in switching them ON/OFF. Like 
traditional machine algorithms , here too, there are certain values that neural nets learn in the 
training phase.  
 
Briefly, each neuron receives a multiplied version of inputs and random weights, 
which is then added with a static bias value (unique to each neuron layer); this is then passed 
to an appropriate activation function which decides the final v alue to be given out of the 
neuron. There are various activation functions available as per the nature of input values. 
Once the output is generated from the final neural net layer, loss function (input vs output) is 
calculated,andbackpropagationisperforme dwheretheweightsareadjustedtomaketheloss 
minimum. Finding optimal values of weights is what the overall operation focuses around. 
Please refer to the following for better understanding.  
 
 
Weights are numeric values that are multiplied by inputs. In backpropagation, they are 
modified to reduce the loss. In simple words, weights are machine learned values fromNeural 
Networks. They self -adjust depending on the difference between predicted outputs vs training
 inputs.  
ActivationFunction isamathematicalform ulathathelpstheneurontoswitch ON/OFF.  
B.Tech –CSE R-20 
Deep Learning   
  
 
• Inputlayer representsdimensionsoftheinput vector.  
• Hidden layer represents the intermediary nodes that divide the input space into 
regions with (soft) boundaries. It takes in a set of weighted input and produces 
output through an activation function.  
• Outputlayer representstheoutputoftheneural  network.  
 
Backpropagation:  
BackpropagationProcessinDeepNeural Network:  
 
Backpropagation is one of the important concepts of a neural network. Our 
task is to classify our data best. For this, we have to update the weights of parameter 
and bias, but how can we do that in a deep neural network? In the linear regression 
model,weusegradientdescenttooptimizetheparameter.Similarlyherewealsouse 
gradient descent alg orithm using Backpropagation.  
For a single training example, Backpropagation algorithm calculates the 
gradient of the error function . Backpropagation can be written as a function of the 
neural network. Backpropagation algorithms are a set of methods used to efficiently 
train artificial neural networks following a gradient descent approach which exploits 
the chain rule.  
The main features of Backpropagation are the iterative, recursive and efficient 
method through which it calculates theupdated weight to improv e the network until 
it is not able to perform the task for which it is being trained. Derivatives of the 
activation function to be known at network design time is required to 
Backpropagation.  
Now, how error function is used in Backpropagation and howBackpr opagation 
works? Let start with an example and do it mathematically to understand how exactly 
updates the weight using Backpropagation.  

B.Tech –CSE R-20 
Deep Learning   
  
 
Input values  
X1=0.05 
X2=0.10  
Initial weight  
W1=0.1  W5=0.40  
W2=0.20  W6=0.45  
W3=0.25  W7=0.50  
W4=0.30  W8=0.55  
Bias Values  
b1=0.35  b2=0.60  
Target Values  
T1=0.01 
T2=0.99  
Now,wefirstcalculatethevaluesofH1andH2byaforward pass.  
Forward Pass  
TofindthevalueofH1wefirstmultiplytheinputvaluefromtheweights as 

B.Tech –CSE R-20 
Deep Learning   
 
H1=x1×w 1+x2×w 2+b1 
H1=0.05×0.15+0.10×0.20+0.3  
H1=0.3775  
 
TocalculatethefinalresultofH1,weperformedthesigmoid function as 
 
 
WewillcalculatethevalueofH2in thesamewayas H1 
H2=x1×w 3+x2×w 4+b1 
H2=0.05×0.25+0.10×0.30+0.35 
H2=0.3925  
TocalculatethefinalresultofH1,weperformedthesigmoid function as 
 
 
 
Now, we calculate thevalues of y1 and y2 inthe same way as we calculate the 
H1 and H2. To find the value of y1, we first multiply the input value i.e., the outcome 
of H1 and H2 from the wei ghts as  
B.Tech –CSE R-20 
Deep Learning   
 
y1=H1×w 5+H2×w 6+b2 
y1=0.593269992×0.40+0.596884378×0.45+0.60 
y1=1.10590597  
Tocalculatethefinalresultofy1weperformedthesigmoidfunction as 
 
 
Wewill calculatethevalueofy2 in thesame wayas y1 
y2=H1×w 7+H2×w 8+b2 
y2=0.593269992×0.50+0.596884378×0.55+0.60 
y2=1.2249214  
TocalculatethefinalresultofH1,weperformedthesigmoid function as 
 
 
 
Our target values are 0.01 and 0.99. Our y1 and y2 value is not matched with 
our target values T1 and T2. Now,  we will find the total error , which is simply the 
difference between the outputs from the target outputs. The total error is calculated 
as 
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
So,thetotalerror is 
 
 
 
Now,wewillbackpropagatethiserror toupdatetheweightsusinga backward  
pass. 
Backwardpassattheoutput layer  
To update the weight, we calculate the error correspond to each weight with 
the help of a total error. The error on weight w is calculated by differentiating total 
error with respect to w.  
 
 
Weperformbackwardprocesssofirstconsiderthelastweightw5 as 
 
 
From equation two, it is clear that we cannot partially differentiate it with 
respect to w5 because there is no any w 5. We split equation one into multiple terms 
so that we can easily differentiat e it with respect to w5 as  
 

B.Tech –CSE R-20 
Deep Learning   
 
 
w5as Now,wecalculateeachtermonebyonetodifferentiateE totalwithrespect to 
 
 
 
Puttingthevalueofe-yin equation (5) 
 
 

B.Tech –CSE R-20 
Deep Learning   
 
 
So, we put the values of  in equation no (3) to find 
the final result.  
 
 
Now,wewillcalculatetheupdatedweightw5 newwiththehelpofthefollowing 
formula  
 
 
In the same way, we calculate w6 new, w7 new, and w8 newand this will give us the 
following values  
w5new=0.35891648 
w6new=408666186 
w7new=0.511301270 
w8new=0.561370121  
BackwardpassatHidden layer  
Now, we will backpropagate to our hidden layer and update the weight w1, 
w2, w3, and w4 as we have done with w5, w6, w7, and w8 weights. We will calculate 
the error at w1 as  
 
 
From e quation (2), it is clear that we cannot partially differentiate it with 
respect to w1 because there is no any w1. We split equation (1) into multiple termsso 
that we can easily differentiate it with respect to w1 as  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Now,wecalculateeachtermonebyonetodifferentiateE totalwithrespect to 
w1as 
 
 
Weagain splitthisbecausethereisnoanyH1finaltermin Etoatalas 
 
 
 
willagainsplitbecauseinE1andE2thereisnoH1 term.  
Splittingisdone as 
 
 
 
Weagain Splitboth becausethereisno anyy1andy2termin E1andE2. We split 
it as 
 
 
 
Now,wefindthevalueof byputtingvaluesinequation(18)and(19)as From 
equation (18)  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Fromequation (8) 
 
 
Fromequation (19) 
 
 
Puttingthevalueofe-y2in equation (23) 

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Fromequation (21) 
 
 
Nowfromequation(16)and (17) 
 

B.Tech –CSE R-20 
Deep Learning   
 
 
Put thevalue of inequation(15) as 
 
 
Wehave weneedto figureout as 
 
 
Puttingthevalueofe-H1in equation (30) 
 
 
We calculate the partial derivative of the total net input to H1 with respect to 
w1 the same as we did for the output neuron:  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
So, we put the values of in equation (13) to find the 
final result.  
 
 
Now,wewillcalculatetheupdatedweightw1 newwiththehelpofthefollowing 
formula  
 
 
Inthesameway, wecalculatew2 new,w3 new,andw4andthiswillgiveusthe following 
values  
w1new=0.149780716 
w2new=0.19956143 
w3new=0.24975114 
w4new=0.29950229  
We have updated all the weights. We found the error 0.298371109 on the 
network when we fed forward the 0.05 and 0.1 inputs. In the first round of 
Backpropagation,thetotalerrorisdownto 0.291027924.Afterrepeatingthisprocess 
10,000,thetotalerrorisdownto0.0000351085.Atthispoint,theoutputsneurons  

B.Tech –CSE R-20 
Deep Learning   
 
generate 0.159121960 and 0.984065734 i.e., nearby our target value when we 
feedforward the 0.05 and 0.1.  
Deeplearningframeworksand libraries:  
DeepLearning Frameworks:  
Keras, TensorFlow and PyTorch are among the top three frameworks that are 
preferred by Data Scientists as well as beginners in the field of Deep Learning. 
This comparison on Keras vs TensorFlow vs PyTorch will provide you with acrisp 
knowledge about the top Deep Learning Frameworks and  help you find out which 
one is suitable for you. In this blog you will get a complete insight into the above 
three frameworks in the following sequence:  
 
• IntroductiontoKeras,TensorFlow& PyTorch  
• Comparison Factors  
• Final Verdict  
Intro duction  
Keras  
 
Keras is an open source neural network library written in Python. It is capable 
ofrunningontopofTensorFlow.Itisdesignedtoenablefastexperimentation with deep 
neural networks . 
TensorFlow  
 
TensorFlow  is an open -source software library for dataflow programming 
across a r ange of tasks. It is a symbolic math library that is used for machine 
learning applications like neural networks.  
B.Tech –CSE R-20 
Deep Learning   
 
PyTorch  
 
PyTorch is an open -source machine learning library for Python, based on 
Torch. It is used for applications such as natural language processing and was 
developed by Facebook’s AI research group.  
Comparison Factors  
All the three frameworks are related to each other and also have certain basic 
differences that distinguishes them from one another.  
 
Theparameters thatdistinguish them:  
 
• Levelof API 
• Speed  
• Architecture  
• Debugging  
• Dataset  
• Popularity  
Levelof API 
 
 
Keras is a high -level API capable ofrunning ontop of TensorFlow,CNTK and 
Theano. It has gained favor for its ease of use and syntactic simplicity,facilitating fast 
development.  
 
TensorFlow is a framework that provides both high and low level APIs. 
Pytorch, on the other hand, is a lower -level API focused on direct work with array 
expressions.Ithasgainedimmenseinterestinthelastyea r,becomingapreferred  
B.Tech –CSE R-20 
Deep Learning   
 
solutionforacademicresearch,andapplicationsofdeeplearningrequiring optimizing 
custom expressions.  
Speed  
 
 
The performance is comparatively slower inKeras whereas TensorFlow and 
PyTorch provide a similar pace which is f ast and suitable for high performance . 
Architecture  
 
Keras has a simple architecture. It is more readable and concise. Tensorflow 
on the other hand is not very easy to use even though it provid es Keras as a 
framework that makes work easier. PyTorch has a complex architecture and the 
readability is less when compared to Keras.  
B.Tech –CSE R-20 
Deep Learning   
 
Debugging  
 
 
 
In keras, there is usually very less frequent need to debug simple networks. 
But in case o f Tensorflow, it is quite difficult to perform debugging. Pytorch on the 
other hand has better debugging capabilities as compared to the other two.  
Dataset  
 
Keras is usually used for small datasets as it is comparatively slower. On the 
otherhand,TensorFlowan dPyTorchareusedfor highperformance models and large 
datasets that require fast execution.  
Popularity  
 
B.Tech –CSE R-20 
Deep Learning   
 
With the increasing demand in the field of Data Science, there has been an 
enormous growth of Deep learning technology in the industry. Wi th this, all the 
three frameworks havegained quite a lot of popularity. Keras tops the list 
followedbyTensorFlowandPyTorch.Ithasgainedimmensepopularitydueto its 
simplicity when compared to the other two.  
 
These were the parameters that distinguish all the three frameworks but there is 
no absolute answer to which one is better. The choice ultimately comes down to  
 
• Technical background  
• Requirements and 
• Ease of Use  
Final Verdict  
Now coming to the final verdict of Keras vs TensorFlow vs PyTorch let’s have 
a look at the situations that are most preferable for each one of these three deep 
learning frameworks  
 
Kerasismost suitable for: 
 
• Rapid Prototyping  
• Small Dataset  
• Multipleback -endsupp ort 
 
 
TensorFlowismostsuitable  for: 
 
• Large Dataset  
• High Performance  
• Functionality  
• Object Detection  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
PyTorchismostsuitable  for: 
 
• Flexibility  
• ShortTraining Duration  
• Debugging capabilities  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIT -II: 
CONVOLUTIONNEURALNETWORK(CNN): Introductionto CNNs  
and their applications in computer vision, CNN basic architecture, 
Activation functions -sigmoid, tanh, ReLU, Softmax layer, Types of 
pooling layers, Training of CNN in TensorFlow, various popular CNN 
architectures:VGG, GoogleNet,ResNetetc, Dropout,Normalization, 
Data augmentation  
 
IntroductiontoCNNsandtheirapplicationsincomputer vision:  
 
Deep Learning has proved to be a very powerful tool because of its 
ability to handle large amounts of data. The interest to use hidden layers has 
surpassed traditional techniques, especially in pattern recognition. One of the  

B.Tech –CSE R-20 
Deep Learning   
 
most popular deep neural networks is Convolutional  Neural Networks (also 
known as CNN or ConvNet) in deep learning, especially when it comes to 
Computer Vision applications.  
 
Sincethe1950s,the earlydaysofAI,researchershavestruggledtomake 
asystemthatcanunderstandvisualdata.Inthefollowingyears,thisfieldcam e to be 
known as Computer Vision. In 2012, computer vision took a quantum leap 
when a group of researchers from the University of Toronto developed an AI 
model that surpassed the best image recognition algorithms, and that tooby a 
large margin.  
The AI syst em, which became known as AlexNet (named after its main 
creator, Alex Krizhevsky), won the 2012 ImageNet computer vision contestwith 
an amazing 85 percent accuracy. The runner -up scored a modest 74 percent on 
the test.  
At the heart of AlexNet was Convoluti onal Neural Networks a special 
type of neural network that roughly imitates human vision.  
Backgroundof CNNs  
CNN’s were first developed and used around the 1980s. The most that a 
CNNcoulddoatthattimewasrecognizehandwrittendigits.Itwasmostlyused 
inthepostalsectorstoreadzipcodes,pincodes,etc.Theimportantthingto  
B.Tech –CSE R-20 
Deep Learning   
 
remember about any deep learning model is that it requires a large amount of 
data to train and also requir es a lot of computing resources. This was a major 
drawback for CNNs at that period and hence CNNs were only limited to the 
postal sectors and it failed to enter the world of machine learning.  
In the past few decades, Deep Learning has proved to be a very p owerful 
tool because of its ability to handle large amounts of data. The interest to use 
hidden layers has surpassed traditional techniques, especially in pattern 
recognition. One of the most popular deep neural networks is Convolutional 
Neural Networks (a lso known as CNN or ConvNet) in deep learning, especially 
when it comes to Computer Vision applications.  
 
 
Since the 1950s, the early days of AI, researchers have struggled to make a 
systemthatcan understand visualdata.In the following years, thisfield ca me to be 
known as Computer Vision. In 2012, computer vision took a quantum leap when a 
group of researchers from the University of Toronto developed an AI model that 
surpassed the best image recognition algorithms, and that too by a large margin.  
The AI sy stem, which became known as AlexNet (named after its main 
creator, Alex Krizhevsky), won the 2012 ImageNet computer vision contest withan 
amazing 85 percent accuracy. The runner -up scored a modest 74 percent on the 
test. 
B.Tech –CSE R-20 
Deep Learning   
 At the he art of AlexNet was Convolutional Neural Networks a special type 
of neural network that roughly imitates human vision. Over the years CNNs have 
become a very important part of many Computer Vision applications and hence a 
part of any computer vision course online. So let’s take a look at the workings of 
CNNs or CNN algorithm in deep learning.  
• Backgroundof CNNs  
• WhatIsa CNN?  
• Howdoesit work?  
• WhatIsaPooling Layer?  
• Limitationsof  CNNs  
 
Backgroundof CNNs  
CNN’s were first developed and used around the 1980s. The most that a 
CNN could do at that time was recognize handwritten d igits. It was mostly used in 
the postal sectors to read zip codes, pin codes, etc. The important thing to 
remember about any deep learning model is that it requires a large amount of data 
to train and also requires a lot of computing resources. This was a major drawback 
for CNNs at that period and hence CNNs were only limited to the postal sectors 
and it failed to enter the world of machine learning.  
In 2012, Alex Krizhevsky realized that it was time to bring back the branch 
of deep learning that uses multi -layered neural networks. The availability of large 
sets of data, to be more specific ImageNet datasets with millions of labeled images 
and an abundance of computing resources enabled researchers to revive CNNs.  
WhatIsa  CNN?  
In deep learning, a Convolutional Neural Network (CNN/ConvNet ) is a 
class of deep neural networks, most commonly applied toanalyze visual imagery.  
B.Tech –CSE R-20 
Deep Learning   
 
Now when we think of a neural network we think about matrix multiplications but 
that is not the case with ConvNet. It uses a special technique called Convolution. 
Now in mathematics convolution is a mathematical operation on two functionsthat 
produces a th ird function that expresses how the shape of one is modified by the 
other.  
 
Bottom line is that the ConvNet role to reduce the images into a form 
thatiseasiertoprocess,withoutlosingfeatures crucialforgoodprediction.  
Howdoesit work?  
Before we go to the work ing of CNN’s let’s cover the basics such as 
what is an image and how is it represented. An RGB image is nothing but a 
matrix of pixel values having three planes whereas a grayscale image isthe 
same but it has a single plane. Take a look at this image to un derstand 
more.  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Forsimplicity,considergrayscaleimagestounderstandhow CNNs  
work.  
 
 
The above image shows what a convolution is.We take a filter/kernel 
(3×3 matrix) and apply it to the input image to get the convolved feature. 
This convolved feature is passed on to the next layer.  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
 
 
In the case of RGB color, channel take a look at this animation to 
understand its working.  
 
Convolutional neural networks are composed of multiple layers of 
artificial neurons. Arti ficial neurons, a rough imitation of their biological 
counterparts, are mathematical functions that calculate the weighted 
sumofmultipleinputsandoutputsanactivationvalue.Whenyouinput an 

B.Tech –CSE R-20 
Deep Learning   
 
image in a ConvNet, each layer generates several acti vation functions that 
are passed on to the next layer.  
The first layer usually extracts basic features such as horizontal or 
diagonal edges. This output is passed on to the next layer which detects 
more complex features such as corners or combinational edg es. As we 
move deeper into the network it can identify even more complex features 
such as objects, faces, etc.  
 
 
Based on the activation map of the final convolution layer, the 
classificationlayeroutputsasetofconfidencescores(valuesbetween0  
B.Tech –CSE R-20 
Deep Learning   
 
and 1) that specify how likely the image is to belong to a “class.” For 
instance, if you have a ConvNet that detects cats, dogs, and horses, the 
output of the final layer is the possibility that the input image contains anyof 
those animals.  
 
 
 
WhatIsaPooling Layer?  
Similar to the Convolutional Layer, the Pooling layer is responsiblefor 
reducing the spatial size of the Convolved Feature. This is to decrease the 
computational po wer required to process the data by reducing the 
dimensions. There are two types of pooling average pooling and max 
pooling.  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
 
 
In Max Pooling, the maximum value of a pixel from a portion of the 
imagecoveredbythekernelisfoundout.MaxPoolingalsoperformsas a Noise 
Suppressant . It discards the noisy activations altogether and also performs 
de-noising along with dimensionality reduction.  
On the other hand, Average Pooling returns the average of all the 
values from the portion of the image covered by the Kernel. Average 
Pooling simply performs dimensionality reduction as a noise suppressing 
mechanism. Hence, we can sa y that Max Pooling performs a lot better 
than Average Pooling . 
 

B.Tech –CSE R-20 
Deep Learning   
 BenefitsofUsingCNNsforMachineandDeep Learning  
 
Deep learning is a form of machine learning that requires a neural network with a minimum of 
three layers. Networks with multip le layers are more accurate than single -layer networks. Deep learning 
applications often use CNNs or RNNs (recurrent neural networks).  
 
The CNN architecture is especially useful for image recognition and image classification, as well 
as other computer visi on tasks becausetheycan processlarge amounts of data andproducehighlyaccurate 
predictions. CNNs can learn the features of an object through multiple iterations, eliminating the need for 
manual feature engineering tasks like feature extraction.  
 
It is possi ble to retrain a CNN for a new recognition task or build a new model based on an 
existing network with trained weights. This is known as transfer learning. This enables ML model 
developers to apply CNNs to different use cases without starting from scratch.  
 
WhatAreConvolutionalNeuralNetworks (CNNs)?  
 
A Convolutional Neural Network (CNN) is a type of deep learning algorithm specificallydesigned 
for image processing and recognition tasks. Compared to alternative classification models, CNNs require 
less preprocessing as they can automatically learn hierarchical feature representations from raw 
inputimages.Theyexcelat assigningimportanceto variousobjectsandfeatureswithintheimagesthrough 
convolutional layers, which apply filters to detect local patterns.  
 
The connectivity pattern in CNNs is inspired by the visual cortex in the human brain, where 
neurons respond to specific regions or receptive fields in the visual space. This architecture enables CNNs 
to effectively capture spatial relationships and patterns  in images. By stacking multiple convolutional and 
pooling layers,CNNscanlearn increasinglycomplex features, leading tohigh accuracyin taskslike image 
classification, object detection, and segmentation.  
 
 
ConvolutionalNeuralNetworkArchitecture Model  
 
Convol utional neural networks are known for their superiority over other artificial neural 
networks, given their ability to process visual, textual, and audio data. The CNN architecture comprises 
three main layers: convolutional layers, pooling layers, and a ful ly connected (FC) layer.  
 
There can be multiple convolutional and pooling layers. The more layers in the network, the 
greaterthecomplexityand(theoretically)theaccuracyofthemachinelearningmodel.Each additional  
B.Tech –CSE R-20 
Deep Learning   
 layerthatprocessestheinputdataincreasesthemodel’sabilitytorecognizeobjectsandpatternsinthe data. 
 
 
TheConvolutional  Layer  
 
 
Convolutional layers are the key building block of the network, where most of the computations 
are carried out. It works by applying  a filter to the input data to identify features. This filter, known as a 
feature detector, checks the image input’s receptive fields for a given feature. This operation is referred to 
as convolution.  
 
The filter is a two -dimensional array of weights that represents part of a 2 -dimensional image. A 
filter is typically a 3×3 matrix, although there are other possible sizes. The filter is applied to a region 
withintheinput imageandcalculatesadotproductbetweenthe pixels,whichisfedto anoutputarray.The filter 
then shifts and repeats the process until it has covered the whole image. The final output of all the filter 
processes is called the feature map.  
 
The CNN typically applies the ReLU (Rectified Linear Unit) transformation to each feature map 
after every convol ution to introduce nonlinearity to the ML model. A convolutional layer is typically 
followed by a pooling layer. Together, the convolutional and pooling layers make up a convolutional block.  
 
Additional convolution blocks will follow the first block, creati ng a hierarchical structure with 
later layers learning from the earlier layers. For example, a CNN model might train to detect cars inimages. 
Cars can be viewed as the sum of their parts, including the wheels, boot, and windscreen. Each feature of a 
car eq uates to a low -level pattern identified by the neural network, which then combines these parts to 
create a high -level pattern.  
 
 
ThePooling Layers  
 
 
A pooling or down sampling layer reduces the dimensionality of the input. Like a convolutional 
operation, pooling operations use a filter to sweep the whole input image, but it doesn’t use weights. The 
filter instead uses an aggregation function to populate the output array based on the receptive field’s values.  
 
 
Therearetwokeytypesof  pooling:  
 
• Aver agepooling: Thefiltercalculatesthereceptivefield’saveragevaluewhenitscansthe input.  
B.Tech –CSE R-20 
Deep Learning   
 • Max pooling: The filter sends the pixel with the maximum value to populate the output array.This 
approach is more common than average pooling.  
 
 
Pooling laye rs are important despite causing some information to be lost, because they help reduce the 
complexity and increase the efficiency of the CNN. It also reduces the risk of overfitting.  
 
 
TheFullyConnected(FC) Layer  
 
 
ThefinallayerofaCNNisafullyconnected layer.  
 
The FC layer performs classification tasks using the features that the previous layers and filters 
extracted. Instead of ReLu functions, the FC layer typically uses a softmax function that classifies inputs 
more appropriately and produces a probability score between 0 and 1.  
 
BasicArchitectureof CNN:  
 
Basic Architecture  
 
TherearetwomainpartstoaCNN architecture  
 
• A convolution tool that separates and identifies the various features 
of the image for analysis in a process called as Feature Extraction.  
• The network of feature extraction consists of many pairs of 
convolutional or pooling layers.  
• A fully connected layer that utilizes the output from the convolution 
process and predicts the class of the image based on th e features 
extracted in previous stages.  
• This CNN model of feature extraction aims to reduce the number of 
features present in a dataset. It creates new features which 
summarizes the existing features contained in an original set of 
features. There are many CNN layersas shown in the CNN 
architecture diagram.  
Convolution Layers  
There are three types of layers that make up the CNN which are the 
convolutionallayers,poolinglayers,andfully -connected(FC)layers.When  
B.Tech –CSE R-20 
Deep Learning   
 these layers are stacked, a CNN architecture will be formed. In addition to 
these three layers, there are two more important parameters which are the 
dropoutlayerandtheactivationfunctionwhicharedefinedbelow.  
 
 
1. Convolutional Layer  
 
This layer is the first layer that is used to extract the various features 
from the input images. In this layer, the mathematical operation 
ofconvolutionisperformedbetweentheinputimageandafilterofa 
particularsizeMxM.Byslidingthefilterovertheinputimage,thedot product is 
taken between the filter and the parts of the input image with respect to the 
size of the filter (MxM).  
 
The output is termed as the Feature map which gives us information 
about the image such as the corners and edges. Later, this feature map is fedto 
other layers to learn several oth er features of the input image.  
 
TheconvolutionlayerinCNNpassestheresulttothenextlayer 
onceapplyingtheconvolutionoperationintheinput.Convolutional 
layersinCNNbenefitalotastheyensurethespatialrelationship between the 
pixels is intact.  
 
2. Pooling Layer  
 
In most cases, a ConvolutionalLayerisfollowedbya PoolingLayer. The 
primary aim of this layer is to decrease the size of the convolved feature map 
to reduce the computational costs. This is performed by decreasing the 
connectionsbetweenlayersandindependentlyop eratesoneachfeature map. 
Depending upon method used, there are several types of Pooling operations. It 
basically summarises the features generated by a convolution layer.  
 
InMaxPooling,thelargestelementistakenfromfeaturemap. Average 
Pooling calculates the average of the elements in a predefined sized 
Imagesection.Thetotalsumoftheelementsinthepredefinedsectionis  
B.Tech –CSE R-20 
Deep Learning   
 computedinSumPooling.ThePoolingLayerusuallyservesasabridge between the 
Convolutional Layer and the FC Layer.  
 
This CNN model gener alises the features extracted by the convolution 
layer, and helps the networks to recognise the features independently. With 
the help of this, the computations are also reduced in a network.  
 
3. FullyConnected Layer  
 
TheFullyConnected(FC)layerconsistsoftheweightsandbiases along with 
the neurons and is used to connect the neurons between two different layers. 
These layers are usually placed before the output layer and form the last few 
layers of a CNN Architecture.  
 
In this, the input image from the previous layers are flattened and fedto 
the FC layer. The flattened vector then undergoes few more FC 
layerswherethemathematicalfunctionsoperationsusuallytakeplace.Inthis 
stage, the classification process begins to take place . The reason two layersare 
connected is that two fully connected layers will perform better than a single 
connected layer. These layers in CNN reduce the human supervision  
 
4. Dropout  
 
Usually, when all the features are connected to the FC layer, it 
cancauseo verfittinginthetrainingdataset.Overfittingoccurswhena 
particularmodelworkssowellonthetrainingdatacausinganegative impact in the 
model’s performance when used on a newdata.  
 
To overcome this problem, a dropout layer is utilised wherein a few 
neurons are dro pped from the neural network during training process 
resulting inreduced size of the model. On passing a dropout of0.3, 30% ofthe 
nodes are dropped out randomly from the neural network.  
 
Dropout results in improving the performance of a machine learning 
model as it prevents overfitting by making the network simpler. It drops 
neurons from the neural networks during training.  
B.Tech –CSE R-20 
Deep Learning   
 5. Activation Functions  
 
Finally, one of the most important parameters of the CNN model is the 
activation function. They are used to learn and approximate any kind of 
continuous and complex relationship between variables of the network. In 
simple words, it decides which information of the model should fire in the 
forward direction and which ones sho uld not at the end of the network.  
 
Itaddsnon -linearitytothenetwork.Thereareseveralcommonly used 
activation functions such as the ReLU, Softmax, tanH and the Sigmoid 
functions. Each of these functions have a specific usage. For a binary 
classificationCNNmo del,sigmoidandsoftmaxfunctionsarepreferredafor a 
multi -class classification, generally softmax us used. In simple terms, 
activation functions in a CNN model determine whether a neuron should be 
activatedornot.Itdecideswhethertheinputtotheworkisimportantor not to 
predict using mathematical operations.  
 
 
 
 
 
TypesofNeuralNetworks  
 
Activation  Functions  
 
Thepopularactivationfunctionsare  
 
a) BinaryStepFunction  
 
Binarystepfunctiondependsonathresholdvaluethatdecideswhether 
aneuronshouldbeactivatedornot.Theinputfedtotheactivationfunctionis 
comparedtoacertainthreshold;iftheinputisgreaterthanit,thentheneuronis 
activated,elseitisdeactivated,meaningthatitsoutputisnotpassedontothe next 
hidden layer.  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
 
Mathematically,itcanberepresentedas:  
 
 
 
 
 
 
 
Thelimitationsofbinarystep functionare asfollows:  
• Itcannotprovidemulti -valueoutputs —forexample,itcannotbeusedfor 
multi -class classificationproblems.  
• Thegradientofthestepfunctioniszero,whichcausesahindranceinthe 
backpropagation process.  

B.Tech –CSE R-20 
Deep Learning   
 
 
b) LinearActivationFunction:  
 
Thelinearactivationfunct ion,alsoknownas"noactivation,"or"identity 
function"(multipliedx1.0),iswheretheactivationisproportionaltotheinput.  
The function doesn't do anything to the weighted sum of the input, it simply 
spitsoutthevalueitwasgiven.  
 
 
 
 
Mathematically,itcanberepresente das: 
 
 
 
 
 
However,alinearactivationfunctionhas twomajorproblems:  
• It’snotpossibletousebackpropagationasthederivativeofthefunction 
isaconstantandhasnorelationtotheinputx.  
• Alllayersoftheneuralnetworkwillcollapseintooneifalinearactivation 
functionisused.Nomatterthenumberoflayersintheneuralnetwork,  
B.Tech –CSE R-20 
Deep Learning   
 
thelastlayerwillstillbealinearfunctionofthefirstlayer.So,essentially, 
alinearactivationfunctionturnstheneuralnet workintojustonelayer.  
 
Non-LinearActivationFunctions  
Thelinearactivationfunctionshownaboveissimplyalinearregression 
model.Becauseof its limited power, this does not allow the model to create 
complexmappingsbetweenthenetwork’sinputsandoutputs.  
Non-linear activation functions solve the following limitations of linear 
activation functions:  
• Theyallowbackpropagationbecausenowthederivativefunctionwould 
berelatedtotheinput,andit’spossibletogobackandunderstandwhich 
weightsintheinputneuronscanprovideabetterprediction.  
• Theyallowthestackingofmultiplelayersofneuronsastheoutputwould 
nowbeanon -linearco mbinationofinputpassedthroughmultiplelayers. 
Anyoutputcanberepresentedasafunctionalcomputationinaneural 
network.  
Belowaretendifferentnon -linearneuralnetworksactivationfunctionsand their 
characteristics.  
a) Sigmoid/LogisticActivationFunction  
 
This function tak es any real value as input and outputs values in the 
rangeof0to1.Thelargertheinput(morepositive),theclosertheoutputvalue will be 
to 1.0, whereas the smaller the input (more negative), the closer the 
outputwillbeto0.0,asshownbelow.  
 
B.Tech –CSE R-20 
Deep Learning   
 
Mathe matically,itcanberepresentedas:  
 
 
 
 
 
 
 
Here’s why sigmoid/logistic activation function is one of the most widely 
used functions:  
• Itiscommonlyusedformodelswherewehavetopredicttheprobability 
asanoutput.Sinceprobabilityofanythingexistsonlybetweentherange 
of0and1,sigmoidistherightchoicebecauseofitsrange.  
• The function is differentiable and provides a smooth gradient, i.e., 
preventingjumpsinoutputvalues.ThisisrepresentedbyanS -shapeof 
thesigmoidact ivationfunction.  
Thelimitationsofsigmoidfunctionarediscussedbelow:  
• Thederivativeofthefunctionisf'(x)=sigmoid(x)*(1 -sigmoid(x)).  
 
 
 
B.Tech –CSE R-20 
Deep Learning   
 
FromtheaboveFigure,thegradientvaluesareonlysignificantforrange  
-3 to 3, and the graph gets much flatter in other regions.It implies that for 
values greater than 3 or less than -3, the function will have very small 
gradients.Asthegradientvalueapproacheszero ,thenetworkceasestolearn 
andsuffersfromthe Vanishinggradient problem.  
• Theoutputofthelogisticfunctionisnotsymmetricaroundzero.Sothe 
outputofalltheneuronswillbeofthesamesign.Thismakesthe training ofthen
euralnetwork moredifficultandunstable.  
b) TanhFunction(HyperbolicTangent)  
Tanhfunctionisverysimilartothesigmoid/logisticactivationfunction, 
andeve nhasthesameS -shapewiththedifferenceinoutputrangeof -1to1. 
InTanh,thelargertheinput(morepositive),theclosertheoutputvaluewillbe 
to1.0,whereasthesmallertheinput(morenegative),theclosertheoutputwill be to 
-1.0. 
 
 
Mathematically,itcanberepresentedas:  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
 
 
 
 
 
Advantagesofusingthisactivationfunctionare:  
• TheoutputofthetanhactivationfunctionisZerocentered;hencewecan 
easily map the output values as strongly negative, neutral, or strongly 
positive.  
• Usually used in hidden layers of a neural network as its values lie 
between -1to;therefore,themeanforthehiddenlayercomesouttobe 
0orveryclosetoit.Ithelpsincenteringthedataandma keslearningfor the 
next layer much easier.  
 
 
It also faces the problem of vanishing gradients similar to the sigmoid 
activationfunction.Plusthegradientofthetanhfunctionismuchsteeperas 
comparedtothesigmoidfunction.  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
c) ReLU Function  
ReLUstandsforRectifiedLinearUnit.Althoughitgivesanimpressionof a 
linear function, ReLU has a derivative function and allows for 
backpropagation whilesimultaneouslymaking itcomputationallyefficient.  
ThemaincatchhereisthattheReLUfunctiondoesnotactivateallthe 
neurons at the same time.  
Theneuronswillonlybedeactivatediftheoutputofthelinear 
transformationislessthan0.  
 
 
 
 
Mathematically,itcanberepresentedas:  
 
  
Note: Althoughboth sigmoidandtanhfacevanishinggradientissue, 
tanhiszerocentered,andthegradientsarenotrestrictedtomoveina certain 
direction. Therefore, in practice, tanh nonlinearity is always preferred 
to sigmoid nonlinearity.  
B.Tech –CSE R-20 
Deep Learning   
 
Theadvantages ofusingReLUas anactivation functionareas follows:  
• Sinceonlyacertainnumberofneuronsareactivated,theReLUfunction 
isfarmorecomputationallyefficientwhencomparedtothesigmoidand tanh 
functions.  
• ReLU accelerates the convergence of gradient descent towards the 
global minimum of the loss function due to its linear, non -saturating 
property.  
Thelimitationsfacedbythisfunctionare:  
• TheDyingReLUproblem.  
 
 
 
 
The negative side of the graph makes the gradient v alue zero. Due to 
thisreason,duringthebackpropagationprocess,theweightsandbiasesfor 
someneuronsarenotupdated.Thiscancreatedeadneuronswhichneverget 
activated.  
• Allthenegativeinputvaluesbecomezeroimmediately,whichdecreases 
themodel’sabilitytofitortrainfromthe dataproperly.  
Note: ForbuildingthemostreliableMLmodels, splityourdataintotrain, validation, and test 
sets. 
B.Tech –CSE R-20 
Deep Learning   
 
d) LeakyReLU Function  
LeakyReLUisanimprovedversionofReLUfunctiontosolvetheDying 
ReLUproblemasithasasmallpositiveslopeinthenegativearea.  
 
 
Mathematically,itcanberepresentedas:  
 
 
 
 
 
TheadvantagesofLeakyReLUaresameasthatofReLU,inadditionto 
thefactthatitdoesenablebackpropagation,evenfornegativeinputvalues.By 
makingthisminormodificationf ornegativeinputvalues,thegradientoftheleft 
sideofthegraphcomesouttobeanon -zerovalue.Therefore,we wouldno 
longerencounterdeadneuronsinthatregion.  
`HereisthederivativeoftheLeakyReLU function.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
 
 
 
 
 
 
 
 
 
 
Thelimitationsthatthisfunctionfacesinclude:  
• Thepredictionsmaynotbeconsistentfornegativeinputvalues.  
• Thegradientfornegativevaluesisasmallvaluethatmakesthelearning 
ofmodelparameterstime -consuming.  
 
 
d) ParametricReLUFunction  
Parametric ReLU is another variant of ReLU that aims to solve the 
problemofgradient’sbecomingzeroforthelefthalfoftheaxis.Thisfunction 
provides the slope of the negative part of the function as an argument a. By 
performingbackpropagation,themostappropriatevalueof aislearnt.  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Mathematically,itcanberepresentedas:  
 
 
 
 
 
 
 
Where "a"is theslopeparameterfornegativevalues.  
TheparameterizedReLUfunctionisusedwhentheleakyReLUfunction 
stillfailsatsolvingtheproblemofdeadneurons,andtherelevantinformationis 
notsuccessfu llypassedtothenextlayer.  
This function’s limitation is that it may perform differently for different 
problemsdependinguponthevalueofslopeparameter a. 
 
 
 
 
Typesofpooling Layers:  

B.Tech –CSE R-20 
Deep Learning   
 AConvolutionalneuralnetwork (CNN )isaspecialtypeof ArtificialNeural Network that is 
usually used for image recognition and processing due to its ability to recognize patterns in 
images. It eliminates the need to extract featur es from visual data manually. It learns images 
by sliding a filter of some size on them and learning not just the features from the data but 
also keeps Translation invariance.  
 
Thetypicalstructureofa CNNconsistsof threebasic layers  
 
1. Convolutional layer: Thes e layers generate a feature map by sliding a filter over the input 
image and recognizing patterns in images.  
2. Poolinglayers: Theselayers downsamplethefeaturemap tointroduceTranslation invariance, 
which reduces the overfitting of the CNN model.  
3. FullyConnectedDen seLayer: Thislayercontainsthe samenumberofunitsasthenumber of 
classes and the output activation function such as “softmax” or “sigmoid”  
 
Whatare Pooling layers?  
Pooling layers are one of the building blocks of Convolutional Neural Networks. 
Where Convolutional layers extract features from images, Pooling layers consolidate the 
features learned by CNNs. Its purpose is to gradually shrink the representation’s spatial 
dimension to minimize the number of parameters and computations in the network.  
 
WhyarePoolinglayers needed?  
ThefeaturemapproducedbythefiltersofConvolutionallayersislocation -dependent. For 
example, If an object in an image has shifted a bit it might not be recognizable by the 
Convolutional layer. So, it means that the feature map records the precise positions offeatures 
in the input. What pooling layers provide is “Translational Invariance” which makes the CNN 
invariant to translations, i.e., even if the inp ut of the CNN is translated, the CNN will still be 
able to recognize the features in the input.  
 
In all cases, poolinghelps to make the representation become approximatelyinvariant 
to smalltranslations of the input. Invariance to translation means that ifwe translate the input 
by a small amount, the values of most of the pooled outputs do not change.  
 
HowdoPoolinglayersachieve  that?  
 
A Pooling layer is added after the Convolutional layer(s), as seen in the structure of a 
CNN above. It down samples the ou tput of the Convolutional layers by sliding the filter of 
some size with some stride size and calculating the maximum or average of the input.  
 
Thereare twotypesofpooling thatare  used:  
 
1. Max pooling : This works by selecting the maximum value from every pool. Max Pooling retains 
themost prominent features of the feature map, and the returned image is sharper than the original 
image.  
2. Average pooling : This pooling layer works by getting the average of the pool. Average pooling 
retains the average values of fea tures of the feature map. It smoothes the image while keeping the 
essence of the feature in an image.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
TheworkingofPoolingLayersusing TensorFlow .CreateaNumPyarray and reshape it.  
 
Max Pooling  
Create a MaxPool2D layer with pool_size=2 and s trides=2. Apply the MaxPool2D 
layer to the matrix, and you will get the MaxPooled output in the tensor form. By applying it 
tothematrix,theMax poolinglayerwillgothroughthematrix bycomputingthemax ofeach 
2×2poolwithajumpof2.Printtheshapeofthetensor.Usetf.sq ueezetoremovedimensions of size 1 
from the shape of a tensor.  
 
Average  Pooling  
Create an AveragePooling2D layer with the same 2 pool_size and strides. Apply the 
AveragePooling2Dlayer tothematrix. Byapplyingit tothematrix,theaveragepoolinglayer will 
go thro ugh the matrix by computing the average of 2×2 for each pool with a jump of 2. Print 
the shape of the matrix and Use tf.squeeze to convert the output into a readable form by 
removing all 1 size dimensions.  
 
The GIF here explains how these pooling layers go  through the input matrix and 
computes the maximum or average for max pooling and average pooling, respectively.  

B.Tech –CSE R-20 
Deep Learning   
  
 
 
GlobalPooling  Layers  
Global Pooling Layers often replace the classifier’s fully connected or Flatten layer. 
The model instead ends with a convolutional layer that produces as many feature maps as 
there are target classes and performs global average pooling on each of the feature maps to 
combine each feature map into a single value.  
 
Create the same NumPy array but w ith a different shape. By keeping the same shape 
as above, the Global Pooling layers will reduce them to one value.  
 
GlobalAverage Pooling  
 
Considering a tensor of shape h*w*n , the output of the Global Average Pooling layer 
is a single value across h*w that summarizes the presence of the feature. Instead of 
downsizingthepatchesoftheinputfeaturemap,theGlobalAveragePoolinglayerdownsizes the 
whole h*w into 1 value by taking the average.  
 
GlobalMax Pooling  
 
With the tensor of shape h*w*n , the output of the Global  Max Pooling layer is a 
single value across h*wthat summarizes the presence of a feature. Instead of downsizing the 
patchesoftheinputfeaturemap,theGlobalMaxPoolinglayerdownsizesthe whole h*w into 1 
value by taking the maximum.  
 
TrainingofCNNin TensorFlow  

B.Tech –CSE R-20 
Deep Learning   
 The MNIST database ( Modified National Institute of Standard Technology 
database ) is an extensive database of handwritten digits, which is used for training 
various image processing systems. It was created by " reintegrating " samples from 
the original dataset of the MNIST . 
If we get familiarized with the building blocks of Connects, we  can build one 
with TensorFlow. We can use the MNIST dataset for image classification.  
Preparing the data is the same as in the previous tutorial. We can run codeand 
jump directly into the architecture of CNN.  
Here, the code isexecuted in Google Colab (an online editor of machine 
learning).WecangotoTensorFloweditorthroughthebelowlink: 
https://colab.research.google.com  
Theseare thestepsusedtotrainingthe CNN.  
Steps: 
Step 1: Upload Dataset 
Step 2: The Input layer 
Step3: Convolutionallayer 
Step 4: Pooling layer  
Step5: ConvolutionallayerandPooling Layer  
Step6: Dense layer 
Step7: Logit  Layer  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
Step1:Upload Dataset  
The MNIST dataset is available with scikit for learning in this URL (Unified 
ResourceLocator).Wecandownloaditandstoreitinourdownloads.We canupload it with 
fetch_mldata ('MNIST Original').  
Createatest/train set 
Weneed tosplitthedatasetinto train_test_split . 
Scalethe features  
Finally,wescalethefu nctionwiththehelpof MinMax Scaler . 
 
1. import numpyas np 
2. import tensorflowas tf 
3. fromsklearn.datasets import fetch_mldata  
4. #ChangeUSERNAMEbytheusernameofthe machine  
5. ##Windows USER  
6. mnist=fetch_mldata( 'C:\\Users \\USERNAME \\Downloads \\MNIST original' ) 
7. ##Mac User 
8. mnist=fetch_mldata( '/Users/USERNAME/Downloads/MNIST original' ) 
9. print(mnist.data.shape)  
10. print(mnist.target.shape)  
11. fromsklearn.model_selection import train_test_split  

B.Tech –CSE R-20 
Deep Learning   
 12. A_train,A_test,B_train,B_test=train_test_split(mnist.data,mnist.target,test_siz 
e=0.2, random_state= 45) 
13. B_train=  B_train.astype( int) 
14. B_test= B_test.astype( int) 
15. batch_size =len(X_train)  
16. print(A_train.shape,B_train.shape,B_test.shape ) 
17. ##rescale  
18. fromsklearn.preprocessing import MinMaxScaler  
19. scaler= MinMaxScaler()  
20. #Trainthe  Dataset  
21. X_train_scaled= scaler.fit_transform(A_train.astype(np.float65))  
 
1. #testthe dataset  
2. X_test_scaled= scaler.fit_transform(A_test.astype(np.float65))  
3. feature_columns=[tf.feature_column.numeric_column( 'x',shape=A_train_scale 
d.shape[ 1:])] 
4. X_train_scaled.shape[ 1:] 
DefiningtheCNN(ConvolutionalNeural Network)  
CNN uses filters on the pixels of any image to learn detailed patterns comparedto 
global patterns with a traditional neural network. To create CNN, we have to define:  
 
1. A convolutional Layer: Apply the number of filters to the feature map. After 
convolution, we need to use a relay activation function to add non -linearity to the 
network.  
2. Pooling Layer: The next step after the Convention is to downsampling the maximum 
facility. The objective is to  reduce the mobility of the feature map to prevent 
overfitting and improve the computation speed. Max pooling is a traditional 
technique, which splits feature maps into subfields and only holds maximum values.  
3. Fully connected Layers: All neurons from the pa st layers are associated with the 
other next layers. The CNN has classified the label according to the features from 
convolutional layers and reduced with any pooling layer.  
CNN Architecture  
o ConvolutionalLayer: Itapplies145x5filters(extracting5x5 -pixelsub -regions),  
B.Tech –CSE R-20 
Deep Learning   
 o Pooling Layer: This will perform max pooling with a 2x2 filter and stride of 2 (which 
specifies that pooled regions do not overlap).  
o ConvolutionalLayer: Itapplies365x5filters,withReLUactivation function  
o PoolingLayer: Again,performsmax Poolingwitha2x2filterandstrideof  2. 
o 1,764 neurons, with the dropout regularization rate of 0.4 (where the probability of 
0.4 that any given element will be dropped in training)  
o Dense Layer (LogitsLayer): Thereare tenneurons, oneforeachdigittargetclass(0 - 9). 
Importantmodulestouseincreatinga CNN:  
 
1. Conv2d().Constructatwo -dimensionalconvolutionallayerwiththenumberoffilters, filter 
kernel size, padding, and activation function like arguments.  
2. max_pooling2d (). Construct a two -dimensional pooling layer using the max -pooling 
algorithm.  
3. Dense().Constructadenselayerwiththehiddenlayersand  units  
Wecandefinea functiontobuild CNN.  
The following represents steps to construct every building block before w rapping 
everything in the function.  
 
Step2:Input layer  
1. #Input layer 
2. defcnn_model_fn(mode,features,  labels):  
3. input_layer=tf.reshape(tensor=features[ "x"],shape=[ -1,26,26,1]) 
Weneedtodefineatensorwiththeshapeofthedata.Forthat,wecanuse the module 
tf.reshape . In this module, we need to declare the tensor to reshapeand to shape the 
tensor. The first argument is the feature of the data, that is defined in the argument 
of a function.  
A picture has a width, a height, and a channel. The MNIST dataset is a 
monochromic picture with the 28x28 size. We set the batch size into -1 in the shape 
argument so that it takestheshapeofthefeatures["x"]. Theadvantageisto tunethe batch 
size to hyperparamet ers. If the batch sizeis 7, the tensor feeds 5,488 values ( 28 * 28 * 
7). 
Step3:Convolutional Layer  
1. #firstCNN Layer  
B.Tech –CSE R-20 
Deep Learning   
 2. conv1= tf.layers.conv2d(  
3. inputs= input_layer,  
4. filters= 18, 
5. kernel_size=[ 7,7], 
6. padding= "same" , 
7. activation=tf.nn.relu)  
The first convolutional layer has 18 filters with the kernel size of 7x7 with equal 
padding. The same padding has both the output tensor and input tensor have the 
same width and height. TensorFlow will add zeros in the rowsand column s to ensure 
the same size. We use the ReLu activation function. The output size will be [28, 28, 
and 14].  
Step4:Pooling layer  
 
The next step after the convolutional is pooling computation. The pooling 
computation will reduce the extension of the data. We c an use the module 
max_pooling2d with a size of 3x3 and stride of 2. We use the previous layer as input. 
The output size can be [batch_size, 14, 14, and 15].  
 
1. ##firstPooling Layer  
2. pool1=tf.layers.max_pooling2d(inputs=conv1,pool_size=[ 3,3],strides= 2) 
Step5:PoolingLayerand SecondConvolutional Layer  
 
The second CNN has exactly 32 filters, with the output size of [batch_size, 14, 14, 
32]. The size of the pooling layer has the same as ahead, and output shape is 
[batch_size, 14, 14, and18].  
 
1. conv2=  tf.layers.conv2d(  
2. inputs=pool1,  
3. filters= 36, 
4. kernel_size=[ 5,5], 
5. padding= "same" , 
6. activation=tf.nn.relu)  
7. pool2=tf.layers.max_pooling2d(inputs=conv2,pool_size=[ 2,2],strides= 2). 
Step 6:Fullyconnected (Dense) Layer  
 
We have to define the fully -connected layer. The feature map has to be 
compressed before to be combined with the dense layer. We can use the module 
reshape with a size of 7*7*36 . 
B.Tech –CSE R-20 
Deep Learning   
 The dense layer will connect 1764 neurons. We add a ReLu activation function 
and can add a ReLu activation function. We add a dropout regularization term with a 
rateof0.3,meaning30percentoftheweightswillbe0.Thedropouttakesplaceonly along the 
training phase. The cnn_model_fn() has an argument mode to declare if the  model 
needs to trained or to be evaluate.  
 
1. pool2_flat=tf.reshape(pool2, [ -1,7*7*36]) 
2. dense=tf.layers.dense(inputs=pool2_flat,units= 7*7*36,activation=tf.nn.relu)  
3. dropout=tf.layers.dropout(inputs=dense,rate= 0.3,training=mode==tf.esti 
mator.ModeKeys.TRAIN)  
Step7:Logits Layer  
 
Finally,wedefinethelastlayerwiththepredictionofmodel.Theoutputshape is equal 
to the batch size 12, equal to the total number of images in the layer.  
 
1. #Logit Layer  
2. logits=tf.layers.dense(input s=dropout, units= 12) 
We can create a dictionary that contains classes and the possibility of each 
class. The module returns the highest value with tf.argmax() if the logit layers. The 
softmax function returns the probability of every class.  
PopularCNNarchitectures -VGG,GoogleNet, ResNet:  
B.Tech –CSE R-20 
Deep Learning   
 TypesofConvolutionalNeuralNetwork Algorithms  
 
LeNet  
 
LeNet is a pioneering CNN designed for recognizing handwritten characters. It was proposed by 
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner in the late 1990s. LeNet consists of a 
series of convolutional and pooling layers, as well as a fully connected layer and softmax classifier. It was 
among the first successful applications of deep learning for computer vision. It has been u sed by banks to 
identify numbers written on cheques in grayscale input images.  
 
 
VGG  
 
VGG (Visual GeometryGroup) is a research group within the Department of Engineering Science 
at the Universityof Oxford. The VGG group is well -known for its work in comput er vision, particularlyin 
the area of convolutional neural networks (CNNs).  
 
One of the most famous contributions from the VGG group is the VGG model, also known as 
VGGNet. The VGG model is a deep neural network that achieved state-of-the-art performance on the 
ImageNet Large Scale Visual Recognition Challenge in 2014, and has been widely used as a benchmarkfor 
image classification and object detection tasks.  
 
The VGG model is characterized by its use of small convolutional fil ters (3×3) and deep 
architecture (up to 19 layers), which enables it to learn increasingly complex features from input images. 
The VGG model also uses max pooling layers to reduce the spatial resolution of the feature maps and 
increase the receptive field,  which can improve its ability to recognize objects of varying scales and 
orientations.  
 
The VGG model has inspired many subsequent research efforts in deep learning, including the 
development of even deeper neural networks and the use of residual connecti ons to improve gradient flow 
and training stability.  
 
 
ResNet  
 
ResNet (short for “Residual Neural Network”) is a family of deep convolutional neural networks 
designed to overcome the problem of vanishing gradients that are common in very deep networks. The idea 
behind ResNet is to use “residual blocks” that allow for the direct propagation of gradients throughthe 
network, enabling the training of very deep networks.  
B.Tech –CSE R-20 
Deep Learning   
 A residual block consists of two or more convolutional layers followed by a n activation function, 
combined with a shortcut connection that bypasses the convolutional layers and adds the original input 
directly to the output of the convolutional layers after the activation function.  
 
This allows the network to learn residual functions that represent the difference between the 
convolutional layers’ input and output, rather than trying to learn the entire mapping directly. The use of 
residual blocks enables the training of very deep networks, with hundreds or thousands of layers, 
significantly alleviating the issue of vanishing gradients.  
 
 
GoogLeNet  
 
GoogLeNet is a deep convolutional neural network developed by researchers at Google. It was 
introduced in 2014 and won the ILSVRC (ImageNet  Large -Scale Visual Recognition Challenge)that year, 
with a top -five error rate of 6.67%.  
 
GoogLeNet is notable for its use of the Inception module, which consists of multiple parallel 
convolutional layers with different filter sizes, followed by a pooling  layer, and concatenation of the 
outputs. This design allows the network to learn features at multiple scales and resolutions, while keeping 
the computational cost manageable. The network also includes auxiliary classifiers at intermediate layers, 
which en courage the network to learn more discriminative features and prevent overfitting.  
 
GoogLeNet builds upon the ideas of previous convolutional neural networks, including LeNet, 
which was one of the first successful applications of deep learning in computer vision. However, 
GoogLeNet is much deeper and more complex than LeNet.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
Dropout:  

B.Tech –CSE R-20 
Deep Learning   
 The term “dropout” refers to dropping out the nodes (input and hidden 
layer) in a neural network (as seen in Figure 1). All the forward and backwards 
connections with a dropped node are temporarily removed, thus creating a 
newnetworkarchitectureoutoftheparentnetwork.Thenodesaredroppedby a 
dropout probability of p.  
 
Considergiveninputx:{1,2,3,4,5}tothefullyconnectedlayer.Wehave a 
dropout layer with probability p = 0.2 (or keep probability = 0.8). During the 
forward propagation (training) from the input x, 20% of the nodes would be 
dropped, i.e. the x could become {1, 0, 3, 4, 5} or {1, 2, 0, 4, 5} and so on. 
Similarly, it applied to the hidden layers.  
 
For instance, if the hidden layers have 1000 neurons (nodes) and a 
dropout is applied with drop probability = 0.5, then 500 neurons would be 
randomly dropped in every iteration (batch).  
 
Generally, for the input layers, the keep probabi lity, i.e. 1 - drop 
probability, is closer to 1, 0.8 being the best as suggested by the authors. For 
the hidden layers, the greater the drop probability more sparse the model, 
where 0.5 is the most optimised keep probability, that states dropping 50% of 
the nodes.  
 
HowdoesDropoutsolvetheOverfittingproblem?  
In the overfitting problem, the model learns the statistical noise. To be 
precise, the main motive of training is to decrease the loss function, given all 
the units (neurons). So in overfitting, a unit may  change in a way that fixes up 
themistakesoftheother units.Thisleadstocomplexco -adaptations,which in 
B.Tech –CSE R-20 
Deep Learning   
 turn leads to the overfitting problem because this complex co -adaptation fails 
to generalise on the unseen dataset.  
 
Now, if we use dropout, it prevents these units to fix up the mistake of 
otherunits,thuspreventingco -adaptation,asineveryiterationthepresenceof a unit 
is highly unreliable. So, by randomly dropping a few units (nodes), it forces the 
layers to take more or less responsibility for the input by taking a probabilistic 
approach.  
 
This ensures that the model is getting generalised and hence reducing 
the overfitting problem.  
 
Figure2:(a)Hiddenlayerfeatureswithout dropout;  
(b)Hiddenlayerfeatureswith dropout  
 
Fromfigure2,wecaneasilymakeoutthatthehiddenlayerwithdropout is 
learning more of the generalised features than the co -adaptations in the layer 
without dropout. It is quite apparent, that dropout breaks such inter -unit 
relations and focuses more on generalisation.  

B.Tech –CSE R-20 
Deep Learning   
 
DropoutImplementation  
 
 
Figure3:(a)Aunit(neuron)duringtrainingispresentwitha probability p and is 
connected to the next layer with weights ‘ w’; 
(b) A unitduring inference/prediction is always present and is 
connected to the next layer with weights, ‘ pw’ 
 
In the original implementation of the dropout layer, during training, a 
unit (node/neuron) in a layer is selected with a keep probability (1 -drop 
probability). Th is creates a thinner architecture in the given training batch, and 
every time this architecture is different.  
 
Inthestandardneuralnetwork,duringtheforwardpropagationwehave the 
following equations:  
 
Figure4:Forwardpropagationofastandardneural network  
 
where : 
z:denotethevectorofoutputfromlayer(l+1)beforeactivation y: 
denote the vector of outputs from layer l  
w:weightofthelayerl b: 
bias of the layer l  
B.Tech –CSE R-20 
Deep Learning   
 
Further, with the activation function, z is transformed into the output for 
layer (l+1). Now, if we have a dropout, the forward propagation equations 
change in the following way:  
 
 
Figure5:Forwardpropagationofalayerwith dropout  
So, before we calculate z,the input to the layer is sampled and multiplied 
element -wise with the independent Be rnoulli variables. rdenotes the Bernoulli 
random variables each of which has a probability p of being 1.  
Basically, racts as a mask to the input variable, which ensures only a few 
unitsarekeptaccordingtothekeepprobabilityofadropout.Thisensuresthat we 
have th inned outputs “ y(bar)” , which is given as an input to the layer during 
feed-forward propagation.  
 
Training Deep Neural Networks is a difficult task that involves several 
problems to tackle. Despite their huge potential, they can be slow and be 
prone to ov erfitting. Thus, studies on methods to solve these problems are 
constant in Deep Learning research.  
Batch Normalization – commonly abbreviated as Batch Norm – is one of 
these methods.Currently,itisawidelyusedtechniqueinthefieldof Deep  

B.Tech –CSE R-20 
Deep Learning   
 
Learning.ItimprovesthelearningspeedofNeuralNetworksandprovides regularization, 
avoiding overfitting.  
 
Normalization:  
Normalization is a pre -processing technique used to standardize data .In 
other words, having different sources of data inside the same range. Not 
normalizing the data before training can cause problems in our network, making it 
drastically harder to train and decrease its learning speed.  
For example, imagine  we have a car rental service. Firstly, we want to 
predict a fair price for each car based on competitors’ data. We have two features 
per car: the age in years and the total amount of kilometers it has been driven for. 
These can have very different ranges,  ranging from 0 to 30 years, while distance 
couldgo from0up tohundredsofthousandsofkilometers.Wedon’twantfeatures to 
have these differences in ranges, as the value with the higher range might bias our 
models into giving them inflated importance.  
There are two main methods to normalize our data. The moststraightforward 
method is to scale it to a range from 0 to 1. The data point to normalize,the mean of 
the data set,the highest value, andthe lowest value. This technique is generally used 
in the inputs of the  data. The non - normalized data points with wide ranges can 
cause instability in Neural Networks. The relatively large inputs can cascade down 
to the layers, causing problems such as exploding gradients.  
Theother techniqueused to normalize datais forcing t hedatapoints to have a 
mean of 0 and a standard deviation of 1, using the following formula:  
 
beingthe data point to normalize,the mean of the data set, andthe standard 
deviation of the data set. Now, each data point mimics a standard normal  
B.Tech –CSE R-20 
Deep Learning   
 
distribution.Havingallthefeaturesonthisscale,noneofthemwillhaveabias, and 
therefore, our models will learn better.  
InBatchNorm,weusethislasttechniquetonormalizebatchesofdata inside 
the network itself.  
 
Batch Normalization  
Batch Norm is a normalization technique done between the layers of a 
NeuralNetwork instead of in the raw data . It isdone along mini -batches instead 
of the full data set. It serves to speed up training and use higher learning rates, 
making learning easier.  
Following thetec hniqueexplained in theprevioussection,wecandefinethe 
normalization formula of Batch Norm as:  
 
being mzthe mean of the neurons’ output and szthe standard deviation of the 
neurons’ output.  
 
HowIs It Applied?  
Thefollowingimagerepresentsaregularfeed -forwardNeural Network:are 
the inputs,the output of the neurons,the output of the activation functions, 
andthe output of the network:  
 
B.Tech –CSE R-20 
Deep Learning   
 
Batch Norm –in the image represented with a red line –is applied to the 
neurons’outputjustbeforeapplyingtheactivationfunction.Usually,aneuronwithout 
BatchNormwouldbecomputedasfollows:  
 
 
beingthelineartransformationofthe neuron,  theweightsofthe neuron,  
thebiasoftheneurons,and  theactivationfunction.Themodellear nsthe 
parameters  and. Adding Batch Norm, it looks as:  
 
 
being  the output of Batch Norm,  the mean of the neurons’ 
output,thestandarddeviationoftheoutputoftheneurons,and   learningparametersof 
Batch Norm. Note that the bias of the neurons () is removed. This is because as we 
subtractthemean ,anyconstantoverthevaluesof  z–suchas  b–canbe ignored as it will 
be subtracted by itself.  
The parameters and shift the mean and standard deviation, 
respectively . Thus, the outputs of Batch Norm over a layer re sult in a distribution 
withameanandastandarddeviationof .Thesevaluesarelearnedover epochs and the 
other learning parameters, such as the weights of the neurons, aiming to decrease 
the loss of the model.  
 
 
Data Augmentation:  
 
Algorithms can use machine learning to identify different objects and classify 
them for image recognition. This evolving technology includes using Data 
Augmentation to produce better -performing models. Machine learning models need 
to identify an object in any condition, even if it is rotated, zoomed in, or a grainy 
image. Researchers needed an artificial way of adding training data with realistic 
modifications.  
 
Data augmentation is the addition of new data artificially derived from existing 
trainingdata.Te chniquesinclude resizing, flipping, rotating, cropping, padding,etc. It  

B.Tech –CSE R-20 
Deep Learning   
 helps to address issues like overfitting and data scarcity, and it makes the model 
robust with better performance. Data Augmentation provides many possibilities to 
alter the original image and can be useful to add enough data for larger models.  
 
DataAugmentationina CNN:  
Convolutional Neural Networks (CNNs) can do amazing things if there is 
sufficient data. However, selecting the correct amount of training data for allof  
the features that need to be trained is a difficult question. If the user does not 
have enough, the networkcanoverfiton the trainingdata.Realisticimages 
contain a variety of sizes, poses, zoom, lighting, noise, etc.  
To make the network robust to these com monly encountered factors, 
the method of Data Augmentation is used. By rotating input images todifferent 
angles, flipping images along different axes, or translating/cropping the images 
the network will encounter these phenomena during training.  
As more pa rameters are added to a CNN, it requires more examples to 
show to the machine learning model. Deeper networks can have higher 
performance, but the trade -off is increased training data needs and increased 
training time.  
 
 
 
DataAugmentation Techniques  DataAugmentation Factor  
Flipping  2-4x(ineach direction)  
Rotation  Arbitrary  
Translation  Arbitrary  
Scaling  Arbitrary  
SaltandPepperNoise  Addition  Atleast2x(dependsonthe implementation)  
B.Tech –CSE R-20 
Deep Learning   
 
StellathePuppysittingonacar seat StellathePuppyFlippedoverthevertical axis.  Atableoutliningthefactorbywhichdifferentmethodsmultiplytheexistingtraining  data.  
DataAugmentation Techniques:  
Some libraries use Data Augmentation by actually copying the training 
images and saving these copies as part of the total. This produces new traini ng 
examples to feed to the machine learning model. Other libraries simply define 
a set of transformsto perform on the input training data. These transforms are 
appliedrandomly.Asa result,the space the optimizer issearchingis increased. 
Thishastheadvantaget hatitdoesnotrequireextra diskspacetoaugmentthe 
training.  
ImageDataAugmentationinvolvesthetechniquessuch as 
a) Flips:  
By Flipping images, the optimizer will not become biased that particular 
features of an image are only on one side. To do this augmentation, theoriginal 
training image is flipped vertically or horizontally over one axis of the image. As 
a result, the features continually change directions.  
 
 
Flipping is a similar augmentation as rotation, however, it produces 
mirrorimages.Aparticularfeaturesuchastheheadof apersoneitherstayson top, 
on the left, on the right, or at the bottom of the image.  
b) Rotation:  
Rotation is an augmentation that is commonly performed at 90 -degree 
anglesbutcanevenhappenatsmallerorminuteanglesiftheneedformore  
B.Tech –CSE R-20 
Deep Learning   
 
data is great. For rotation, the background color is commonly fixed so that it 
can blend when the image is rot ated. Otherwise, the model can assume the 
background change is a distinct feature. This works best when the background 
is the same in all rotated images.  
 
 StellathePuppysittingonacarseatStella thePuppyrotated90  degrees.  
 
Specific features move in rotations. For example, the head of a person 
will be rotated 10, 22.7, or -8 degrees. However, rotation does not change the 
orientation of the feature and will not produce mirror images like flips. This 
helps models not consider the angle to be a distinct feature of the human.  
c) Translation:  
Translation of an image means shifting the main object in the image in 
various directions. For example, consider a person in the canter with all their 
parts visible in the frame and take it as a base image. Next, shift th e person to 
one corner with the legs cut from the bottom as one translated image.  
 
B.Tech –CSE R-20 
Deep Learning   
 d) Scaling:  
 
Scaling provides more diversity in the training data of a machine learning 
model. Scaling the image will ensure that the object is recognized by the network 
regardlessof howzoomedin oroutthe image is. Sometimes the object istinyin the 
center. Sometimes, the object is zoomed in the image and even cropped at some 
parts.  
e) Salt andPepperNoise Addition  
 
Salt and pepper noise addition is the addition of black and white dots (looking 
like salt and pepper) to the image. This simulates dust and imperfections in real 
photos. Even if the cameraof thephotographeris blurryorhasspots on it, the image 
would be better recognized by the model. The training data set is augmented to train 
the model with more realistic images.  
 
 
onlypartly visible.  
 
 
 
 Stella thePuppy sitting onacar seat Stella thePuppyscaleduptobeeven larger than 
sheis in reallife. 
 
 
 
 StellathePuppysittingonacar seat StellathePuppywithSaltandPeppernoise added  
tothe image  StellathePuppysittingonacar seat Stella thePuppytranslatedandcroppedso she’s  
B.Tech –CSE R-20 
Deep Learning   
 BenefitsofDataAugmentationina CNN   
 
• Prediction improvement in a model becomes more accurate because 
DataAugmentationhelpsinrecognizingsamplesthemodelhasnever seen 
before.  
• There is enough data for the model to understand and train all the 
availableparameters.Thiscanbeessentialinapplicationswheredata 
collection is difficult.  
• HelpspreventthemodelfromoverfittingduetoDataAugmentation 
creating more variety in the data.  
• Can save timeinareaswherecollectingmoredata istime -consuming.  
• Can reducet hecostrequiredforcollectingavarietyofdataifdata 
collection is costly.  
 
 
DrawbacksofData Augmentation:  
 
Data Augmentation is not useful when the variety required by the application 
cannot be artificially generated. For example, if one were training a bird re cognition 
model and the training data contained only red birds. The training data could be 
augmented by generating pictures with the color of the bird varied.  
 
However, the artificial augmentation method may not capture the realisticcolor 
details of birds when there is not enough variety of data to start with. For example, if 
the augmentation method simply varied red for blue or green, etc. Realistic non -red 
birds may have more complex color variations and the model may fail to recognize 
the color. Ha ving sufficient data is still important if one wants Data Augmentation to 
work properly.  
B.Tech –CSE R-20 
Deep Learning   
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
UNIT -III 
RECURRENT NEURAL NETWORK (RNN): Introduction to 
RNNs and their applications in sequential data analysis, Back 
propagation through time (BPTT), Vanishing Gradient Problem, 
gradient clipping Long Short -Term Memory (LSTM) Networks, 
Gated Recurrent Units, Bidirectional LSTMs, Bidirectional RNNs.  
IntroductiontoRNNsandtheirapplicationsin sequentialdata analysis:  
RecurrentNeuralNetwork (RNN) worksbetterthanasimpleneural network
 when  data is sequential  like Time -Series  data and text data. 
 
ADeepLearningapproachformodellingsequentialdatais RNN:  
RNNs were the standard suggestion for working with sequential data 
beforet headventofattentionmodels.Specificparametersfor each  
B.Tech –CSE R-20 
Deep Learning   
 
element of the sequence may be required by a deep feedforward model. It 
may also be unable to generalize to variable -length sequences.  
 
 
Recurrent Neural Networks use the same weights for each elementof 
the sequence, decreasing the number of parameters and allowing the 
model to generalize to sequences of varying lengths. RNNs generalize to 
structured data other than sequential data, such as geographical or 
graphical data, becaus e of its design.  
Recurrent neural networks, like many other deep learningtechniques, 
are relatively old. They were first developed in the 1980s, but we didn’t 
appreciate their full potential until lately. The advent of long short - term 
memory (LSTM) in the 1990s, combined with an increase in computational 
power and the vast amounts of data that we now have todeal with, has 
really pushed RNNs to the forefront.  
WhatisaRecurrentNeuralNetwork (RNN)?  
Neural networks imitate the function of the human brain in t he fieldsof 
AI, machine learning, and deep learning, allowing computer programs to 
recognize patterns and solve common issues.  
RNNs are a type of neural network that can be used to model 
sequence data. RNNs, which are formed from feedforward networks, are 
similartohumanbrainsintheirbehaviour.Simplysaid,recurrent neural  
B.Tech –CSE R-20 
Deep Learning   
 
networkscananticipatesequentialdatainawaythatotheralgorithms can’t.  
 
All of the inputs and outputs in standard neural networks are 
independent of one another, however in some  circumstances, such aswhen 
predicting the next word of a phrase, the prior words are necessary, and so 
the previous words must be remembered. As a result, RNN was created, 
which used a Hidden Layer to overcome the problem. The most important 
component of RNN is the Hidden state, which remembersspecific 
information about a sequence.  
RNNs have a Memory that stores all information about the 
calculations. It employs the same settings for each input since it produces 
the same outcome by performing the same task  on all inputs or hidden 
layers.  
TheArchitectureofaTraditional RNN  
RNNs are a type of neural network that has hidden states and allows 
past outputs to be used as inputs. They usually go like this:  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
 
 
RNN architecture can vary depending on the problem you’re trying to 
solve. From those with a single input and output to those with many (with 
variations between).  
BelowaresomeexamplesofRNN architectures.  
 
• One To One: There is only one pair here. A one -to-one architectureis 
used in traditional neural networks.  
• One To Many: A single input in a one -to-many network might resultin 
numerous outputs. One too many networks are used in the 
production of music, for example.  

B.Tech –CSE R-20 
Deep Learning   
 
• Many To One: In this scenario, a single output is produced by 
combining many inputs from distinct time steps. Sentiment analysis 
andemotion identification usesuchnetworks,in which theclass label is 
determined by a sequence of words.  
• Many To Many: Formany tomany,therearenumerous options. Two 
inputs yield three outputs. Machine translation systems, such as 
English to French or vice versa translation systems, use many to 
many networks.  
HowdoesRecurrentNeuralNetworks work?  
The information in recurrent neural networks cycles through a loop to 
the middle -hidden layer.  
 
 
The input layer xreceives and processes the neural network’s input 
before passing it on to the middle layer.  
Multiple hidden layers can be found in the middle layer h, each with 
its own activation functions,weights,and b iases.Youcanutilizearecurrent 
neural network if the various parameters of different hidden layers are not 
impacted by the preceding layer, i.e. There is no memory in the neural 
network.  
B.Tech –CSE R-20 
Deep Learning   
 
The different activation functions, weights, and bia ses will be 
standardized by the Recurrent Neural Network, ensuring that each hidden 
layer has the same characteristics. Rather than constructing numerous 
hidden layers, it will create only one and loop over it as many times as 
necessary.  
CommonActivation Functions:  
A neuron’s activation function dictates whether it should be turned on 
or off. Nonlinear functions usually transform a neuron’s output to a number 
between 0 and 1 or -1 and 1.  
 
 
 
 
 
Thefollowingaresomeofthemostcommonlyutilized functions:  
 
• Sigmoid: Theformula g(z)=1/(1+e^ -z)is usedtoexpress this. 
• Tanh: Theformula g(z)=(e^ -z–e^-z)/(e^ -z+e^ -z)isusedto express this.  
• ReLu: The formula g(z)=max(0,z) is usedto express this. 
 
 
ApplicationsofRNN Networks:  
B.Tech –CSE R-20 
Deep Learning   
 1. Machine Translation:  
RNN can be used to build a deep learning model that can translatetext 
from one language to another without the need for human intervention. You 
can, for example, translate a text from your native language to English.  
 
 
2. Text Creation:  
RNNs can also be used to build a deep learning model for text 
generation. Based on the previous sequence of words/characters used in the 
text, a trained modellearns the likelihoodofoccurrenceofa word/character. A 
model can be trained at the character, n -gram, sente nce, or paragraph level.  
 
 
3. Captioningof images:  
The process of creating text that describes the content of an image is 
known as image captioning. The image's content can depict the object as 
well as the action of the object on the image. In the image below,  for 
example,thetraineddeep learning modelusingRNNcandescribetheimage as 
"A lady in a green coat is reading a book under a tree.”  
 
 
4. Recognitionof Speech:  
This is also known asAutomatic Speech Recognition (ASR), and it is 
capable of converting human  speech into written or text format. Don't mix 
up speech recognition and voice recognition; speech recognition primarily 
focuses on converting voice data into text, whereas voice recognition 
identifies the user's voice.  
B.Tech –CSE R-20 
Deep Learning   
 Speech recognition  technologies that are used on a daily basis by 
various users include Alexa, Cortana, Google Assistant, and Siri.  
 
 
 
5. ForecastingofTime Series:  
After being trained on historical time -stamped data, an RNN can be 
used to create a time series prediction model t hat predicts the future 
outcome. The stock market is a good example.  
 
For example, Stock market data can be used to build a machine 
learning model that can forecast future stock prices based on what the model 
learns from historical data. This can assist in vestors in making data -driven 
investment decisions.  
 
RecurrentNeuralNetworkVsFeedforwardNeural Network:  
A feed -forward neural network has only one route ofinformation 
flow: from the input layer to the output layer, passing through the hidden 
layers. The data flows across the network in a straight route, never going 
through the same node twice.  
The information flow between an RNN and a feed -forward 
neural network is depicted in the two figures below.  
B.Tech –CSE R-20 
Deep Learning   
 BackpropagationThroughTime -RNN:  
Backpropagation is a training algorithm that we use for training neural 
networks. When preparing a neural network, we are tuning the network's 
weights to minimize the error concerning the available actual values with the 
help of the Backpropagation algorithm. Backpropagation is a supervised learning 
algorithm as we find errors concerning already given values.  
The backpropagation training algorithm aims to m odify the weights of a 
neural network to minimize the error of the network results compared to some 
expected output in response to corresponding inputs.   
 
 
 
Feed -forward neural networks are poor predictions of what will 
happen next because theyhave no memoryof the information theyreceive. 
Because it simply analyses the current input, a feed -forward network hasno 
idea of temporal order. Apart from its training, it has no memory of what 
transpi red in the past.  
The information is in an RNN cycle via a loop. Before making a 
judgment, it evaluates the current input as well as what it has learned from 
past inputs. A recurrent neural network, on the other hand, may recall due 
to internal memory. It p roduces output, copies it, and then returns it to the 
network.  
 

B.Tech –CSE R-20 
Deep Learning   
 
ThegeneralalgorithmofBackpropagationisas follows:  
1. We first train input data and pr opagate it through the network to get 
an output.  
2. Compare the predicted outcomes to the expected results and calculate 
the error.  
3. Then,wecalculatethederivativesoftheerrorconcerningthenetwork 
weights.  
4. We use these calculated derivatives to adjust the weights  to minimize 
the error.  
5. Repeattheprocessuntiltheerroris minimized.  
In simple words, Backpropagation is an algorithm where the informationof 
cost function is passed on through the neural network in the backward direction. 
The Backpropagation training algorit hm is ideal for training feed - forward neural 
networks on fixed -sized input -output pairs.  
 
UnrollingTheRecurrentNeural Network  
Recurrent Neural Network deals with sequential data. RNN predicts 
outputs using not only the current inputs but also by considering those that 
occurred before it. In other words, the current outcome depends on the current 
production and a memory element (w hich evaluates the past inputs).  
 
Thebelowfiguredepictsthearchitectureof RNN.  
 
 
 
 
 
 
 
 
 
 
We use Backpropagation for training such networks with a slight change. 
We don't independently train the network at a specific time "t." We train it at 
aparticulartime"t "aswellasallthathashappenedbeforetime"t"liket -1,t-2,t-3. 
S1, S2, S3are the hidden states at time t 1, t2, t3, respectively, andWs is 
theassociated weight matrix.  
B.Tech –CSE R-20 
Deep Learning   
 BackpropagationThrough Time  
Ws, Wx, and Wy do not change across the timestamps, which means 
thatfor all inputs in a sequence, the values of these weights are the same.  
Theerrorfunctionisdefined as: 
 
Thepointstoconsider are: 
Whatisthetotallossforthis network?  
Howdoweupdatetheweights,Ws,Wx,and Wy?  
The total loss we have to calculate is the sum in overall timestamps, i.e., 
E0+E1+E2+E3+...Now tocalculatetheerrorgradientconcerning Ws,Wx,andWy. It is 
relatively easy to calculate the loss derivative concerning Wy as the derivative 
only depends on the current timestamp values.  
 
Formula:   
 
To calculate the error, we take the output and calculate its 
errorconcerning the actual result, but we have multiple outputs at each 
timestamp.Thus, the regular Backpropagation won't work here. Therefore, we 
modify thisalgorithm and call the new algorithm as Backpropagation through 
time.  
 
 
 
 
x1, x2, x3are the inputs at time t 1, t2, t3, respectively, and W xis the associated 
weight matrix.  
Y1, Y 2, Y 3are the outcomes at time t 1, t2, t3, respectively, and W yis the 
associated weight matrix.  
At time t 0, we feed input x0 to the network and output y0. At time t1, we 
provideinputx 1tothenetworkandreceiveanoutputy1.Fr omthefigure,wecan see 
that to calculate the outcome. The network uses input x and the cell state from 
the previous timestamp. To calculate specific Hidden state and output at each 
step, here is the formula:  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
Thencalculatingthederivativeoflos sconcerningWsandWx,becomes complex.  
Formula:  
 
 
 
The value of s 3depends on s 2, which is a function of W s. Therefore, we 
cannot calculate the derivative of s 3, taking s 2as constant. In RNN networks, the 
derivative has two parts, implicit and explicit. We assume all other inputs as 
constant in the explicit part, whereas we sum over all the indirect paths in the 
implicit part.  
 
 
 
 
Thegeneralexpressioncanbewritten as: 
 
 
 
Similarly,forWx,itcanbewritten as: 
 
 
 
Now that we have calculated allthree der ivatives,we can easilyupdate the 
weights. This algorithm is known as Backpropagation through time (BPTT), aswe 
used values across all the timestamps to calculate the gradients.  
Thealgorithmata glance:  
• Wefeedasequenceof timestampsofinputandoutputpairstothe network.  
• Then,weunrollthenetworkthencalculateandaccumulateerrors across 
each timestamp.  

B.Tech –CSE R-20 
Deep Learning   
 • Finally,werollupthenetworkandupdate weights.  
• Repeatthe process.  
Limitationsof  BPTT:  
BPTT has difficulty with local optima. Local optima are a more significant 
issue with recurrent neural networks than feed -forward neural networks. The 
recurrent feedback in such networks creates chaotic responses in the error 
surface,which causeslocal optima to occur frequentlyand inthe wrong locations 
on the error surface.  
When using BPTT in RNN, we face problems such as exploding gradient 
and vanishing gradient. To avoid issues such as exploding gradient, we use a 
gradient clipping method to check if the gradient value is greater than the 
threshold or not at each timestamp. If it is, we normalize it. This helps to tackle 
exploding gradient.  
We can use BPTT up to a limited number of steps like 8 or 10. If we 
backpropagate further, the gradient becomes too negligible and is a Vanishi ng 
gradient problem. To avoid the vanishing gradient problem, some of the possible 
solutions are:  
• Using ReLU activation function in place of tanh or sigmoid activation 
function.  
• Properinitializingthe weightmatrixcanreducetheeffectofvanishing 
gradients. For example, using an identity matrix helps us tackle this 
problem.  
• UsinggatedcellssuchasLSTMor GRUs.  
VanishingGradient Problem:  
 
Thegradient descent algorithmfindstheglobal minimumof thecostfunctionthat is 
going to be an optimal setup for the network. Information travels through the neural 
network from input neurons to the output neurons, while the error is calculated and 
propagated bac k through the network to update the weights.  
ItworksquitesimilarlyforRNNs,butadditionaltasks include:  
B.Tech –CSE R-20 
Deep Learning   
 
• Firstly, information travels through time in RNNs, which means that 
informationfromprevioustimepointsisusedasinputforthenexttime points.  
• Secondly,wecancalculatethecostfunction,ortheerror,ateachtime point.  
 
Basically, during the training, your cost function compares your outcomes (red 
circles on the image below) to your desired output. As a result, you have these values 
throughout the time series, for every single one of these red circles.  
 
 
 
 
The focus is o n one error term e t. We calculate the cost function e t and then 
propagate the cost function back through the network because of the need to updatethe 
weights.  
Essentially, every single neuron that participated in the calculation of the 
output, associated w ith this cost function, should have its weight updated in order to 
minimize that error. And the thing with RNNs is that it’s not just the neurons directly 
belowthisoutputlayer thatcontributedbutall of theneuronsfarbackintime.So, you have 
to propagate all t he way back through time to these neurons.  
The problem relates to updating w rec (weight recurring) – the weight that isused 
to connect the hidden layers to themselves in the unrolled temporal loop.  
For instance, to get from x t-3 to x t-2 we multiply x t-3 by wrec. Then, to get from 
xt-2 to x t-1 we again multiply x t-2 by w rec. So, we multiply with the same exact weight 
multipletimes, andthisiswherethe problemarises:when we multiplysomethingbya small 
number, the value decreases very quickly.  
B.Tech –CSE R-20 
Deep Learning   
 
As we know, weights are assigned at the start of the neural network with the 
random values, which are close to zero, and from there the network trains them up. 
But, when you start with wrec close to zero and multiply x t, xt-1, xt-2, xt-3, … by this 
value, your gradient becomes less and less with each multiplication.  
 
 
 
 
 
Whatdoesthismeanforthenetwork?  
The lower the gradient is, the harder it is for the network to update the weights 
and the longer it takes to get to the final result.  
For instance, 1000 epochs might be enough to get the final weight for the time 
point t, but insufficient for training the weights for the time point t -3 due to a verylow 
gradient at this point. However, the problem is not only that half of the network is n ot 
trained properly.  
The output of the earlier layers is used as the input for the further layers. Thus, 
the training for the time point t is happening all along based on inputs that are coming 
fromuntrained layers. So, because of the vanishing gradient, t he whole network is not 
being trained properly.  
To sum up, if wrec is small, you have vanishing gradient problem, and if wrec 
is large, you have exploding gradient problem. For the vanishing gradient problem,the 
further you go through the network, the lowe r your gradient is and the harder it is to 
train the weights, which has a domino effect on all of the further weightsthroughout 
the network.  
B.Tech –CSE R-20 
Deep Learning   
 That was the main roadblock to using Recurrent Neural Networks. However, 
the possible solutions t o this problem are as follows:  
Solutionstothevanishinggradientproblem  
Incaseofexplodinggradient,you can: 
 
• Stopbackpropagatingafteracertainpoint,whichisusuallynotoptimal because not 
all of the weights get updated.  
• Penalizeorartificiallyreduce gradient.  
• Putamaximumlimitona gradient.  
 
 
Incaseofvanishinggradient,you can: 
 
• Initializeweightssothatthepotentialforvanishinggradientis minimized.  
• HaveEchoStateNetworksthataredesignedtosolvethevanishinggradient problem.  
• HaveLongShort -TermMemoryNetworks (LSTMs).  
 
GradientclippingLongShort -TermMemory(LSTM) Networks:  
Training a neural network can become unstable given the choice of error 
function, learning rate, or even the scale of the target variable. Large updates to 
weightsduringtrainingcancausea numericaloverfloworunderflowoften referred to as 
“Exploding Gradients.”  
The problem of exploding gradients is more common with recurrent neur al 
networks, such as LSTMs given the accumulation ofgradients unrolled overhundreds 
of input time steps.  
A common and relatively easy solution to the exploding gradients problem isto 
change the derivative of the error before propagating it backward through  the network 
and using it to update the weights. Two approaches include rescaling the gradients 
given a chosen vector norm and clipping gradient values that exceed a preferred range. 
Together, these methods are referred to as “Gradient Clipping.”  
 
• Training neuralnetworkscanbecomeunstable,leadingtoanumerical overflow 
or underflow referred to as exploding gradients.  
B.Tech –CSE R-20 
Deep Learning   
 • The training process can be made stable by changing the error gradients 
either by scaling the vector norm or clipping gradient v alues to a range.  
• How to update anMLP model for aregression predictive modeling problem 
with exploding gradients to have a stable training process using gradient 
clipping methods?  
 
ExplodingGradientsand Clipping  
Neural networks are trained using the stochastic gradient descentoptimization 
algorithm. This requires first the estimation of the loss on one or more training 
examples, then the calculation of the derivative of the loss, which is propagated 
backward through the network in order to update the weights. Weights are updated 
using a fraction of the back propagated error controlled by the “ Learning Rate ”. 
 
It is possible for the updates to the weights to be so large that the weights 
either overflow or underflow their numerical precision. In practice, the weights can 
take on the value of an “ NaN ” or “Inf” when they overflow or underflow and for 
practical purposes the network will be useless from that point forward, forever 
predicting NaN values as signals flow through the invalid weights.  
 
The difficulty that arises is that when the parameter gradient  is very large, a 
gradient descent parameter update could throw the parameters very far, into aregion 
where the objective function is larger, undoing much of the work that hadbeen done 
to reach the current solution.  
 
The underflow or overflowof weights gen erally refers to asan instability of the 
network training process and is known by the name “ exploding gradients ” as the 
unstable training process causes the network to fail to train in such a way that the 
model is essentially useless.  
In a given neural net work, such as a Convolutional Neural Network or 
Multilayer Perceptron, this can happen due to a poor choice of configuration. Some 
examples include:  
 
• Poorchoiceoflearningratethatresultsinlargeweight  updates.  
B.Tech –CSE R-20 
Deep Learning   
 • Poor choice of data preparation, allowing large differences in the target 
variable.  
• Poorchoiceoflossfunction,allowingthecalculationof largeerror values.  
Exploding gradients is also a problem in recurrent neural networks such as the 
LongShort -TermMemorynetwo rkgiventheaccumulationoferrorgradientsin the unrolled 
recurrent structure . 
Exploding gradients can be avoided in general by careful configuration of the 
networkmodel,suchaschoiceofsmalllearningrate,scaledtargetvariables,and a standard 
loss function . Nevertheless, exploding gradient s may still be an issue with recurrent 
networks with a large number of input time steps.  
 
One difficulty when training LSTM with the full gradient is that the derivatives 
sometimes become excessively large, leading to numerical problems. To prevent 
this, [ we] clipped the derivative of the loss with respect to the network inputs to the 
LSTM layers (before the sigmoid and tanh functions are applied) to lie within a 
predefined range . 
 
A common solution to exploding gradients is to change the error derivative 
before propagating it backward through the network and using it to update the 
weights. By rescaling the error derivative, the updates to the weights will also be 
rescaled, dramatically decreasing the likelihood of an overflow or underflow.  
 
Therearetwo main methodsforupdatingtheerrorderivativeas follows:  
 
• Gradient Scaling.  
• Gradient Clipping.  
Gradient scaling involves normalizing the error gradient vector such thatvector 
norm (magnitude) equals a defined value, such as 1.0. One simplemechanism to 
deal with a sudden increase in the norm of the gradients is to rescale them whenever 
they go over a threshold  
Gradient clipping involves forcing the gradient values (element -wise) to a 
specific minimum or maximum value if the gradient exceeded an expected 
range.Togeth er, these methods are often simply referred to as “ gradient clipping .” 
B.Tech –CSE R-20 
Deep Learning   
 When the traditional gradient descent algorithm proposes to make a verylarge 
step, the gradient clipping heuristic intervenes to reduce the step size to be small 
enoug h that it is less likely to go outside the region where the gradientindicates the 
direction of approximately steepest descent. It is a method that only addresses the 
numerical stability of training deep neural network models and does not offer any 
general improvement in performance.  
 
The value for the gradient vector norm or preferred range can be configuredby 
trial and error, by using common values used in the literature or by first observing 
common vector norms or ranges via experimentation and then choos ing a sensible 
value.  
 
Experimental analysis reveals that for a given task and model size, training is 
not very sensitive to this [gradient norm] hyperparameter and the algorithm behaves 
well even for rather small thresholds.  
 
It is common to use the same gradient clipping configuration for all layers in 
the network. Nevertheless, there are examples where a larger range of error 
gradients are permitted in the output layer compared to hidden layers.  
 
The output derivatives […]were clipped in the range [−100, 100], and the LSTM 
derivatives were clipped in the range [−10, 10]. Clipping the output gradients proved 
vital for numerical stability; even so, the networks sometimes had numerical 
problems late on in training, after they had started overfitting on the training data . 
 
GatedRecurrentUnit (GRU):  
A Gated Recurrent Unit (GRU) is a Recurrent Neural Network (RNN) architecture 
type. Like other RNNs, a GRU can process sequential data such as time series, natural 
language, and speech . The main difference between a GRU and other RNN architectures, 
such as the Long Short -Term Memory (LSTM) network, is how the network handles 
information flow through time.  
Example:  
"Mymom gavemea bicycleonmy birthdaybecauseshe knew thatI wanted to go bik ing with 
my friends."  
B.Tech –CSE R-20 
Deep Learning   
 
As it can be observed from the above sentence, words that affect each other can be 
further apart. For example, "bicycle" and "go biking" are closely related but are placed 
further apart in the sentence. An RNN networ k finds tracking the state with such a long 
context difficult. It needs to find out what information is important. However, a GRU cell 
greatly alleviates this problem.  
 
 
GRUnetworkwasinventedin2014.Itsolvesproblemsinvolvinglongsequenceswith 
contextsplacedfurtherapart,liketheabovebikingexample.Thisispossiblebecauseofhow the 
GRU cell in the GRU architecture is built.  
UnderstandingtheGRU Cell:  
The GRU cell is the basic buildin g block of a GRU network. It comprises three main 
components: an update gate, a reset gate, and a candidate hidden state.  
 
 
 
 
One of the key advantages of the GRU cell is its simplicity. Since it has fewer 
parameters than a long short -term memory (LSTM) c ell, it is faster to train and run and less 
prone to overfitting.  
Additionally, one thing to remember is that the GRU cell architecture is simple, the 
cell itself is a black box, and the final decision on how much we should consider the past 
state and how much should be forgotten is taken by this GRU cell.  
GRUvs LSTM   
 
 GRU  LSTM  
Structure  Simplerstructurewithtwogates 
(update and reset gate)  More complexstructurewiththree gates 
(input, forget, and output gate)  
Parameters  Fewer  parameters  (3 weight 
matrices)  Moreparameters (4weight matrices)  
B.Tech –CSE R-20 
Deep Learning   
 
 
 GRU  LSTM  
Training  Fasterto train Slow to train 
 Inmostcases,GRUtendto use LSTMhasamorecomplexstructure and 
Space 
Complexity  fewermemoryresourcesduetoits 
simpler  structure  and fewer  
parameters,thusbettersuited for alargernumberofparameters,thusmight 
require  more  memory  resources  and 
couldbelesseffectiveforlarge datasets  
 largedatasetsor sequences.  orsequences.  
 Generallyperformedsimilarly to LSTM generally performs well on many 
tasks but is more computationally 
expensive and requires more memory 
resources. LSTM has advantages over 
GRU in natural language understanding 
and machine translation tasks.   LSTMonmanytasks,butin some  
 cases,GRUhasbe enshown to 
Performance  outperformLSTMandvice versa.  
 It'sbettertotrybothandsee which  
 worksbetterforyourdataset and 
 task. 
 
TheArchitectureof GRU  
AGRUcellkeepstrackoftheimportantinformationmaintainedthroughoutthe network. A 
GRU network achieves this with the following two gates:  
 
 
 
 
 
inputs:  • Reset Gate  
• Update Gate.  
GivenbelowisthesimplestarchitecturalformofaGRUcell.AGRUcelltakes two 
1. Theprevioushidden state  
2. Theinputinthecurrent timestamp.  
The cell combinestheseandpasses them through the update and reset gates. To get 
the output in the current timestep, we must pass this hidden state through a dense layer 
with softmax activation to predict the output. Doing so, a new hidden state is obtained and 
then passed on to the next time step.  
 
 
B.Tech –CSE R-20 
Deep Learning   
 Update gate  
An update gate determ ines what current GRU cell will pass information to the next 
GRU cell. It helps in keeping track of the most important information . 
 
 
Obtainingtheoutput oftheUpdateGateinaGRU cell: 
The input to the update gate is the hidden layer at the previous timestep, h(t−1) and 
the current input ( xt). Both have their weights associated with them which are learned 
during the training process. Let us say that the weights associated with h(t−1) isU(z), and that 
of xtis Wz. The output of the update gate Ztis given by,  
zt=σ(W(z)xt+U(z)h(t−1) 
Reset gate  
A reset gate identifies the unnecessary information and decides what informationto 
be laid off from the GRU network. Simply put, it decides what information to delete atthe 
specific timestamp.  
Obtainingtheoutput oftheResetGatein aGRU cell: 
The input to the reset gate is the hidden layer at the previous timestep h(t−1)andthe 
current input xt. Both have their weights associated withthem whichare learned during 
thetrainingprocess.Letussaythattheweights associatedwith h(t−1)isUr,andthatof xt is Wr. The 
output of the update gate rt is given by,  
rt=σ(W(r)xt+U(r)h(t−1)) 
It is important to note that  the weights associated with the hidden layer at the 
previous timestep and the current input are different for both gates. The values for these 
weights are learned during the training process.  
HowDoesGRU Work?  
Gated Recurrent Unit (GRU)networks process sequential data, such as time series or 
natural language, bypassing the hidden state from one time step to the next. The hidden 
state is a vector that captures the information from the past time steps relevant to the 
currenttimestep.ThemainideabehindaGRUistoallowthenetworktodecidewhat  
B.Tech –CSE R-20 
Deep Learning   
 information from the last time step is relevant to the current time step and what 
information can be discarded.  
 
 
 
 
CandidateHidden State  
A candidate's hidden state i s calculated from the reset gate. This is used todetermine 
the information stored from the past. This is generally called the memory component in a 
GRU cell. It is calculated by,  
ht′=tan h(Wxt+rt⊙Uh t−1) 
Here,W -weightassociatedwiththecurrent input  
 
rt-Outputofthereset gate  
 
U-Weightassociatedwiththehiddenlayeroftheprevious  timestep  
 
ht-Candidatehidden state.  
 
Hidden state  
The following formula gives the new hidden state and depends on the update gate 
and candidate hidden state.  
ht=zt⊙ht−1+(1− zt)⊙ht′ 
Here,z t-OutputofupdategateKaTeXparseerror Expected'EOF'got'’'atposition2: h’ t - 
Candidate hidden state  
ht−1-Hiddenstateattheprevious timestep  
It can be observed that whenever ztis 0, the information at the previously hidden 
layer gets forgotten. It is updated with the value of the new candidate hidden layer 
(as1− ztwillbe1).If ztis1,thentheinformationfromthepreviously hidden layerismaintained.This 
is how the most relevant information is passed from one state to the next.  
ForwardPropagationinaGRU Cell 
InaGatedRecurrentUnit(GRU)cell,theforwardpropagationprocessincludes several 
steps:  
B.Tech –CSE R-20 
Deep Learning   
 
• Calculatetheoutput oftheupdategate( zt)usingtheupdategate formula:  
 
 
 
 
• Calculatetheoutputoftheresetgate( rt)usingtheresetgate formula:  
 
 
 
• Calculatethecandidate'shidden state.  
 
B.Tech –CSE R-20 
Deep Learning   
 
 
• Calculatethenewhidden state.  
 
 
This is how forward propagation happens in a GRU cell at a GRU network. Next, the 
process of how the w eights is learnt in a GRU network to make the right prediction have to 
be understood.  
BackpropagationinaGRU Cell 
Let eachhiddenlayer(orangecolour)representa GRU cell. 
 
 
In the above image, it is observed that whenever the network predicts wrongly, the 
network compares it with the original label, and the loss is then propagated throughout the 
network.Thishappensuntilalltheweights'valuesareidentifiedsothatthevalueof theloss 
function used to compute the loss is minimum. During this time, the weights and bi ases 
associated with the hidden layers and the input are fine -tuned.  
B.Tech –CSE R-20 
Deep Learning   
  
AnalogybetweenLSTMandGRUintermsofarchitectureand performance:  
LSTM and GRU are two types of recurrent neural networks (RNNs) that can handle 
sequential data, such as tex t, speech, or video. They are designed to overcome the problem of 
vanishing or exploding gradients that affect the training of standard RNNs. However, they 
have different architectures and performance characteristics that make them suitable for 
different a pplications. In this article, you will learn about the differences and similarities 
between LSTM and GRU in terms of architecture and performance.  
LSTM Architecture  
LSTM stands for long short -term memory, and it consists of a series of memory cells 
that can  store and update information over long time steps. Each memory cell has three 
gates: an input gate, an output gate, and a forget gate. The input gate decides what 
information to add to the cell state, the output gate decides what information to output 
from the cell state, and the forget gate decides what information to discard from the cell 
state. The gates are learned by the network based on the input and the previous hidden 
state.  
GRU  Architecture  
GRU standsfor gated recurrentunit, and it is asimplified versionof LSTM. It hasonly 
two gates: a reset gate and an update gate. The reset gate decides how much of the 
previous hidden state to keep, and the update gate decides how much of the new input to 
incorporate into the hidden state. The hidden state also a cts as the cell state and theoutput, 
so there is no separate output gate. The GRU is easier to implement and requires fewer 
parameters than the LSTM.  
Performance Comparison  
The performance of LSTM and GRU depends on the task, the data, and the 
hyperparameters. Generally, LSTM is more powerful and flexible than GRU, but it is also 
more complex and prone to overfitting. GRU is faster and more efficient than LSTM, but it 
may not capture long -term dependencies as well as LSTM. Some empirical studies  have 
shownthatLSTMandGRUperformsimilarlyonmanynaturallanguageprocessingtasks,  
B.Tech –CSE R-20 
Deep Learning   
 such as sentiment analysis, machine translation, and text generation. However, some tasks 
may benefit from the specific features of LSTM or GRU, such as image captioning, speech 
recognition, or video analysis.  
SimilaritiesBetweenLSTMand GRU  
Despite their differences, LSTM and GRU share some common characteristics that 
makethembotheffectiveRNNvariants.Theybothusegatestocontroltheinformationflow and to 
avoid the va nishing or exploding gradient problem. They both can learn long -term 
dependencies and capture sequential patterns in the data. They both can be stacked into 
multiple layers to increase the depth and complexity of the network.  
They both can be combined with  other neural network architectures, such as 
convolutional neural networks (CNNs) or attention mechanisms, to enhance their 
performance.  
DifferencesBetweenLSTMand GRU  
The main differences between LSTM and GRU lie in their architectures and their 
trade -offs.  LSTM has more gates and more parameters than GRU, which gives it more 
flexibility and expressiveness, but also more computational cost and risk of overfitting. GRU 
has fewer gates and fewer parameters than LSTM, which makes it simpler and faster, but 
also less powerful and adaptable.  
LSTM has a separate cell state and output, which allows it to store and output 
different information, while GRU has a single hidden state that serves both purposes, which 
may limit its capacity. LSTM and GRU may also have diff erent sensitivities to the 
hyperparameters, such as the learning rate, the dropout rate, or the sequence length.  
 
Bidirectional LSTM  
Introduction:  
To understand the working of Bi -LSTM first, the working of the unit cell of LSTM 
and LSTM network has to be understood. LSTM stands for long short -term memory. In 
1977, Hochretier and Schmidhuber introduced LSTM networks. These are the most 
commonly used recurrent neural networks.  
B.Tech –CSE R-20 
Deep Learning   
 
Needof LSTM  
As the sequential data is better handled by recurrent  neural networks, but 
sometimes it is also necessary to store the result of the previous data. For example, “I 
will play cricket” and “I can play cricket” are two different sentences with different 
meanings. The meaning of the sentence depends on a single word so, it is necessary to 
store the data of previous words. But no such memory is available in simple RNN. To 
solve this problem, LSTM is adopted.  
 
TheArchitectureoftheLSTM Unit  
 
 
TheLSTMunithasthree gates.  
a) Input  gate  
First, the current state x(t) and previous hidden state h(t -1) are passed into the 
input gate, i.e., the second sigmoid function. The x(t) and h(t -1) values are transformed 
between0and1,where 0isimportant,and1is notimportant.Furthermore,thecurrent and 
hidden state information will be passe d through the tanh function. The output from the 
tanh function will range from -1 to 1, and it will help to regulate the network. The 
output values generated from the activation functions are ready for point -by-point 
multiplication.  
b) Forget gate  
The forget gate decides which information needs to be kept for further 
processing and which can be ignored. The hidden state h(t -1) and current input X(t) 
informationarepassedthroughthesigmoidfunction.Afterpassingthevalues through  
B.Tech –CSE R-20 
Deep Learning   
 thesigmoidfu nction,itgeneratesvaluesbetween0and1thatconcludewhetherthe part of 
the previous output is necessary (by giving the output closer to 1).  
c) Output  gate  
The output gate helps in deciding the value of the next hidden state. This state 
contains information on pre vious inputs. First, the current and previously hidden state 
values are passed into the third sigmoid function. Then the new cell state generated 
from the cell state is passed through the tanh function. Both these outputs aremultiplied 
point -by-point. Base d upon the final value, the network decides which information the 
hidden state should carry. This hidden state is used for prediction.  
Finally, the new cell state and the new hidden state are carried over to the next 
step. To conclude, the forget gate dete rmines which relevant information from the prior 
steps is needed. The input gate decides what relevant information can be added fromthe 
current step, and the output gates finalize the next hidden state.  
 
HowdoLSTM work?  
TheLengthyShortTermMemoryarchitecture wasinspiredbyanexaminationof 
error flow in current RNNs, which revealed that long time delays were inaccessible to 
existing designs due to backpropagated error, which either blows up or decays 
exponentially.  
An LSTM la yer is made up of memory blocks that are recurrently linked. These 
blocks can be thought of as a differentiable version of a digital computer's memory 
chips. Each one has recurrently connected memory cells as well as three multiplicative 
units – the input,  output, and forget gates – that offer continuous analogs of the cells' 
write, read, and reset operations.  
 
WhatisBi -LSTM?  
Bidirectional LSTM networks function by presenting each training sequence 
forward and backward to two independent LSTM networks, both  of which are coupled 
to the same output layer. This means that the Bi -LSTM contains comprehensive, 
sequential information about all points before and after each point in a particular 
sequence.  
In other words, rather than encoding the sequence in the forwa rd direction only, 
weencodeitinthebackwarddirectionaswellandconcatenatetheresultsfromboth  
B.Tech –CSE R-20 
Deep Learning   
 
forwardandbackwardLSTMateachtimestep.Theencodedrepresentationofeach word now 
understands the words before and after the specific word.  
Belowisthebas icarchitectureofBi -LSTM.  
 
 
WorkingofBi -LSTM:  
Consider the sentence “I will swim today”. The below image represents the 
encoded representation of the sentence in the Bi -LSTM network.  
 
So, when forward LSTM occurs, “I” will be passed into the LSTM network at timet 
= 0, “will” at t = 1, “swim” at t = 2, and “today” at t = 3. In backward LSTM “today” will be 
passedinto the network at time t = 0, “swim” at t = 1, “will” at t = 2, and“I” at t = 3. In this 
way, both the results of forward and backward LS TM at each time step are calculated.  
B.Tech –CSE R-20 
Deep Learning   
 
UNIT -IV 
GENERATIVEADVERSARIALNETWORKS (GANS):  
Generativemodels,Conceptandprinciplesof GANs,Architecture of 
GANs (generator and discriminator networks), Comparison 
between discriminative and generative m odels, Generative 
Adversarial Networks (GANs), Applicationsof GANs  
 
GenerativeAdversarialNetworksandits models  
Introduction:  
 
 
 
 
Generative Adversarial Networks (GANs) were developed in 2014 by Ian 
Goodfellow and his teammates. GAN is basically an approach to generativemodeling 
that generates a new set of data based on training data that look like training data. 
GANs have two main blocks (two neural networks) which compete with each other 
and are able to capture, copy, and analyze the variations in a dataset.The two 
models are usually called Generator and Discriminator which we will coverin 
Components on GANs. The term GAN can be separated into three parts.  
 
• Generative – To learn a generative model, which describes how data is generated in 
terms of a  probabilistic model. In simple words, it explains how data is generated 
visually.  
B.Tech –CSE R-20 
Deep Learning   
  
• Adversarial –Thetrainingofthemodelisdoneinanadversarial setting.  
• Networks –Usedeepneuralnetworksfortraining purposes.  
 
The generator network takes random inp ut (typically noise) and generates 
samples, such as images, text, or audio, that resemble the training data it 
wastrainedon.The goalof the generatoristo produce samples that 
areindistinguishable from real data.  
 
The discriminator network, on the other hand , tries to distinguish between real 
and generated samples. It is trained with real samples from the training data and 
generated samples from the generator. The discriminator’s objective is to correctly 
classify real data as real and generated data as fake.  
 
The training process involves an adversarial gamebetweenthe generator and 
the discriminator. The generator aims to produce samples that fool the discriminator, 
while the discriminator tries to improve its ability to distinguish between real and 
generated data. This adversarial training pushes both networks to improve over time.  
 
As training progresses, the generator becomes more adept at producing 
realistic samples, while the discriminator becomes more skilled at differentiating 
between real and generated data. Ideally, this process converges to a point where 
the generator is capable of generating high -quality samples that are difficult for the 
discriminator to distinguish from real data.  
 
GANs have demonstrated impressive results in various domai ns, such as 
image synthesis, text generation, and even video generation. They have been used 
for tasks like generating realistic images, creating deepfakes, enhancing low - 
resolution images, and more. GANs have greatly advanced the field of generative 
mode ling and have opened up new possibilities for creative applications in artificial 
intelligence.  
B.Tech –CSE R-20 
Deep Learning   
 
 
WhyGANs was Developed?  
 
Machine learning algorithms and neural networks can easily be fooled to 
misclassify things by adding some amount of noise to data. After adding some 
amountof noise, the chancesof misclassifyingthe imagesincrease.Hence the small 
rise that, is it possible to implement something that neural networks can start 
visualizing new patterns like sample train data. Thus, GANs were  built that generate 
new fake results similar to the original.  
 
ComponentsofGenerativeAdversarialNetworks (GANs):  
 
WhatisGeometricIntuitionbehindtheworkingof GANs?  
 
Two major components of GANs are Generator and Discriminator. The role of 
the generator is like a thief to generate the fake samples based on the original 
sample and make the discriminator fool to understand Fake as real. On the other 
hand, a Discriminator is like a Police whose role is to identify the abnormalities in the 
samples c reated by Generator and classify them as Fake or real. This competition 
between both the component goes on until the level of perfection is achieved where 
Generator wins making a Discriminator fool on fake data.  
 
 
 
B.Tech –CSE R-20 
Deep Learning   
 
1) Discriminator –It is a  supervised approach means It is a simple classifier that 
predicts data is fake or real. It is trained on real data and provides feedback to a 
generator.  
 
2) Generator –It is an unsupervised learning approach. It will generate data that is 
fake data based on original(real) data. It is also a neural network that has hidden 
layers, activation, loss function. Its aim is to generate the fake image based on 
feedback and make the discriminator fool that it cannot predict a fake image. And 
when the discriminator is m ade a fool by the generator, the training stops and wecan 
say that a generalized GAN model is created.  
 
 
 
 
 
Here, the generative model captures the distribution of data and is trained in 
such a manner to generate the new sample that tries to maximize the probability of 
the discriminator to make a mistake (maximize discriminator loss). The discriminator 
on other hand is based on a model that estimates the probability that the sample it 
receives is from training data not from the generator and tries to class ify it accurately 
and minimize the GAN accuracy. Hence the GAN network is formulated as aminimax 
game where the Discriminator is trying to minimize its reward V(D, G) and the 
generator is trying to maximize the Discriminator loss.  
 
Thebelowfigureaddressestheconstraints 
How is an actual architecture of GAN?  
B.Tech –CSE R-20 
Deep Learning   
 
 
Howtwoneuralnetworksarebuildandtrainingandpredictionis  done?  
 
 
Both the components are neural networks.The generator output is directly 
connected to the input of the discriminator. And discriminator predicts it and through 
backpropagation, the generator receives a feedback signal to update weights and 
improve performance. The discriminator is a feed -forward neural network.  
Training&PredictionofGenerativeAd versarialNetworks(GANs): 
Step -1) Define a Problem  
The problem statement is key to the success of the project so the first step is 
to define the problem. GANs work with a different set of problems you are aiming so 
you need to define What you are creating l ike audio, poem, text, Image is a type of 
problem.  
 
Step -2)SelectArchitectureof GAN  
 
There are many different types of GAN & based on the scenario(s), a suitable 
GANarchitecture is chosen.  
 
Step -3)TrainDiscriminatoronReal Dataset  
 
Now, Discriminator is trained on a real dataset. It is only having a 
forwardpath.NobackpropagationisthereinthetrainingoftheDiscriminatorinnepochs.  
B.Tech –CSE R-20 
Deep Learning   
  
And the provided Data is without Noise and only contains real images, and for 
fakeimages, Discr iminator uses instances created by the generator as negative 
output.  
 
Discriminator Training:  
 
• Itclassifiesbothrealandfake data.  
• Thediscriminatorlosshelpsimproveitsperformanceandpenalizeitwhenit misclassifies 
real as fake or vice -versa.  
• weightsofthediscriminatorareupdatedthroughdiscriminator loss.  
 
Step -4)Train Generator  
 
Provide some Fake inputs for the generator (Noise) and it will use some 
random noise and generate some fake outputs. when Generator is trained, 
Discriminator is Idle and when Discriminator is trained, Generator is Idle. During 
generator training through any random noise as input, it tries to transform it into 
meaningful data. to get meaningful output from the generator takes time and runs 
under many epochs. Steps to tr ain a generator are listed below.  
 
• Getrandomnoiseandproduce ageneratoroutput on noise sample  
• Predictgeneratoroutputfromdiscriminatorasoriginalor fake.  
• Calculatediscriminator loss.  
• Performbackpropagationthroughdiscriminator,andgeneratorbothtocalculate gradients . 
• Usegradientstoupdategenerator  weights.  
 
Step -5)TrainDiscriminatoronFake Data  
 
The samples which are generated by Generator will pass to Discriminator and 
It will predict the data passed to it is Fake or real and provide feedback to Generator 
again.  
B.Tech –CSE R-20 
Deep Learning   
 
 
Step -6)TrainGeneratorwiththeoutputof Discriminator  
 
Again, Generator will be trained on the feedback given by Discriminator andtry 
to improve performance. This is an iterative process and continues running until the 
Generator is not succe ssful in making the discriminator fool.  
 
 
 
GenerativeAdversarialNetworks(GANs)Loss Function:  
The loss function is used in minimize and maximize of the iterative process. 
The generator tries to minimize the following loss function while the discriminatortries 
to maximize it. It is the same as a minimax game if you have ever played.  
 
 
 
• D(x)isthediscriminator’sestimateoftheprobabilitythatrealdatainstancexis  real. 
• Existhe expectedvalueoverall realdata instances.  
• G(z)isthe generator’soutput when given noise z. 
• D(G(z))isthediscriminator’sestimateoftheprobabilitythatafakeinstanceis  real. 
• Ez istheexpectedvalueoverallrandominputstothegenerator(ineffect,theexpected value 
over all generated fake instances G(z)).  
B.Tech –CSE R-20 
Deep Learning   
 ChallengesFacedbyGenerative AdversarialNetworks  (GANs):  
 
1. The problem of stability between generator and discriminator. The 
discriminator should not be too strict nor too lenient.  
2. Problem to determine the positioning of objects - Suppose in a picture wehave 
3 horse and generator have created 6 eyes and 1 horse.  
3. The problem in understanding the global objects –GANs do not understand 
the global structure or holistic structure which is similar to the problem of 
perspective. It means sometimes GAN generates an im age that is unrealistic 
and cannot be possible.  
4. Problem in understanding the perspective - It cannot understand the 3 -d 
images and if we train it on such types of images then it will fail to create 3 -d 
images because today GANs are capable to work on 1 -d images.  
 
DifferentTypesofGenerativeAdversarialNetworks (GANs):  
1) DC GAN –It is a Deep convolutional GAN. It is one of the most used, powerful, 
and successful typesof GANarchitecture.It is implemented with help of ConvNets in 
place ofaMulti -layeredperceptron.The ConvNetsusea convolutionalstrideandare built 
without max pooling and layers in this network are not completely connected.  
 
2) Conditional GAN and Unconditional GAN (CGAN) –Conditional GAN is deep 
learning neural network in which  some additional parameters are used. Labels are 
also put in inputs of Discriminator in order to help the discriminator to classify the 
input correctly and not easily full by the generator.  
 
3) Least Square GAN (LSGAN) –It is a type of GAN that adopts the lea st-square 
lossfunctionforthediscriminator.Minimizingtheobjectivefunctionof LSGANresults in 
minimizing the Pearson divergence.  
B.Tech –CSE R-20 
Deep Learning   
  
4) Auxilary Classifier GAN (ACGAN) –It is the same as CGAN and an advanced 
version of it. It says that the Discrim inator should not only classify the image as real 
or fake but should also provide the source or class label of the input image.  
 
5) Dual Video Discriminator GAN –DVD -GAN is a generative adversarial network 
for video generation built upon the BigGAN architecture. DVD -GAN uses two 
discriminators: a Spatial Discriminator and a Temporal Discriminator . 
 
6) Single Image Super Resolution GAN (SRGAN) – Its main function is to  
transform low resolution to high resolution known as Domain Transformation.  
 
7) Cycle GAN - It is released in 2017 which performs the task of Image Translation. 
Suppose we have trained it on a horse image dataset and we can translate it into 
zebra images.  
8) Info GAN –Advance version of GAN which is capable to learn to disentangle 
representationinanunsupervisedlearningapproach.  
 
TopGenerativeAdversarialNetworks Applications:  
 
1) Generate Examples for Image Datasets: GANs can be used to generate new 
examples for image datasets in various domains, such as medical imaging, satellite 
imagery,and naturallanguageprocessing .Bygeneratingsyntheticdata, researcherscan 
augment existingdatasets and improve the performance of machine learning models.  
 
2) Generate Photographs of Human Faces: GANs can generate realistic 
photographs of human faces, including images of people who do not exist in the real 
world. We can use these rendered images for various purposes, such as creating 
avatars for online games or social media profiles.  
 
3) Generate Realistic Photographs: GANs can generate realistic photograp hs of 
various objects and scenes, including landscapes, animals, and architecture. These  
B.Tech –CSE R-20 
Deep Learning   
 renderedimagescanbeusedtoaugmentexistingimagedatasetsortocreateentirely new 
datasets.  
 
4) Generate Cartoon Characters: GANs can be used to generate cart oon 
characters that are similar to those found in popular movies or television shows. 
These developed characters can create new content or customize existingcharacters 
in games and other applications.  
 
5) Image -to-Image Translation: GANs can translate images from one domain to 
another, such as convertinga photograph of a real -world scene intoa line drawingor a 
painting. We can create new content or transform existing images in various ways.  
 
6) Text-to-Image Translation: GANs can be used to generate images based on a 
given text description. We can use it to create visual representations of concepts or 
generate images for machine learning tasks.  
 
7) Semantic -Image -to-Photo Translation: GANs can translate images from a 
semantic  representation (such as a label map or a segmentation map) into a realistic 
photograph. We can use it to generate synthetic data for training machine learning 
models or to visualize concepts more practically.  
 
8) Face Frontal View Generation: GANs can genera te frontal views of faces from 
images that show the face at an angle. We can use it to improve face recognition 
algorithm’s performance or synthesize pictures for use in other applications.  
 
9) Generate New Human Poses: GANs can generate images of people in n ew 
poses, such as difficult or impossible for humans to achieve. It can be used to create 
new content or to augment existing image datasets.  
 
10) Photos to Emojis: GANs can be used to convert photographs of people into 
emojis, creating a more personalized and expressive form of communication.  
 
11) Photograph Editing: GANs can be used to edit photographs in various ways, 
such as changing the background, adding or removing objects, or altering the 
appearance of people or animals in the image.  
B.Tech –CSE R-20 
Deep Learning   
 12) Face Aging: GANs can be used to generate images of people at different ages, 
allowing users to visualize how they might look in the future or to see what theymight 
have looked like in the past.  
 
DifferencesBetweenDiscriminativeandGenerative Models  
1) Core  Idea 
Discriminative models draw boundaries in the data space, while generative 
models try to model how data is placed throughout the space. A generative model 
explains how the data was generated, while a discriminative model focuses on 
predicting the labe ls of the data.  
 
2) Mathematical Intuition  
In mathematical terms, discriminative machine learning trains a model, which 
isdonebylearningparametersthatmaximizetheconditional probability P(Y|X). On the 
other hand, a generative model learns parameters by maximizing  the joint probability 
of P(X, Y) . 
 
3) Applications  
Discriminative models recognize existing data, i.e., discriminative modeling 
identifies tags and sorts data and can be used to classify data, while Generative 
modeling produces something.  
 
Since these models use different approaches to machine learning, both are 
suited for specific tasks i.e., Generative models are useful for unsupervised learning 
tasks. In contrast, discriminative models are useful for supervised learning tasks. 
GANs(Generativeadversar ialnetworks)canbethoughtofasa competitionbetween the 
generator, which is a component of the generative model, and the discriminator, so 
basically, it is generative vs. discriminative model.  
 
4) Outliers  
 
Generativemodelshavemoreimpactonoutliersthandiscriminat ivemodels.  
B.Tech –CSE R-20 
Deep Learning   
  
5) Computational Cost  
 
Discriminative models are computationally cheap as compared to generative 
models.  
 
ComparisonBetweenDiscriminativeandGenerative  Models:  
1) Based on  Performance  
 
Generative models need fewer data to train compared with discriminative 
models since generative models are more biased as they make stronger 
assumptions, i.e., assumption of conditional independence . 
 
2) BasedonMissing Data  
 
In general, if we have missing data  in our dataset, then Generative modelscan 
work with these missing data, while discriminativemodels can’t.This isbecause, in 
generative models, we can still estimate the posterior by marginalizing the unseen 
variables. However, discriminative models usuall y require all the features X to be 
observed.  
 
3) Basedonthe Accuracy Score  
 
If the assumption of conditional independence violates, then at that time, 
generative models are less accurate than discriminative models.  
 
4) Based on Applications  
 
Discriminative models are called “discriminative” since they are useful for 
discriminating Y’s label, i.e., target outcome, so they can only solve classification 
problems. In contrast, Generative models have more applications besides 
classification, such as  samplings, Bayes learning, MAP inference, etc.  
 
GenerativeModelsvsDiscriminative Models:  
Machine learning (ML) and Deep Learning (DL) are two of the most exciting 
andconstantlychangingfieldsofstudyofthe21stcentury.Usingthese  
B.Tech –CSE R-20 
Deep Learning   
 
technologies,machinesaregiventheabilitytolearnfrompastdataandpredictor make 
decisions from future, unseen data.  
 
The inspiration comes from the human mind, how we use past expe riences to 
help us make informed decisions in the present and the future. And while there are 
already many applications of ML and DL, the future possibilities are endless.  
 
Computers utilize mathematics, algorithms, and data pipelines to draw 
meaningful in ferences from raw data since they cannot perceive data andinformation 
like humans - not yet, at least. There are two ways we can improve a machine’s 
efficiency: either get more data or come up with newer or more robust algorithms.  
 
Quintillions of data are generated all over the world almost daily, so getting 
fresh data is easy. But in order to work with this gigantic amount of data, we need 
new algorithms or we need to scale up existing ones.  
 
Mathematics, especially branches like calculus, probability,  statistics, etc., is 
the backbone of these algorithms or models. They can be widely divided into two 
groups:  
 
1. Discriminative models  
2. Generative models  
 
Mathematically, generative classifiers assume a functional form for P(Y) and 
P(X|Y), then generate estimat ed parameters from the data and use the Bayes’ 
theorem to calculate P(Y|X) (posterior probability). Meanwhile, discriminative 
classifiers assume a functional form of P(Y|X) and estimate the parameters directly 
from the provided data.  
 
B.Tech –CSE R-20 
Deep Learning   
 Discriminative model  
 
The majority of discriminative/conditional models, are used for supervised 
machine learning. They do what they ‘literally’ say, separating the data points into 
different classes and learning the boundaries using probability estimates and 
maximum likelihood . 
 
Outliers have little to no effect on these models. They are a better choice than 
generative models, but this leads to misclassification problems which can be a major 
drawback.  
 
Here are some examples and a brief description of the widely used 
discrimina tive models:  
 
1. Logisticregression: Logisticregression can be considered the linearregressionof 
classification models. The main idea behind both the algorithms is similar, but while 
linear regression is used for predicting a continuous dependent variable, logistic 
regression is used to differentiate between two or more classes.  
 
2. Support vector machines: This is a powerful learning algorithm with applicationsin 
both regression and classification scenarios. An n -dimensional space containing the 
data points is divided into classes by decision boundaries using support vectors. The 
best boundary is called a hyperplane.  
 
3. Decision trees: A graphical tree -like model is used to map decisions and their 
probable outcomes. It could be thought of as a robust ve rsion of If -else statements.  
 
A few other examples are commonly -used neural nets, k -nearest neighbor 
(KNN), conditional random field (CRF), random forest, etc.  
 
Generative model  
 
As the name suggests, generative models can be used to generate new data 
points. These models are usually used in unsupervised machine learning problems. 
Generative models go in -depth to model the actual data distribution and learn the 
different data points, rather than model just the decision boundary between classes.  
 
These m odels are prone to outliers, which is their only drawback when 
compared to discriminative models. The mathematics behind generative models is 
quite intuitive too. The method is not direct like in the case of discriminative models.  
B.Tech –CSE R-20 
Deep Learning   
 
Tocalcu lateP(Y|X),they firstestimatethepriorprobability P(Y)andthelikelihood 
probability P(X|Y) from the data provided.  
 
Putting the values into Bayes’ theorem’s equation, we get an accurate 
valuefor P(Y|X).  
 
Someexamplesaswellasadescriptionofgenerativemodelsare asfollows:  
 
1. Bayesian network: Also known as Bayes’ network,  this model uses a directed 
acyclic graph (DAG) to draw Bayesian inferences over a set of random variables to 
calculate probabilities. It has many applications like prediction, anomaly detection, 
time series prediction, etc.  
 
2. Autoregressive model: Mainly u sed for time series modeling, it finds a correlation 
between past behaviors to predict future behaviors.  
 
3. Generative adversarial network (GAN): It’s based on deep learning technology 
and uses two sub models. The generator model trains and generates new dat apoints 
and the discriminative model classifies these ‘generated’ data points into real or fake.  
 
SomeotherexamplesincludeNaiveBayes,Markovrandomfield,hiddenMarkov model 
(HMM), latent Dirichlet allocation (LDA), etc.  
 
Discriminativevsgenerative:Whichistheb estfitforDeep Learning?  
 
B.Tech –CSE R-20 
Deep Learning   
 
Discriminative models divide the data space into classes by learning the 
boundaries, whereas generative models understand how the data is embedded into 
the space. Both the approaches are widely different, which makes them suited for 
specific tasks.  
 
Deep learning has mostly been using supervised machine learning algorithms 
like Artificial Neural Networks (ANNs), convolutional neural networks (CNNs), and 
Recurrent Neural Networks (RNNs). ANN is the earliest in the trio and leverages 
artificial neurons, backpropagation, weights, and biases to identifypatterns based on 
the inputs. CNN is mostly used for image recognition and computer vision tasks. It 
works by pooling important features from an inpu t image. RNN, which is the latest of 
the three, is used in advanced fields like natural language processing, handwriting 
recognition, time series analysis, etc.  
 
These arethefieldswherediscriminative modelsareeffective andbetterused for 
deep learning as th ey work well for supervised tasks. Apart from these, deep learning 
and neural nets can be used to cluster images based on similarities. Algorithms like 
autoencoder, Boltzmann machine, and self -organizing maps are popular 
unsupervised deep learning algorith ms. They make use of generativemodels for 
tasks like exploratory data analysis (EDA) of high dimensional datasets, image 
denoising, image compression, anomaly detection and even generating new images.  
 
This Person Does Not Exist - Random Face Generator is an interesting website that 
uses a type of generative model called StyleGAN to create realistic human faces, 
even though the people in these images don’t exist!  
 
B.Tech –CSE R-20 
Deep Learning   
 
UNIT -V 
AUTO -ENCODERS: Auto -encoders, Architecture and components of auto - 
encoders (encoder and decoder), Training an auto -encoder for data 
compression and reconstruction, Relationship between Autoencoders and 
GANs, Hybrid Models: Encoder -Decoder GANs.  
Auto -encoders:  
Autoencoders are a type of deep learning algorithm that are designed to 
receive an input and transform it into a different representation. They play an 
important part in image construction . Artificial Intelligence encircles a wide range of 
technologies and te chniques that enable computer systems to solve problems like 
Data Compression which is used in computer vision, computer networks, computer 
architecture, and many other fields.  
Autoencoders areunsupervised neural networks that use machine learningto 
do this compression for us.  
What Are  Autoencoders?  
An autoencoder neural network is an Unsupervised M achine learning algorithm 
that applies backpropagation, setting the target values to be equal to the inputs. 
Autoencoders are used to reduce the size of our inputs into a smaller representation. If 
anyone needs the original data, they can reconstruct it fro m the compressed data.  
 
 
Similar machine learning algorithm i.e., PCA (Principal Component Analysis) which 
does the same task also co -exists.  
 
Autoencoders:Its Emergence  
AutoencodersarepreferredoverPCA because:  
B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
▪ Anautoencodercanlearn non-lineartransformations witha non-linear 
activation function and multiple layers.  
▪ It doesn’thave to learndense layers. It can use convolutionallayers to learn 
which is better for video, image and series data.  
▪ Itismoreefficienttolearnseverallayerswithanautoencoderratherthan learn one 
huge transformation with PCA.  
▪ Anautoencoderprovidesa representationofeachlayerasthe output.  
▪ Itcanmakeuseof pre-trainedlayers fromanothermodeltoapplytransfer learning 
to enhance the e ncoder/decoder.  
 
Applicationsof Autoencoders  
1) Image Coloring  
 
Autoencoders are used for converting any black and white picture into a 
colored image. Depending on what is in the picture, it is possible to tell what thecolor 
should be.  
 
2) Feature variation  
It extracts only the required features of an image and generates the output by 
removing any noise or unnecessary interruption.  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
3) Dimensionality Reduction  
The reconstructed image is the same as our input but with reduced 
dimensions. It helps in providing the similar image with a reduced pixel value.  
 
 
 
4) Denoising Image  
 
The input seen by the autoencoder is not the raw input but a stochastically 
corrupted version. A denoising autoencoder is thus trained to reconstruct the or iginal 
input from the noisy version.  

B.Tech –CSE R-20 
Deep Learning   
 
 
 
 
5) Watermark Removal  
It is also used for removing watermarks from images or to remove any object while filming a 
video or a movie.  
 
 
Architectureof Autoencoders  
AnAutoencoderconsistofthree layers:  
 
1. Encoder  
2. Code  
3. Decoder  
 
 
• Encoder: This part of the network compresses the input into a latent space 
representation .Theencoderlayer encodes theinputimageasa compressed 
representation in a reduced dimension. The compressed imageis the distorted 
version of the original image.  
• Code: Thispart of the network represents the compressed input which is fed to 
the decoder.  

B.Tech –CSE R-20 
Deep Learning   
 
• Decoder: This layer decodes the encoded image back to the original 
dimension. The decod ed image is a lossy reconstruction of the original image 
and it is reconstructed from the latent space representation.  
 
Thelayerbetweentheencoderanddecoder,ie.thecodeisalsoknown as 
Bottleneck . This is a well -designed approach to decide which aspects of ob served 
data are relevant information and what aspects can be discarded. It does this by 
balancing two criteria:  
 
• Compactnessofrepresentation,measuredasthe compressibility.  
• Itretainssomebehaviourallyrelevant variablesfromthe input.  
 
Traininganauto -encoderfordatacompressionand reconstruction:  
 
An autoencoder consists of two parts: an encoder network and a decoder 
network. The encoder network compresses the input data, while the decodernetwork 
reconstructs the compressed data back into i ts original form. The compressed data, 
also known as the bottleneck layer, is typically much smaller than the input data.  
 
The encoder network takes the input data and maps it to a lower -dimensional 
representation. This lower -dimensional representation is the compressed data. The 
decoder network takes this compressed data and maps it back to the original input 
data. The decoder network is essentially the inverse of the encoder network.  
 
The bottleneck layer is the layer in the middle of the autoencoder that contains 
the compressed data. This layer is much smaller than the input data, which  
B.Tech –CSE R-20 
Deep Learning   
 is what allows for compression. The size of the bottleneck layer determines the 
amount of compression that can be achieved. Autoencoders differ from other  deep 
learning architectures, such as convolutional neural networks (CNNs) and recurrent 
neural networks (RNNs), in that they do not require labeled data. Autoencoders can 
learn the underlying structure of the data without any explicit labels.  
 
Image Compr essionwith Autoencoders  
 
There are two types of image compression: lossless and lossy. Lossless 
compression methods preserve all of the data in the original image, while lossy 
compression methods discard some of the data to achieve higher compression rates.  
 
Autoencoders can be used for both lossless and lossy compression. Lossless 
compression can be achieved by using a bottleneck layer that is the same size asthe 
input data. In thiscase, the autoencoderessentiallylearns to encode anddecode the 
input data without any loss of information.  
 
Lossy compression can be achieved by using a bottleneck layer that issmaller 
than the input data. In this case, the autoencoder learns to discard some of the data 
to achieve higher compression rates. The amo unt of data that is discarded depends 
on the size of the bottleneck layer.  
Herearesomeexamplesofimagecompressionusing autoencoders:  
 
• A 512×512 color image can be compressed to a 64×64 grayscale image 
using an autoencoder with a bottleneck layer of size 64.  
• A 256×256 grayscale image can be compressed to a 128×128grayscale 
image using an autoencoder with a bottleneck layer of size 128. 
The effectiveness of autoencoder -based compression techniques can be 
evaluated by comparing the compressed and reconstructed images to the original 
images. The most common evaluation metric is the peak signal -to-noise ratio 
(PSNR), which measures the amount of noise introduced by the compression 
algorithm. Higher PSNR values indicate better compression quality.  
B.Tech –CSE R-20 
Deep Learning   
 ImageReconstructionwith Autoencoders  
 
Autoencoders are a type of neural network that can be used for image 
compression and reconstruction. The process involves compressing an image into a 
smaller representation and then reconstructing it b ack to its original form. Image 
reconstruction is the process of creating an image from compressed data.  
 
Explanationofimagereconstructionfromcompressed data:  
 
The compressed data can be thought of as a compressed version of the 
original image. To reconstr uct the image, the compressed data is fed through a 
decoder network, which expands the data back to its original size. The reconstructed 
image will not be identical to the original, but it will be a close approximation.  
 
Howautoencoderscanbeusedforimage reconstruction:  
 
Autoencoders use a loss function to determine how well the reconstructed 
image matches the original. The loss function calculates the difference between the 
reconstructed image and the original image. The goal of the autoencoder is to 
minimiz e the loss function so that the reconstructed image is as close to the original 
as possible.  
 
Examplesofimagereconstructionusing autoencoders:  
 
An example of image reconstruction using autoencoders is the MNISTdataset, 
which consists of handwritten digits. The autoencoder is trained on the dataset to 
compress and reconstruct the images. Another example is the CIFAR -10 dataset, 
which consists of 32×32 color images of objects. The autoencoder can be trained on 
this dataset to compress and r econstruct the images.  
 
Autoencoder -basedreconstructiontechniquesefficiency evaluation:  
 
The effectiveness of autoencoder -based reconstruction techniques can be 
evaluated using metrics such as Peak Signal -to-Noise Ratio (PSNR) and Structural 
SIMilarityindex (SSIM).PSNRmeasuresthequalityofthereconstructedimageby  
B.Tech –CSE R-20 
Deep Learning   
 comparingittotheoriginalimage,whileSSIMmeasuresthestructuralsimilarity between 
the reconstructed and original images.  
 
VariationsofAutoencodersforImageCompressionand Reconstruction  
 
Autoencoders can be modified and improved for better image compression 
and reconstruction. Some of the variations of autoencoders are:  
 
1) Denoising autoencoders:  
 
Denoising autoencoders are used to remove noise from images. The 
autoencoder is trained on noisy images and is trained to reconstruct the original 
image from the noisy input.  
 
2) Variational autoencoders:  
 
Variational autoencoders (VAEs) are a type of autoencoder that learn the 
probability distribution of the input data. VAEs are trained to generate  new samples 
from the learned distribution. This makes VAEs suitable for image generation tasks.  
 
3) Convolutional autoencoders:  
 
Convolutional autoencoders (CAEs) use convolutional neural networks 
(CNNs) for image compression and reconstruction. CNNs are spec ialized neural 
networks that can learn features from images.  
 
Comparisonoftheeffectivenessofdifferenttypesofautoencodersforimage compression & 
reconstruction:  
 
The effectiveness of different types of autoencoders for image compression 
and reconstruction ca n be compared using metrics such as PSNR and SSIM. CAEs 
are generally more effective for image compression and reconstruction than other 
types of autoencoders. VAEs are better suited for image generation tasks.  
 
Real -Time Examples:  
 
A real -time example of an autoencoder for image compression and 
reconstructionisGoogle’sGuetzlialgorithm.Guetzliusesacombinationofa  
B.Tech –CSE R-20 
Deep Learning   
 perceptual metric and a psycho -visual model to compress images while maintaining 
their quality. Another example is the  Deep Image Prior algorithm, which uses a 
convolutional neural network to reconstruct images from compressed data.  
 
ApplicationsofAutoencodersforImageCompressionand Reconstruction  
 
Autoencoders have become increasingly popular for image compression and 
reconstruction tasks due to their ability to learn efficient representations of the input 
data. In this, we will explore some of the common applications of autoencoders for 
image compression and reconstruction.  
 
1) Medical Imaging:  
 
Autoencoders have shown great promise in medical imaging applicationssuch 
as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and X - Ray 
imaging. The ability of autoencoders to learn feature representations from high - 
dimensional data has made them useful for compressi ng medical images while 
preserving diagnostic information.  
 
For example, researchers have developed a deep learning -basedautoencoder 
approach for compressing 3D MRI images, which achieved higher 
compressionratiosthantraditionalcompressionmethodswhilepreser vingdiagnostic 
quality. This can have significant implications for improving the storage and 
transmission of medical images, especially in resource -limited settings.  
 
2) Video Compression:  
 
Autoencoders have also been used for video compression, where the goal is 
to compress a sequence of images into a compact representation that can be 
transmitted or stored efficiently. One example of this is the video codec AV1, which 
uses a combination ofautoencodersand traditional compression methods to achieve 
higher c ompression rates while maintaining video quality. The autoencoder 
component of the codec is used to learn spatial and temporal features of the video 
frames, which are then used to reduce redundancy in the video data.  
B.Tech –CSE R-20 
Deep Learning   
 3) Autonomous Vehicles:  
 
Autoencoders are also useful for autonomous vehicle applications, where the 
goal is to compress high -resolution camera images captured by the vehicle’ssensors 
while preserving critical information for navigation and obstacle detection. For 
example, researc hers have developed an autoencoder -based approach for 
compressing images captured by a self -driving car, which achieved highcompression 
ratioswhilepreservingtheaccuracyof objectdetectionalgorithms.This can have 
significant implications for improving the pe rformance and reliability of autonomous 
vehicles, especially in scenarios where high -bandwidth communication is not 
available.  
 
4) SocialMediaandWeb Applications:  
 
Autoencoders have also been used in social media and web applications, 
where the goal is to reduce the size of image files to improve website loading times 
and reduce bandwidth usage. For example, Facebook uses an autoencoder -based 
approach for compressing images uploaded to their platform, which achieves high 
compression ratios while preserving image quality. This has led to faster loading 
times for images on the platform and reduced data usage for users.  
 
Comparison  of the effectiveness  of autoencoder -based  compression  and 
reconstruction techniques for different applications:  
 
The effectiveness of autoencoder -based compression and reconstruction 
techniques can vary depending on the application and the specific requirements of 
the task. For example, in medical imaging applications, the preservation ofdiagnostic 
informationiscritical, while in soci almediaapplications, image qualityand loading times 
may be more important. Researchers have compared theeffectiveness of 
autoencoder -based compression and reconstruction techniques with traditional 
compression methods and have found that autoencoder -based methods often 
outperformtraditionalmethodsin termsof compression ratio and image quality.  
 
RelationshipbetweenAutoencodersand GANs:  
B.Tech –CSE R-20 
Deep Learning   
 Autoencoders and GANs are both powerful techniques for learning from data 
in an unsupervised way, but they have some differences and trade -offs.Autoencoders 
are easier to train and more stable, but they tend to produce blurry or distorted 
reconstructions or generations. GANs are harder to train and more proneto mode 
collapse, where they produce only a few modes  of the data distribution, but 
theytendtoproducesharperandmorediversegenerations.Dependingonyourgoal and 
your data, you might prefer one or the other, or even combine them in a hybrid 
model.  
 
Autoencoders are unsupervised models, which means that they are nottrained 
on labeled data. Instead, they are trained on unlabeled data and learn to reconstruct 
the input data. GANs, on the other hand, are supervised models, which means that 
they are trained on labeled data. The generator in a GAN is trained to gen erate data 
that looks like the labeled data, and the discriminator is trained to distinguish 
between real and fake data. Autoencoders are typically used for tasks such as image 
denoising and compression. GANs are typically used for tasks such as image 
gene ration and translation.  
 
HybridModels:Encoder -Decoder GANs:  
 
HowcanyoucombineGANsandautoencoderstocreatehybridmodelsforvarious tasks?  
 
Generativeadversarialnetworks(GANs)andautoencodersaretwopowerfultypesof 
artificial neural networks that can learn from dat a and generate new samples. But what if 
you could combine them to create hybrid models that can perform various tasks, such as 
image synthesis, anomaly detection , or domain adaptation.  
GANsand autoencoders  
GANs are composed of two networks: a generator and a discriminator. The 
generator tries to create realistic samples from random noise, while the discriminator 
tries to distinguish between real and fake samples. The two networks compete with 
each other, improving their skills over time. Autoencoders are com posed of two 
networks:anencoderandadecoder.Theencodercompressestheinputdatainto a 
B.Tech –CSE R-20 
Deep Learning   
 lower -dimensional representation, while the decoder reconstructs the input datafrom 
the representation. The goal is to minimize the reconstruction error, whi le learning 
useful features from the data.  
Hybrid models  
Hybrid models are models that combine GANs and autoencoders in different 
ways, depending on the task and the objective. For example, you can use an 
autoencoder as the generator of a GAN, and train it to fool the discriminator, while 
also minimizing the reconstruction error. This way, we can generate realistic samples 
that are similar to the input data, but also have some variations. Alternatively, youcan 
use a GAN as the encoder of an autoenco der, and train it to encode the input data 
into a latent space that is compatible with the discriminator. This way, you can learn 
ameaningfulrepresentation ofthedatathatcanbeusedfordownstreamtasks, such as 
classification or clustering.  
Image synthesis  
One of the most common tasks for hybrid models is image synthesis, which is 
the process of creating new images from existing ones, or from scratch. For example, 
you can use a hybrid model to synthesize images of faces, animals, or landscapes, by 
using an autoe ncoder as the generator of a GAN, and feeding it with real images or 
random noise. This way, you can create diverse and realistic images that preserve the 
attributes of the input data, but also have some variations. You can also use a hybrid 
model to synth esize images of different domains, such as converting photos to 
paintings, or day to night, by using a GAN as the encoder of an autoencoder, and 
feeding it with images from both domains. This way, you can learn a common latent 
space that can be used to tra nsfer the style or the attributes of one domain to 
another.  
B.Tech –CSE R-20 
Deep Learning   
 Anomaly detection  
Another task for hybrid models is anomaly detection, which is the process of 
identifying abnormal or unusual patterns in the data, such as outliers, frauds, or 
defects. For example, you can use a hybrid model to detect anomalies in images,such 
as damaged products, or medical conditions, by using an autoencoder as the 
generator of a GAN, and feeding it with normal images. This way, you can train the 
autoencoder to reconstruct normal images well, but fail to reconstruct abnormal 
images.  
Then, we can use the reconstruction error or the discriminator score as a 
measure of anomaly. You can also use a hybrid model to detect anomalies in time 
series, such as sensor readings, or financial transactions, by using a GAN as the 
encoder ofan autoencoder, and feeding it with normal time series. This way, you can 
train the GAN to encode normal time series well, but fail to encode abnormal time 
series. Then, we can use the la tent space or the discriminator score as a measure of 
anomaly.  
Domain adaptation  
A third task for hybrid models is domain adaptation, which is the process of 
adapting a model trained on one domain to work on another domain, without 
requiring labeled data fr om the target domain. For example, you can use a hybrid 
model to adapt a model trained on images of handwritten digits to work on images 
of handwritten letters, by using a GAN as the encoder of an autoencoder, andfeeding 
it with images from both domains. T his way, you can train the GAN toencode both 
domains into a shared latent space that is invariant to the domain differences.  
